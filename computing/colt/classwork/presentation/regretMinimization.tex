\documentclass[10pt]{amsart}
\usepackage{amsmath,amssymb, algorithm}


\input{../../macros}


%opening
\title{Book chapter presentation: Learning, Regret Minimization and Equilibria.}
\author{vishvAs vAsuki}

\begin{document}
\maketitle

\part{Outline}

\section{Intro about the paper}
Book chapter presentation: Learning, Regret Minimization and Equilibria. \cite{algGameTheory} Authors: Avrim Blum, Yishay Mansour. A few results appeared first in COLT 2005: From external to internal regret.

We will see some important results and techniques from this work.

\section{Plan}
20 minutes available. 8 minutes for introducing the model. 10 minutes for proving result about making an internal regret alg from an external regret alg. 2 minutes remarking about connection with game theory and equilibria.

\part{Introducing the model}

\section{Players, strategies, utilities}
Players $P = \set{p_{i}}$. A strategy is not a move but an algorithm to make moves. $S_{i}$: strategy set of $p_{i}$. Strategy vector (strategy profile): $s = (s_{1}, .. s_{n})$. $s_{-i}$: s sans $s_{i}$.

\subsection{Mixed/ randomized strategies}
Independent mixed strategy of i: a Prob Distr over $S_{i}: D_{i}$.

Mixed strategy profile, perhaps $p_{i}$ coordinated: Probability distribution over $\times_{i}S_{i}$: D.

\subsection{Utility}
Preference ordering of outcomes for i: Cost, utility of strategy:\\
 $c_{i}(s) = -u_{i}(s)$.

\subsubsection{$\eps$ dominated strategy}
$s_{i}$ dominated by $s_{i}'$ if : $u_{i}(s'_{i}, s_{-i}) \geq u_{i}(s_{i}, s_{-i}) + \eps$.

\section{Repeated games with partial info about utilities}
$p_{1}$ in uncertain environment ($p_{-1}$); utilities of $p_{-1}$ not known. Eg: Choosing a route to go to school.

\subsection{Model}
Same game repeated T times; At time t:
$p_{1}$ uses online alg H to pick distr $D_{H}^{(t)}$ over $S_{1}$. $p_{1}$ picks action $k_{1}^{(t)}$ from $D_{H}^{(t)}$. Loss/ cost function for $p_{1}$: $c_{1}:\times_{i}S_{i} \to [0,1]$. $c_{1}^{(t)}(k_{1}^{(t)}) \dfn c_{1}(k_{1}^{(t)}, D_{-1}^{(t)})$, $c_{1}(D) \dfn E_{x \distr D}[c_{1}(x)]$.

\subsubsection{Model with full info about costs}
H gets cost vector $c_{1}^{(t)} \in [0,1]^{|S_{1}|}$, pays cost \\
$c_{1}(D_{H}^{(t)}, D_{-1}^{(t)}) = E_{k_{1}^{(t)} \distr D_{H}^{(t)}}[c_{1}(k_{1}^{(t)}, D_{-1}^{(t)})] = E_{k_{1}^{(t)} \distr D_{H}^{(t)}}[c_{1}^{(t)}(k_{1}^{(t)})]$.

Total loss for H: $L_{H}^{(T)} = \sum c_{1}(D_{H}^{(t)}, D_{-1}^{(t)})$.

\subsubsection{Model with partial info about costs}
Aka Multi Armed Bandit (MAB) model. $p_{1}$ (or H) pays cost for $k_{1}^{(t)}$: $c_{1}(k_{1}^{(t)}, D_{-1}^{(t)}) = c_{1}^{(t)}(k_{1}^{(t)})$.

Total loss for H: $L_{H}^{(T)} = \sum c_{1}(k_{1}^{(t)}, D_{-1}^{(t)})$.

\subsubsection{Goal}
Minimize $\frac{L_{?}^{(T)}}{T}$. Maybe other $p_{i}$ do the same. $D_{-1}^{(t)}$ and $c_{1}^{(t)}$ can vary arbitrarily over time; so, model is adversarial.

\subsection{Regret analysis}
H incurs loss $L_{H}^{(T)}$; $p_{1}$ sees simple policy $\pi$ would have had much lower loss. Comparison class of algs G. $\pi$ best alg in G: $L_{\pi}^{(T)} = min_{g \in G} L_{g}^{(T)}$. Regret $R_{G} = L_{H}^{(T)} - L_{\pi}^{(T)} = max_{g \in G} (L_{H}^{(T)} - L_{g}^{(T)})$.

\subsubsection{Goal}
Minimize $R_{G}$.

\subsubsection{Lower bound for regret wrt all policies}
$G_{all} = \set{g: T \to S_{1}}$: $\exists$ sequence of loss vectors $c_{1}^{(t)}$: $R_{G_{all}} \geq T(1-|S_{1}|^{-1})$.

So, must restrict G.

\subsection{External regret}
Aka Combining Expert Advice. $G = \set{i^{T} : i \in S_{1}}$, policies where all $k_{1}^{(t)}$ are the same; $\pi$ is best single action. $L_{\pi}^{(T)} = \sum c_{1}(\pi, D_{-1}^{(t)})$.

If H has low external regret bound: H matches performance of offline alg. \why  H comparable to optimal prediction rule from some large hyp class H. \why

\subsubsection{Rand Weighted majority alg (RWM)}
Suppose $c_{1}^{(t)} \in \set{0,1}^{|S_{1}|}$. Treat $S_{1}$ as a bunch of experts: Want to put as much wt as possible on best expert. Let $|S_{1}| = N$. Init weights $w_{i}^{(1)} = 1$, total wt $W^{(1)} = N$, $Pr_{D_{H}^{(1)}}(i) = N^{-1}$.

If $c_{1}^{(t-1)}(i) = 1$, $w_{i}^{(t)} = w_{i}^{(t)}(1-\eta)$, $Pr_{D_{1}^{(t)}}(i) = \frac{w_{i}^{(t)}}{W^{(t)}}$. \why Like analysis of mistake bound of panel of k experts in colt ref.

For $\eta < 2^{-1}$, $L_{H}^{(T)} \leq (1+ \eta) \min_{i \in S_{1}}L_{i}^{(t)} + \frac{\ln N}{\eta}$. Any time H sees significant expected loss, big drop in W. $W^{(T+1)} \geq max_{i}w_{i}^{(T+1)} = (1-\eta)^{\min_{i}L_{i}^{(T)}}$. \tbc

For $\eta = \min \set{\sqrt{\ln N/ T}, 2^{-1}}$: $L_{H}^{(T)} \leq \min_{i} L_{i}^{(T)} + 2\sqrt{T\ln N}$. If T unknown, use 'guess and double' with const loss in regret. \why

\subsubsection{Polynomial weights alg}
Extension of RWM to $c_{1}^{(t)} \in [0,1]^{|S_{1}|}$. Wt update is $w_{i}^{(t)} = w_{i}^{(t)}(1-\eta c^{(t-1)}(i))$. $L_{H}^{(T)} \leq \min_{i} L_{i}^{(T)} + 2\sqrt{T\ln N}$. \why

\subsubsection{Rand Alg Lower bounds}
If $T <  \log_{2} N$: For any online alg H, $\exists$ stochastic generation of losses: $E[L_{H}^{(T)}] = T/2$, but $\min_{i} L_{i}^{(t)} = 0$: at t=1 let N/2 actions get loss 1; at time t: half the actions which had a loss 0 at time t-1 get loss 1; so, probability mass on actions with 0 = $2^{-1}$.

If N=2, $\exists$ stochastic generation of losses: $E[L_{H}^{(T)} - \min_{i} L_{i}^{(T)}] = \Omega(\sqrt{T})$. \why

\subsubsection{Convergence to equilibrium: 2 player constant sum repeated game}
All $p_{i}$ use alg H with external regret R; Value of game: $(v_{i})$. Avg loss: $\frac{L_{H}^{(T)}}{T} \leq v_{i}$. \why If $R_{G} = O(\sqrt{T})$, convergence to $v_{i}$.

\part{Models to be introduced if there is time}
\section{Nash equilibrium}

Defn: D or $\set{D_{i}}$ where even if all $p_{i}$ know all $D_{i}$, no treachery profitable. Maybe D not unique. So each $p_{i}$ can decide $D_{i}$ if he knows $D_{-i}$.

\subsection{Randomized (mixed) strategies} Not Pure strategy s, but distr D. Risk neutral $p_{i}$ maximize $u_{i}(D) = E_{s \distr D}[u_{i}(s)]$, with $Pr_{s \distr D}(s) = \prod_{i} Pr_{s_{i} \distr D_{i}}(s_{i})$.

\subsection{Existance of Equilibria}
Any game with $|P|, |S_{i}|$ finite, $\exists$ mixed strategy Nash equilib. \why

\subsection{$\eps$ Nash equilib}
A special case: $\forall i, D': u_{i}(D) \geq u_{i}(D'_{i}, D_{-i}) - \eps$

\section{Correlated equilibrium D}
(Aumann). Coordinator has distr D, samples s from D, tells each $p_{i}$ its $s_{i}$. $p_{i}$ not told $s_{j}$, but knows it is correlated to $s_{i}$; so knows all $Pr(s_{-i}|s_{i})$. D known to every $p_{i}$. D is correlated equilib if it is not in any $p_{i}$'s interest to deviate from s, assuming other $p_{i}$ follow instructions: \\
$E_{s_{-i} \distr D|s_{i}} [u_{i}(s_{i}, s_{-i})] \geq E_{s_{-i}\distr D|s_{i}} [u_{i}(s'_{i}, s_{-i})] $.

Mixed strategy Nash equilibrium is the special case where $D_{i}$ are independently randomized (with diff coins).

\subsection{Regret defn}
$f_{i}:S_{i} \to S_{i}$, regret $r_{i}(s,f) = u_{i}(f_{i}(s_{i}), s_{-i}) - u_{i}(s)$:\\
 $E_{s \distr D}[r_{i}(s,f_{i})] \geq 0$.

\subsection{$\eps$ correlated equilibrium}
$E_{s \distr D}[r_{i}(s,f_{i})] \leq \eps$.

\subsection{Traffic light/ Chicken} $C = \mat{(-100, -100) & (1,0)\\(0,1) & (0, 0)}$. s = (1, 2) and (2, 1) stable; so coordinator picks one randomly. This correlation increases payoff as the low expected utility mixed strategy $D_{i} = (101^{-1}, 1-101^{-1})$ is avoided.

\part{Important results}
\subsection{Low external regret alg in partial cost info model}
Exploration vs exploitation tradeoff in algs.

Alg MAB: Divide time T into K blocks; in each time block $\tau+1$: explore and get cost vector: execute action i at random time to get vector of RV's: $\hat{c}^{(\tau)}$, also exploit: use distr $D^{(\tau)}$ as strategy; pass $\hat{c}^{(\tau)}$ to full info external regret alg F with ext regret $R^{(K)}$ over K time steps; get distr $D^{(\tau + 1)}$ from F.

Max Loss during exploration steps: NK. RV for total loss of F over K time blocks: $\hat{L}_{F}^{(T)} = \frac{T}{K}\sum_{\tau}p^{\tau}c^{\tau} \leq \frac{T}{K}(min_{i} \hat{L}_{i}^{(K)} + R^{(K)}$. Taking expectation, $L_{MAB}^{(T)} = E[\hat{L}_{MAB}^{(T)}]= E[\hat{L}_{F}^{(T)} + NK] \leq \frac{T}{K}(E[min_{i} \hat{L}_{i}^{(K)}] + R^{(K)}) + NK \leq \frac{T}{K}(min_{i} E[\hat{L}_{i}^{(K)}] + R^{(K)}) + NK \leq min_{i}L_{i}^{(T)} + \frac{T}{K}R^{(K)} + NK$.

Using the $O(\sqrt{K\log N})$ alg, with $K=(\frac{T}{K}R_{K})$, we get $L_{MAB}^{(T)} \leq min_{i}L_{i}^{(T)} + O(T^{2/3}N^{1/3}\log N)$.

\subsection{Swap regret}
Comparison alg (H,g) is H with some swap fn $g:S_{1} \to S_{1}$.

\subsubsection{Internal regret}
A special case: Swap every occurance of action $b_{1}$ with action $b_{2}$. Modification fn: $switch_{i}(k_{i},b_{1}, b_{2}) = k_{i}$ except $switch_{i}(b_{1},b_{1}, b_{2}) = b_{1}$.

\subsubsection{Low Internal regret alg using external regret minimization algs}
Let $N=|S_{i}|$; $(A_{1}, .., A_{N})$ copies of alg with external regret bound R. Master alg H gets from $A_{i}$ distr $q_{i}^{(t)}$ over $S_{i}$; makes matrix $Q^{(t)}$ with $q_{i}^{(t)}$ as rows; finds stationary distr vector $p^{(t)} = p^{(t)}Q^{(t)}$: Picking $k_{i} \in S_{i}$ same as picking $A_{j}$ first, then picking $k_{i} \in S_{i}$; gets loss vector $c^{(t)}$; gives $A_{i}$ loss vector $p_{i}^{(t)}c^{(t)}$.

$\forall j: L_{A_{i}} = \sum_{t} p_{i}^{(t)}\dprod{c^{(t)},q_{i}^{(t)}} \leq \sum_{t} p_{i}^{(t)}c_{j}^{(t)} + R$. Also, Sum of percieved losses = actual loss. So, for any swap fn g, $L_{H}^{T}\leq \sum_{i}\sum_{t} p_{i}^{(t)}c_{g(i)}^{(t)} + NR = L_{F,g}^{(T)} + NR$.

Thence, using polynomial weights alg, swap regret bound $O(\sqrt{|S_{1}| T \log |S_{1}|})$.

\subsubsection{Convergence to Correlated equilibrium}
Every $p_{i}$ uses strategy with swap regret $\leq R$: then empirical distr Q over $\times_{i} S_{i}$ is an $\frac{R}{T}$ correlated equilibrium. $R = L_{H}^{(T)} - L_{H,g}^{(T)} = \sum_{t} E_{s^{(t)} \distr D^{(t)}}[r_{i}(s,g)] = T E_{s \distr Q}[r_{i}(s,g)]$.

Convergence if all players have sublinear swap regret.

\subsubsection{Frequency of dominated strategies}
$p_{1}$ uses alg with swap regret R over time T; w: avg over T of prob weight on $\eps$ dominated strategies; so $\eps wT \leq R$; so $w \leq \frac{R}{T\eps}$.

If alg minimizes external regret using polynomial weights alg, freq of doing dominated actions tends to 0.

\bibliographystyle{plain}
\bibliography{../../gameTheory/gameTheory}

\end{document}
