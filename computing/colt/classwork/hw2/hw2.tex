\documentclass[10pt]{amsart}
\usepackage{amssymb, algorithm2e}

\input{../../macros}

\newtheorem{thm}{Theorem}[subsection]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{claim}[thm]{Claim}

\theoremstyle{remark}
\newtheorem{defn}[thm]{Definition}
\newtheorem*{notation}{Notation}
\newtheorem*{ack}{Acknowledgement}
\newtheorem{alg}[thm]{Algorithm}
\newtheorem{rem}[thm]{Remark}
\newtheorem{fact}[thm]{Fact}

%opening
\title{Computational Learning Theory: Answers to Homework 2}
\author{vishvAs vAsuki}

\begin{document}

\maketitle

\section{}
\begin{defn}
A sober algorithm in the Mistake bounded model is an algorithm which, whenever it makes a mistake, alters its hypothesis to be consistent with all the examples it has seen so far.
\end{defn}

\begin{lem}
 If a Concept class C is efficiently learnable in the mistake bound model, it can be learnt by an efficient sober algorithm.
\end{lem}
\begin{proof}
 Let L be a non-sober algorithm which learns C with mistake bound b. So, upon making a mistake on a certain example, L does not update its hypothesis to be consistent with all the examples it has seen so far.

Suppose that, at the end of a certain sequence S of examples, L makes a mistake on x. Also suppose that L does not update its hypothesis at all; or even if it does, its new hypothesis is still not consistent with S. Then, we can conceive of L repeatedly receiving the same sequence S, causing it to exceed its claimed mistake bound b.

Hence, after seeing S a some finite number of times, if L is to stay within its mistake bound, it must update its hypothesis to be consistent with S.

The efficient sober algorithm L' is obtained by executing such an update whenever a mistake is seen.
\end{proof}

\begin{thm}
If C is efficiently learnable in the mistake bounded learning model, it is efficiently learnable in the PAC learning model.
\end{thm}
\begin{proof}
If C is efficiently learnable in the mistake bounded model, then there exists an efficient sober algorithm L which learns it.

Suppose that we run L on a set S of $m = \Omega(\eps^{-1}\log(\del^{-1}) + \frac{d}{\eps}\log(\eps^{-1}))$ examples. Then, L ends up with a hypothesis h consistent with S.
Now, we use one of the VCD/ Occam razor theorems to conclude that, with probabilility $\del^{-1}$, h is $\eps$ close to the target concept. Further more, L is efficient.
\end{proof}

\section{}
\begin{ack}
The lower bound is due to Sungho Yun.
\end{ack}

\begin{thm}
Let C be the conjunctions on n literals. VCD for C is n.
\begin{rem}
C includes the conjunction of size 0, which accepts every possible example.
\end{rem}
\end{thm}
\begin{proof}
Let $e_{i}$ represent a sample with the bit $x_{i}=1$, and with all other bits $x_{j} = 0$. All conjunctions are assumed to have no redundant variables.

The set $S = \set{e_{1}, .. e_{n}}$ is shattered by C. The reason is as follows: Consider any $S' \subset S$. This subset has exactly $n-|S'|$ bit values in common; and all $e_{i} \in S-S'$ disagrees with atleast one of these bit values. Corresponding to any set of bit values, there is a conjunction. Hence the dichotomy which labels S' and S-S' differently is realizable.

Suppose that a set $S = \set{s_{1}, .. s_{t}}$ was shattered by C. Let the conjunction which selects only $s_{i}$ in S be called $c_{i}$. Think of conjunctions as a set of variables involved in the conjunction's expression. Consider the conjunction which admits $s_{1}, .. s_{i}$ and rejects everything else in S. This conjunction is $C_{i} = \Inters_{j=1}^{i} c_{j}$. Suppose that the number of literals in $C_{i}$ is $L_{i}$. We note that $L_{i+1} \leq L_{i}$. But, if $L_{i} = L_{i-1}$, we will be violating the assumtion that $C_{i}$ admits $s_{1}, .. s_{i}$ and rejects $s_{i+1}$. Hence, we observe that $L_{i+1} \leq L_{i} - 1$.

But, $L_{1} \leq n$. And we know that $L_{t} \geq 0$. Hence, we conclude that $t\leq n$.
\end{proof}

\section{}
\begin{thm}
C = axis aligned rectangles. C is efficiently agnostically learnable.
\end{thm}
\begin{proof}
Consider an algorithm which samples a set of S points, uses
exhaustive search to find the rectangle with the least error rate.
It then uses it as the hypothesis.

To bound the size of S, we use one of the VCD-style bounds: $|S| = \Omega(\eps^{-2}(4 + \log \frac{1}{\del}))$. Then, with probability $1- \del$, we are $\eps$ close to the rectangle with the least error over all possible samples.

This algorithm is efficient as it takes time polynomial in $|S|$.
\end{proof}


\section{}
Distribution used: Uniform distribution on the unit sphere in $R^{n}$: $S^{n-1}$.

Let c be the target origin centered halfspace defined by vector u and hyperplane $u_{\perp}$.

Algorithm L for classifying point p:

\begin{algorithm}
\KwIn{p}
\KwOut{The label h(p)}
Choose set S of labeled points uniformly at random.\\
Use this net of points to classify a future random point p via m-nearest-neighbor. Let $S' = \set{p': p'\in S, \dprod{p',p} \geq 0}$. Return $h(p) = maj_{p' \in S'} (sgn(\dprod{p',u}))$. Let $|S'| = m$ \\
\end{algorithm}

\begin{thm}
Let $m = poly(n, \eps^{-1})$. $Pr_{p, S}(h(p) \neq sgn(\dprod{p, u})) \leq \eps$.
\end{thm}
\begin{proof}
Without loss of generality, suppose that p is in the halfspace. The analysis for p not in the halfspace will be identical.

Let t be the angle between p and the hyperplane $u_{\perp}$.

Consider some $p' \in S'$; so its angle with p is less than $\frac{\pi}{2}$. $sgn(\dprod{p',u})<0$ iff its angle with u $> \frac{\pi}{2}$; that is, if it is in the space $p_{\perp} - u_{\perp}$. We use the following fact:
\begin{fact}
If angle between u, p is $\frac{\pi}{2} - t$, $Pr(sgn(\dprod{u,x}) \neq sgn(\dprod{p,x})) = \frac{\frac{\pi}{2} - t}{\pi} = \frac{\pi - 2t}{2\pi}$.
\end{fact}

So, using symmetry, we find that Pr(p' is misleading) = $Pr_{S'}(sgn(\dprod{p',u})<0) = \frac{\pi - 2t}{4\pi}$.

For our algorithm to predict erroneously, at least m/2 points in S' should be misleading.
The expected number of misleading points in S' is $\frac{\pi - 2t}{4\pi}$.

Using the Chernoff bound and some algebra, we find the probability that this is over $\frac{m}{2}$ to be $e^{(\frac{\pi + 2t}{4 \pi} - \frac{1}{2}\ln(\frac{2\pi}{\pi - 2t}))m}$. We see that this quantity is less than $\eps/4$ when $m \geq \frac{\ln \frac{\eps}{4}}{(-\frac{\pi + 2t}{4 \pi} + \frac{1}{2}\ln(\frac{2\pi}{\pi - 2t}))}$.

\begin{fact}
For small a: $Pr_{p}(t< a)\leq a n^{0.5}$.
\end{fact}

So, for $a = \frac{\eps}{4n^{0.5}}$, $Pr_{p}(t < a) \leq \frac{\eps}{4}$. By the way of worst case analysis, let us assume that whenever $t<a$, L fails.

Hence, using the union bound, we see that Pr(L fails when given p inside the halfspace) $\leq \frac{\eps}{2}$ if we choose $m = \frac{\ln \frac{\eps}{4}}{(-\frac{\pi + 2a}{4 \pi} + \frac{1}{2}\ln(\frac{2\pi}{\pi - 2a}))}$.

By symmetry, Pr(L fails when given p outside the halfspace) $\leq \eps/2$ for the same value of m.

Thus, for m polynomial in n and $\eps^{-1}$, L fails with probability $\eps$.
\end{proof}

% \bibliographystyle{plain}
% \bibliography{../colt}


\end{document}
