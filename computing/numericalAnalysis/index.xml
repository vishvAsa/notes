<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>&#43;Numerical Analysis on Vishvas&#39;s notes</title>
    <link>file:///storage/emulated/0/notesData/notes/computing/numericalAnalysis/</link>
    <description>Recent content in &#43;Numerical Analysis on Vishvas&#39;s notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="file:///storage/emulated/0/notesData/notes/computing/numericalAnalysis/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Common problems</title>
      <link>file:///storage/emulated/0/notesData/notes/computing/numericalAnalysis/common_problems/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>file:///storage/emulated/0/notesData/notes/computing/numericalAnalysis/common_problems/</guid>
      <description>Interpolation See approximation theory ref.
Root finding Find \(x: f(x) = 0\).
Newton&amp;rsquo;s method Find the root iteratively. Take a local linear approximation \(g(x) = f(x_n) + \gradient f(x_n)^{T}(x-x_n)\) for \(f\), find its root; and use it as the next guess for the root.
\(x_{n+1} = x_{n} + \frac{f(x_{n})}{f&amp;rsquo;(x_{n})}\). For vector function \(f\): \(x_{n+1} = x_{n} + (\gradient(f(x_{n})))^{-1}f(x_{n})\).
This becomes the Newton&amp;rsquo;s method in optimization, when applied to solving for the optimality condition \(\gradient f(x) = 0\).</description>
    </item>
    
    <item>
      <title>Dense algebra</title>
      <link>file:///storage/emulated/0/notesData/notes/computing/numericalAnalysis/dense_algebra/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>file:///storage/emulated/0/notesData/notes/computing/numericalAnalysis/dense_algebra/</guid>
      <description>Decompositional approach Easy to analyze stability. Can reuse decomposition to solve multiple instances of the problem: eg consider A = LU.
Condition number of a matrix Condition number of Ax when x perturbed \(k = \sup_{\change x} \frac{\norm{A\change x}}{\norm{\change x}}\frac{\norm{x}}{\norm{Ax}} = \norm{A}\frac{\norm{x}}{\norm{Ax}} \leq \norm{A}\norm{A^{-1}}\) or \(\leq \norm{A}\norm{A^{+}}\) if A not square.
So, condition of \(A^{-1}b\) when b perturbed, \(k = \norm{A^{-1}}\frac{\norm{Ax}}{\norm{x}} \leq \norm{A}\norm{A^{-1}}\).
Condition number of matrix wrt norm \(k(A) = \norm{A}\norm{A^{-1}}\).</description>
    </item>
    
    <item>
      <title>Low error computation</title>
      <link>file:///storage/emulated/0/notesData/notes/computing/numericalAnalysis/low_error_computation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>file:///storage/emulated/0/notesData/notes/computing/numericalAnalysis/low_error_computation/</guid>
      <description>Round off errors See the computer architecture survey&amp;rsquo;s chapters on number storage where representation accuracy and arithmetic accuracy are described. Tolerance of the problem and of the algorithm to representation error and error in basic arithmetic computations is considered later.
Underflow avoidance Sometimes, in case of computations involving multiplication and division with very small numbers (eg: probability calculation), there is the chance of underflow: a small non-0 number \(x\) may be stored as 0; which would then lead to \(0\) and \(\infty\) results during later multiplication and division.</description>
    </item>
    
  </channel>
</rss>