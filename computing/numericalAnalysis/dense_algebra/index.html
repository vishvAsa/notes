<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Vishvas&#39;s notes  | Dense algebra</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.72.0" />
    
    
      <META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
    

    <link href="/storage/emulated/0/notesData/notes/computing/numericalAnalysis/dense_algebra/" rel="alternate" type="application/rss+xml" title="Vishvas&#39;s notes" />
    <link href="/storage/emulated/0/notesData/notes/computing/numericalAnalysis/dense_algebra/" rel="feed" type="application/rss+xml" title="Vishvas&#39;s notes" />

    <meta property="og:title" content="Dense algebra" />
<meta property="og:description" content="Decompositional approach Easy to analyze stability. Can reuse decomposition to solve multiple instances of the problem: eg consider A = LU.
Condition number of a matrix Condition number of Ax when x perturbed \(k = \sup_{\change x} \frac{\norm{A\change x}}{\norm{\change x}}\frac{\norm{x}}{\norm{Ax}} = \norm{A}\frac{\norm{x}}{\norm{Ax}} \leq \norm{A}\norm{A^{-1}}\) or \(\leq \norm{A}\norm{A^{&#43;}}\) if A not square.
So, condition of \(A^{-1}b\) when b perturbed, \(k = \norm{A^{-1}}\frac{\norm{Ax}}{\norm{x}} \leq \norm{A}\norm{A^{-1}}\).
Condition number of matrix wrt norm \(k(A) = \norm{A}\norm{A^{-1}}\)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="file:///storage/emulated/0/notesData/notes/computing/numericalAnalysis/dense_algebra/" />

<meta itemprop="name" content="Dense algebra">
<meta itemprop="description" content="Decompositional approach Easy to analyze stability. Can reuse decomposition to solve multiple instances of the problem: eg consider A = LU.
Condition number of a matrix Condition number of Ax when x perturbed \(k = \sup_{\change x} \frac{\norm{A\change x}}{\norm{\change x}}\frac{\norm{x}}{\norm{Ax}} = \norm{A}\frac{\norm{x}}{\norm{Ax}} \leq \norm{A}\norm{A^{-1}}\) or \(\leq \norm{A}\norm{A^{&#43;}}\) if A not square.
So, condition of \(A^{-1}b\) when b perturbed, \(k = \norm{A^{-1}}\frac{\norm{Ax}}{\norm{x}} \leq \norm{A}\norm{A^{-1}}\).
Condition number of matrix wrt norm \(k(A) = \norm{A}\norm{A^{-1}}\).">

<meta itemprop="wordCount" content="4293">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Dense algebra"/>
<meta name="twitter:description" content="Decompositional approach Easy to analyze stability. Can reuse decomposition to solve multiple instances of the problem: eg consider A = LU.
Condition number of a matrix Condition number of Ax when x perturbed \(k = \sup_{\change x} \frac{\norm{A\change x}}{\norm{\change x}}\frac{\norm{x}}{\norm{Ax}} = \norm{A}\frac{\norm{x}}{\norm{Ax}} \leq \norm{A}\norm{A^{-1}}\) or \(\leq \norm{A}\norm{A^{&#43;}}\) if A not square.
So, condition of \(A^{-1}b\) when b perturbed, \(k = \norm{A^{-1}}\frac{\norm{Ax}}{\norm{x}} \leq \norm{A}\norm{A^{-1}}\).
Condition number of matrix wrt norm \(k(A) = \norm{A}\norm{A^{-1}}\)."/>

      
    

    <script src="/storage/emulated/0/notesData/notes/webpack_dist/dir_tree-bundle.js"></script>
    <script type="text/javascript">
    
    let baseURL = "file:\/\/\/storage\/emulated\/0\/notesData\/notes";
    let basePath = "/" + baseURL.split("/").slice(3).join("/");
    let siteParams = JSON.parse("{\u0022contactlink\u0022:\u0022https:\/\/github.com\/vvasuki\/notes\/issues\/new\u0022,\u0022disqusshortcode\u0022:\u0022vvasuki-site\u0022,\u0022env\u0022:\u0022production\u0022,\u0022githubeditmepathbase\u0022:\u0022https:\/\/github.com\/vvasuki\/notes\/edit\/master\/content\/\u0022,\u0022mainSections\u0022:[\u0022math\u0022],\u0022mainsections\u0022:[\u0022math\u0022]}");
    let pageDefaultsList = JSON.parse("[{\u0022scope\u0022:{\u0022pathPrefix\u0022:\u0022\u0022},\u0022values\u0022:{\u0022comments\u0022:true,\u0022layout\u0022:\u0022page\u0022,\u0022search\u0022:true,\u0022sidebar\u0022:\u0022home_sidebar\u0022}}]");
    let sidebarsData = JSON.parse("{\u0022home_sidebar\u0022:{\u0022contents\u0022:[{\u0022url\u0022:\u0022recdir:\/\/\u0022},{\u0022contents\u0022:[{\u0022title\u0022:\u0022ज्यौतिषम्\u0022,\u0022url\u0022:\u0022..\/jyotiSham\/\u0022},{\u0022title\u0022:\u0022संस्कृतम्\u0022,\u0022url\u0022:\u0022..\/sanskrit\/\u0022},{\u0022title\u0022:\u0022मीमांसा\u0022,\u0022url\u0022:\u0022..\/mImAMsA\/\u0022},{\u0022title\u0022:\u0022Notes Home\u0022,\u0022url\u0022:\u0022..\/notes\/\u0022},{\u0022title\u0022:\u0022काव्यम्\u0022,\u0022url\u0022:\u0022..\/kAvya\/\u0022},{\u0022title\u0022:\u0022संस्काराः\u0022,\u0022url\u0022:\u0022\/..\/saMskAra\/\u0022},{\u0022title\u0022:\u0022Vishvas\u0027s home page\u0022,\u0022url\u0022:\u0022\/..\/\u0022}],\u0022title\u0022:\u0022सङ्ग्रहान्तरम्\u0022},{\u0022title\u0022:\u0022\\u003ci class=\\\u0022fas fa-hand-holding-heart\\\u0022\\u003e\\u003c\/i\\u003eDonate Via Vishvas\u0022,\u0022url\u0022:\u0022https:\/\/vvasuki.github.io\/interests\/dharma-via-vishvas\/\u0022}],\u0022title\u0022:\u0022Parts\u0022}}");
    let autocompletePageUrl = "\/storage\/emulated\/0\/notesData\/notes\/data\/pages.tsv";
    
    var pageRelUrlTree = {};
</script>

    <script>
    
    let pageVars = {};
    pageVars.pageUrlMinusBasePath = "\/storage\/emulated\/0\/notesData\/notes\/computing\/numericalAnalysis\/dense_algebra\/".replace(basePath, "/");
    pageVars.pageParams = {};
    pageVars.pageSource = "computing\/numericalAnalysis\/dense_algebra.md";
    console.log(pageVars.pageSource);
    var pageDefaults;
    for (let possiblePageDefaults of pageDefaultsList) {
      if (pageVars.pageSource.startsWith(possiblePageDefaults.scope.pathPrefix)) {
        pageDefaults = possiblePageDefaults.values
      }
    }
    
    </script>
    <script src="/storage/emulated/0/notesData/notes/webpack_dist/main-bundle.js"></script>
    <script src="/storage/emulated/0/notesData/notes/webpack_dist/transliteration-bundle.js"></script>
    <script src="/storage/emulated/0/notesData/notes/non_webpack_js/disqus.js"></script>
    <script src="/storage/emulated/0/notesData/notes/webpack_dist/ui_lib-bundle.js"></script>
    <link rel="stylesheet" href="/storage/emulated/0/notesData/notes/css/fonts.css">
    <link rel="stylesheet" href="/storage/emulated/0/notesData/notes/css/@fortawesome/fontawesome-free/css/solid.min.css">
    

    <link rel="stylesheet" href="/storage/emulated/0/notesData/notes/css/@fortawesome/fontawesome-free/css/fontawesome.min.css">

    
    <link rel="alternate" hreflang="sa-Deva" href="#ZgotmplZ" />
    <link rel="alternate" hreflang="sa-Deva" href="#ZgotmplZ?transliteration_target=devanagari" />
    <link rel="alternate" hreflang="sa-Knda" href="#ZgotmplZ?transliteration_target=kannada" />
    <link rel="alternate" hreflang="sa-Mlym" href="#ZgotmplZ?transliteration_target=malayalam" />
    <link rel="alternate" hreflang="sa-Telu" href="#ZgotmplZ?transliteration_target=telugu" />
    <link rel="alternate" hreflang="sa-Taml-t-sa-Taml-m0-superscript" href="#ZgotmplZ?transliteration_target=tamil_superscripted" />
    <link rel="alternate" hreflang="sa-Taml" href="#ZgotmplZ?transliteration_target=tamil" />
    <link rel="alternate" hreflang="sa-Gran" href="#ZgotmplZ?transliteration_target=grantha" />
    <link rel="alternate" hreflang="sa-Gujr" href="#ZgotmplZ?transliteration_target=gujarati" />
    <link rel="alternate" hreflang="sa-Orya" href="#ZgotmplZ?transliteration_target=oriya" />
    <link rel="alternate" hreflang="sa-Beng-t-sa-Beng-m0-assamese" href="#ZgotmplZ?transliteration_target=assamese" />
    <link rel="alternate" hreflang="sa-Beng" href="#ZgotmplZ?transliteration_target=bengali" />
    <link rel="alternate" hreflang="sa-Guru" href="#ZgotmplZ?transliteration_target=gurmukhi" />
    <link rel="alternate" hreflang="sa-Cyrl" href="#ZgotmplZ?transliteration_target=cyrillic" />
    <link rel="alternate" hreflang="sa-Sinh" href="#ZgotmplZ?transliteration_target=sinhala" />
    <link rel="alternate" hreflang="sa-Shar" href="#ZgotmplZ?transliteration_target=sharada" />
    <link rel="alternate" hreflang="sa-Brah" href="#ZgotmplZ?transliteration_target=brahmi" />
    <link rel="alternate" hreflang="sa-Modi" href="#ZgotmplZ?transliteration_target=modi" />
    <link rel="alternate" hreflang="sa-Tirh" href="#ZgotmplZ?transliteration_target=tirhuta_maithili" />
    <link rel="alternate" hreflang="sa-Latn-t-sa-Zyyy-m0-iast" href="#ZgotmplZ?transliteration_target=iast" />
  </head>

  <body class="ma0 bg-near-white">
    


    
<header class="p-1 bg-yellow">
    <nav role="navigation">
      <div id="div_top_bar" class="row">
        <div class="col-md-3 justify-content-start">
          <a  href="/storage/emulated/0/notesData/notes/" class="btn col border">
            <i class="fas fa-gopuram ">Vishvas&#39;s notes <br/> Dense algebra</i>
          </a>
        </div>
        <div id="div_top_bar_right" class="col-md-auto d-flex justify-content-center">
          <form action="/storage/emulated/0/notesData/notes/search">
            <input id="titleSearchInputBox" placeholder="शीर्षिकान्विष्यताम्" name="s"><i class="btn fas fa-search "></i>
          </form>
          <a href="/storage/emulated/0/notesData/notes/custom_site_search/"  class="btn btn-default">Full search</a>
          <select name="transliterationDropdown" size="1" onchange="module_transliteration.updateTransliteration()">
            <option value="devanagari" selected="">स</option>
            <option value="iast">ā</option>
            <option value="kannada">ಅ</option>
            <option value="malayalam">അ</option>
            <option value="telugu">క</option>
            <option value="tamil_superscripted">க²</option>
            <option value="tamil_extended">க</option>
            <option value="grantha">𑌅</option>
            <option value="gujarati">અ</option>
            <option value="oriya">ଅ</option>
            <option value="assamese">অস</option>
            <option value="bengali">অ</option>
            <option value="gurmukhi">ਅ</option>
            <option value="cyrillic">пу</option>
            <option value="sinhala">අ</option>
            <option value="sharada">𑆑𑇀𑆰</option>
            <option value="brahmi">𑀅</option>
            <option value="modi">𑘦𑘻𑘚𑘲</option>
            <option value="tirhuta_maithili">𑒁</option>
          </select>
        </div>
        <div class="row col  d-flex justify-content-center">
          <div><a  name="previousPage" href="" class="btn btn-secondary">###<i class="fas fa-caret-left"></i></a></div>
          <div ><a name="nextPage" href="" class="btn btn-secondary"><i class="fas fa-caret-right"></i> ### </a></div>
        </div>
        <ul id="top-bar-right-custom" class="list-group list-group-horizontal">
        </ul>
      </div>
    </nav>
</header>

    
    <div class="row" name="contentRow">
      
      <aside class="col-md-3 card border " id="sidebar">
        <div id="sidebarTitle" class="card-title bg-light-gray border d-flex justify-content-between" >
          <a name="sidebarToggleLink" data-toggle="collapse" href="#sidebar_body" role="button" aria-expanded="true" aria-controls="sidebar_body" onclick="module_main.default.sidebarToggleHandler()">
            Menu <i class="fas fa-caret-down"></i></a>
        </div>
        <nav class="card-body p-0 collapse show" id="sidebar_body">
          <ul id="displayed_sidebar" class="list pl2 p-2 bg-yellow">
        </ul>
        </nav>
      </aside>
      <main class="col p-3" role="main">
        
<header class='border d-flex justify-content-between'>
    <h1 id="Dense algebra">Dense algebra</h1>
    
    <a id="editLink" class="btn btn-primary"  href="https://github.com/vvasuki/notes/edit/master/content/computing/numericalAnalysis/dense_algebra.md"><i class="fas fa-edit"></i></a>
    
</header>
<article>
  <aside id="toc_card" class="card border ">
    <div id="toc_header" class="card-title border d-flex justify-content-between">
        <a data-toggle="collapse" href="#toc_body" role="button" aria-expanded="true" aria-controls="toc_body">
          What's in this page? <i class="fas fa-caret-down"></i> </a>
    </div>
    <div id="toc_body" class="card-body collapse p-0">
      
      <ul id="toc_ul" class="list p-0">
      </ul>
    </div>
  </aside>
  <div id="post_content">
  <h2 id="decompositional-approach">Decompositional approach</h2>
<p>Easy to analyze stability. Can reuse decomposition to solve multiple instances of the problem: eg consider A = LU.</p>
<h2 id="condition-number-of-a-matrix">Condition number of a matrix</h2>
<h3 id="condition-number-of-ax-when-x-perturbed">Condition number of Ax when x perturbed</h3>
<p>\(k = \sup_{\change x} \frac{\norm{A\change x}}{\norm{\change x}}\frac{\norm{x}}{\norm{Ax}} = \norm{A}\frac{\norm{x}}{\norm{Ax}} \leq \norm{A}\norm{A^{-1}}\) or \(\leq \norm{A}\norm{A^{+}}\) if A not square.</p>
<p>So, condition of \(A^{-1}b\) when b perturbed, \(k = \norm{A^{-1}}\frac{\norm{Ax}}{\norm{x}} \leq \norm{A}\norm{A^{-1}}\).</p>
<h3 id="condition-number-of-matrix-wrt-norm">Condition number of matrix wrt norm</h3>
<p>\(k(A) = \norm{A}\norm{A^{-1}}\).</p>
<p>2 norm condition number: \(\frac{\sw_{1}}{\sw_{n}}\). So, here, \(k(A) = k(A^{T})\).</p>
<p>Ill conditioned matreces. Loose \(\log k(A)\) digits of accuracy: change 1 bit in \(\frac{\norm{\change x}}{\norm{x}}\), change in \(\frac{\norm{A\change x}}{\norm{Ax}}\) is worth \(\log k(A)\) bits.</p>
<h3 id="condition-of-htextxa-1binverse-problem-when-a-perturbed">Condition of \htext{\(x=A^{-1b\)}{inverse problem} when A perturbed}</h3>
<p>\((A+\change A)(x+\change x) = b\). So, ignoring \(\change A \change x\), \(\change x = -A^{-1}(\change A)x\). By Cauchy Schwartz, \(\norm{\change x} = \norm{A^{-1}}\norm{(\change A)}\norm{x}\). So, \(k = sup_{\change A}\frac{\norm{\change x }\norm{A}}{\norm{\change A}\norm{x}} \leq \norm{A}\norm{A^{-1}}\).</p>
<h2 id="solving-axb-for-x">Solving Ax=b for x</h2>
<h3 id="assumption">Assumption</h3>
<p>\(b \in range(A)\).</p>
<h3 id="stability-and-expense">Stability and expense</h3>
<p>Using SVD is most reliable, but expensive. LU is cheapest, QR is more expensive, but more reliable. Usually, look at condition number k(A) and decide method.</p>
<h3 id="triangularization-by-row-elimination">Triangularization by row elimination</h3>
<h4 id="using-palu">Using PA=LU</h4>
<p>Make augmented matrix \(X = \mat{ A &amp; b}\) corresponding to problem Ax = b; Use row operations to reduce X to \(\mat{ U &amp; L^{-1}b}\): thence get the problem \(U x = L^{-1}b\); do back substitution: to get : \(\mat{ I &amp; UL^{-1}b}\) corresponding to problem \(Ix = UL^{-1}b\). Thus got \(x_r \in range(A^{T})\).</p>
<p>Then, to get set of all solutions: \(\set{x_r + x_n: x_n \in N(A)}\).</p>
<h4 id="cost">Cost</h4>
<p>If you have L,U: Solve LUx = b by solving Ly = b: forward substitution: \(O(m^{2} flops)\); then solving Ux=y :back substitution: \(O(m^{2} flops)\).</p>
<h4 id="stability">Stability</h4>
<p>Back substitution is stable.</p>
<h3 id="with-aqr">With A=QR</h3>
<h4 id="using-householder-reflections">Using Householder reflections</h4>
<p>Get A=QR; get \(y = Q^{*}b\); get \(x = R^{-1}y\). For stability, householder reflections used to get A=QR.</p>
<p>Note: \(y = Q^{<em>}b\) implicitly found as part of using householder reflections to get \(R = Q^{</em>}A\) by instead doing \(\mat{R &amp; y} = Q^{*}\mat{A &amp; b}\).</p>
<h4 id="backward-stability">Backward stability</h4>
<p>For \(\frac{\norm{\Del A}}{\norm{A}} = O(\eps)\), \((A+ \Del A)\hat{x} = b\): use \(A + \del A= \tilde{Q}\tilde{R}\) from backward stability of Householder; \((\tilde{Q}+\del Q)y = b\); \((\tilde{R} + \del R)\hat{x} = y\); set \(\Del A = \del A + \del Q\tilde{R} + \tilde{Q}\del R\).</p>
<p>So accuracy: \(\frac{\norm{\del x}}{\norm{x}}=k(A)\eps\).</p>
<h3 id="use-svd">Use SVD</h3>
<p>Take \(Ax = U\SW V^{*} x = b\), each of these is easy to invert. Costly, but very accurate: \(\SW\) reveals numerical rank, in case of very small \(sw_{i}\), drop the corresponding \(u_{i}\) or \(v_{i}\) and get a well conditioned problem.</p>
<h3 id="use-determinants-impractical">Use determinants (Impractical)</h3>
<p>(Cramer) Let C be cofactor matrix : \(C_{i,j}\) multiple of \(A_{i,j}\) in \(|A|\) formula; \(C^{T}Ax = C^{T}b\); so \(x = \frac{C^{T}b}{det(A)}\); so for \(x_{j}\), find jth row of \(C^{T}\) (= cofactors of jth col of A) times b = det (A where b replaces jth col).</p>
<h3 id="find-the-inverse">Find the inverse</h3>
<p>Find \(A^{-1}b\): A change of basis operation, saying b in terms of C(A) rather than \(e_{i}\)&rsquo;s.</p>
<h3 id="for-hermitian-ve-semidefinite-htextxaa">For Hermitian +ve semidefinite \htext{\(X=A^{*A\)}{..}}</h3>
<h4 id="use-lu-cholesky">Use LU/ Cholesky</h4>
<p>Use \(X = R^{*}R\). Time needed to solve two triangular systems: \(O(m^{2})\); time needed to make R: \(O(m^{3}/3)\).</p>
<h5 id="stability-1">Stability</h5>
<p>Diagonal heavy if \(X \succ 0\): so no pivoting required. \why Cholesky alg to make R backward stable. So, \((A+\del A) \tilde{x} = b\) for relatively small \(\del A\).</p>
<h4 id="avoiding-htextxaa-if-a-known">Avoiding \htext{\(X=A^{*A\)}{..} if A known}</h4>
<p>In doing \(A^{*}A\), you square the condition number. So, if A is ill conditioned, you get an even more ill conditioned problem.</p>
<p>So, don&rsquo;t do this. Instead, replace A with LU or QR or \(U\SW V^{*}\) and solve.</p>
<h4 id="use-aqr-if-a-known">Use A=QR, if A known</h4>
<p>Get \(A=\hat{Q}\hat{R}\) (\(2mn^{2} - \frac{2}{3}n^{3}\) flops), solve \(Rx=\hat{Q}^{*}b\). More numerically stable than Cholesky \why.</p>
<p>Get \(A=\hat{Q}\hat{R}\), so \(A^{<em>}A = \hat{R}^{</em>}\hat{R}\) (Cholesky: LU for \(A^{<em>}A\)); get \(\hat{R}^{</em>}\hat{R}x = A^{<em>}b\). Implementation: solve \(\hat{R}^{</em>}w=A^{<em>}b\), then \(Rx=w\). Finding \(A^{</em>}A\) + Cholesky = \(mn^{2} + \frac{n^{3}}{3}\) flops.</p>
<h4 id="diagonalize-and-solve">Diagonalize and solve</h4>
<p>Find thin SVD (\(2mn^{2} + 11n^{3}\) flops): \(A=\hat{U}\hat{\SW}V^{<em>}\): \(P=A(A^{</em>}A)^{-1} A^{<em>} = \hat{U}\hat{U}^{</em>}\); so \(\hat{U}\hat{\SW}x = \hat{U}\hat{U}^{<em>}b\), \(\hat{\SW}V^{</em>}x = \hat{U}^{*}b\); very dependable.</p>
<h2 id="iteratively-solve-axb">Iteratively solve Ax=b</h2>
<h3 id="get-x-one-component-at-a-time">Get x one component at a time</h3>
<p>Take A = D + L + U, with D diagonal, L/ U strictly lower/ upper triangular.</p>
<h4 id="using-updated-components-immediately">Using updated components immediately</h4>
<p>Aka Gauss Siedel.</p>
<p>Take \(x^{(k)}\), get \(x^{(k+1)}\) by this:\<br>
find \(x^{(k+1)}<em>{j}\) using \(A, b, x^{(k+1)}</em>{1:j-1}, x^{(k)}_{j+1:n}\). So, \(Dx^{k+1} = b - Lx^{k+1} - Ux^{k}\); \((D+L)x^{k+1} = b - Ux^{k}\).</p>
<p>Not guaranteed to converge.</p>
<h4 id="use-only-old-guesses-of-x">Use only old guesses of x</h4>
<p>Aka Jacobi iteration.</p>
<p>Take \(x^{(k)}\), get \(x^{(k+1)}\) by doing this: find \(x^{(k+1)}<em>{j}\) using \(A, b, x^{(k)}</em>{i\neq j}\) : so using old iterates of x uniformally rather than some old and some new iterates in finding \(x^{(k+1)}_{j}\). So, \(Dx^{k+1} = b - Lx^{k} - Ux^{k}\).</p>
<p>Guaranteed to converge.</p>
<h3 id="using-htexti-a-1neumann-series-for-square-a">Using \htext{\((I-A)^{-1\)}{Neumann} series for square A}</h3>
<p>(Neumann) See linear algebra ref. Can be used in solving \(A&rsquo;x = b\) where \(A&rsquo; = (I-A)\).</p>
<h2 id="overdetermined-system-of-equations-least-squares-solution">Overdetermined system of equations: Least squares solution</h2>
<h3 id="problem">Problem</h3>
<p>Solve \(\min_x \norm{Ax-b}\), given \(m&gt;n\), b not in C(A). Error vector \(e=A\hat{x}-b\). We want to \(\min_x \norm{e}^{2}\).</p>
<p>For versions with regularizers, weights etc.. see optimization ref.</p>
<h4 id="importance">Importance</h4>
<p>Solving this for the case where 2 norm is used in the problem specification \(\equiv\) orthogonal projection.</p>
<p>Useful also in linear regression; in that context also the maximum likelihood solution for data generated by a linear fn in the presense of Gaussian noise: see statistics, optimization ref.</p>
<h3 id="solution">Solution</h3>
<h4 id="projection--inverse-operation">Projection + inverse operation</h4>
<p>Two steps: First, project \(b\) to \(range(A)\); that is, find \\(b&rsquo; = \argmin_{v \in ran(A)} \norm{b-v}\). Then, solve \(Ax = b&rsquo;\).</p>
<p>When error magnitude is measured wrt some other norms, other, perhaps oblique projections may be needed.</p>
<h3 id="solution-form-2-norm-minimization">Solution form: 2-norm minimization</h3>
<p>\(\norm{Ax-b}^{2} = (Ax-b)^{<em>}(Ax-b)\), set \(\gradient f(x) = 0\) to find minimum. This is equivalent to solving \<br>
\(A^{</em>}(b-A\hat{x})=0\) or \(A^{<em>}A\hat{x}=A^{</em>}b\) (Normal equations).</p>
<h4 id="as-orthogonal-projection--inverse">As orthogonal projection + inverse</h4>
<p>Note that the condition \(A^{*}(b-A\hat{x}) = 0\) matches the condition from geometric intuition that the error vector \(e \perp ran(A)\).</p>
<h4 id="solution-algorithm">Solution algorithm</h4>
<p>The projection and inversion can either be done together or separately. In the former case, projection can be accomplished by finding an orthogonal basis for \(ran(A)\) using either QR or SVD.</p>
<h5 id="non-singular-a">Non-singular A</h5>
<p>\((A^{<em>}A)^{-1}\) invertible. \(\hat{x}=(A^{</em>}A)^{-1}A^{*}b, p = A\hat{x}\).</p>
<p>\(P=A(A^{<em>}A)^{-1} A^{</em>}\). \((A^{<em>}A)^{-1}=A^{-1}(A^{</em>})^{-1}\) iff A is square and both exist.</p>
<h6 id="pseudoinverse-for-non-singular-a">Pseudoinverse for non-singular A</h6>
<p>See linear algebra ref.</p>
<h5 id="rank-deficient-a">Rank deficient A</h5>
<p>\((A^{<em>}A)^{-1}\) not invertible, rank deficient. \pf \(N(A^{</em>}A) = N(A)\): \(A^{<em>}Ax = 0 \implies x^{</em>}A^{*}Ax = \norm{Ax} = 0\). I-P projects to \(N(A^{T}\)) (not N(A)): \((I-P)b=e\).</p>
<h6 id="solution-1">Solution</h6>
<p>\(A^{<em>}A\hat{x}=A^{</em>}b\) has many solutions for \(x\). This is equivalent to the solution obtained by projecting \(b\) to \(ran(A)\) to get \(b&rsquo;\) and then solving  \(Ax = b&rsquo;\).</p>
<h2 id="triangularization-by-row-elimination-1">Triangularization by row elimination</h2>
<p>Aka Gaussian elimination. Get PA=LDU where L is unit lower triangular, D is diagonal, U is unit upper triangular. This is unique: see linear algebra ref.</p>
<h3 id="algorithm">Algorithm</h3>
<p>In step i, subtract multiples of row \(A_{i,:}\) from rows \(A_{i+1:m,:}\) so as to make \(A_{i+1:m, i}\) subcolumn 0. These row operations correspond to doing \(L^{-1}A = U\) to get U from A.</p>
<h4 id="pivoting">Pivoting</h4>
<p>But, maybe \(A_{i,i} = 0\). In this case, we need to bring row k with \(A{k,i} \neq0\) in place of row i for the algorithm to proceed.</p>
<p>Pivoting is also needed for stability of the algorithm.</p>
<h4 id="cost-1">Cost</h4>
<p>Find L, U (\(\frac{2m^{3}}{3}\) flops).</p>
<h4 id="various-formulations">Various Formulations</h4>
<p>Reordering the loops: ijk version puts 0&rsquo;s column-wise. ikj version puts 0&rsquo;s row-wise, has good storage properties \why.</p>
<h5 id="reducing-memory-usage">Reducing memory usage</h5>
<p>Overwrite A with L and U.</p>
<h5 id="outer-product-formulation">Outer product formulation</h5>
<p>\(A_{2:m,2:m}-\frac{A_{2:m,1}A_{1,2:m}^{*}}{a_{1,1}}\).</p>
<h5 id="reducing-memory-access">Reducing memory access</h5>
<p>Let \(l_{k} = 0+A_{k+1:m,k}\) after k-1 steps of triangularization; then k+1th step of traing is to make \(L^{-1}_{k} = I-l_{k}e_{k}^{*}; A=L^{-1}_{k}A\). Reducing A to U is to reduce it to \(\mat{U_{1,1} &amp; U_{1,2}\ 0 &amp; U_{2,2}}\); can do this repeatedly. So make yer alg use this decomposition, and block \(\times\) as basic ops, to avoid having to get each col to cache.</p>
<h3 id="instablity-when-no-pivoting">Instablity when no pivoting</h3>
<p>\(A=\mat{10^{-20} &amp; 1\ 1 &amp; 1}\) yields \(\tilde{L}=\mat{1 &amp; 0\ 10^{20} &amp; 1}\) and \(\tilde{U}=\mat{10^{-20} &amp; 1\ 0 &amp; -10^{20}}\) with \(A-\tilde{L}\tilde{U}\) big: so stable, but not backward stable (considering functions of m or A in error bound).</p>
<p>Instability when, resulting from pivot very small wrt other elements in A, element t in \(\tilde{L}\) or \(\tilde{U}\) huge: relative error \(O(\eps)\) but abs error \(O(\eps t)\).</p>
<p>\(|\Del A| \leq 3n\eps|L||U|\). \why \(\tilde{L}\tilde{U} = A + \Del A =LU + \Del A, \frac{\norm{\Del A}}{\norm{L}\norm{U}} = O(\eps)\).</p>
<h3 id="partial-pivoting-gepp">Partial pivoting (GEPP)</h3>
<p>At step k, pivot selected as biggest element in \(A_{k:m,k}\). Take \(\prod L_{i}P_{i}A = U\) where \(L_{i}\) are atomic unit lower triangular, use \(PLP^{*} = L&rsquo;\) (row exchange in L = col exchange in L&rsquo;), get PA = LU.</p>
<h4 id="stability-of-gepp">Stability of GEPP</h4>
<p>\(L_{i,j} \leq 1; \norm{L} =O(1)\); let Growth Factor \(\rho = \frac{\max |U_{i,j}|}{\max |A_{i,j}|}\); \(\norm{U} = O(\rho \norm{A})\); so \(\tilde{L}\tilde{U} = \tilde{P}A + \Del A, \frac{\norm{\Del A}}{\norm{A}} = O(\rho \eps)\).</p>
<p>Maximal instability: \(\rho = 2^{m-1}\): Eg: \(A=\mat{1 &amp; 1\ -1 &amp; 1}\); m-1 digits of accuracy lost. Unstability occurs very rarely; usually \(\rho \leq m^{-0.5}\). Used in Matlab \ op.</p>
<h3 id="complete-pivoting">Complete pivoting</h3>
<p>At step k, pivot selected as biggest element in \<br>
\(A_{k:m,k:m}\). \(O(m^{3}) = \sum i^{2}\) flops: expensive. \(PAP&rsquo; = LU\). Stable.</p>
<h3 id="avoidance-of-pivoting">Avoidance of pivoting</h3>
<p>If X is +ve definite, no pivoting required: As all principle submatrices are +ve definite - so non-singular, X = LU exists.</p>
<p>Also, if X is diagonally dominant, diagonal dominance is preserved during triangularization. So, it does not require pivoting.</p>
<h3 id="formula-for-pivots">Formula for pivots</h3>
<p>Let \(A_{k}\) be submatrix of first k*k elements; then from block multiplication, \(P_{k}A_{k} = L_{k}D_{k}U_{k}\) holds; so pivots can also be found by \(\frac{|D_{k}|}{|D_{k-1}|} = \frac{|A_{k}|}{|A_{k-1}|}\).</p>
<h3 id="symmetric-elimination-algorithm-for-spd-a">Symmetric Elimination Algorithm for spd A</h3>
<p>Do Gaussian elimination + extra column ops to diagonalize/ maintain symmetry at each step.</p>
<p>$A =
\mat{a_{1,1} &amp; A_{2,1}^{*}\<br>
A_{2,1} &amp; A_{2,2}}
= \mat{1 &amp; 0\<br>
\frac{A_{2,1}}{a_{1,1}} &amp; I}
\mat{a_{1,1} &amp; 0\<br>
0 &amp; A_{2,2}-\frac{A_{2,1}A_{2,1}^{*}}{a_{1,1}}}
\mat{1 &amp; \frac{A_{2,1}}{a_{1,1}}\<br>
0 &amp; I}
=LDL^{*}
\(. Get \)R^{*}R\( by doing \)LD^{1/2}$ at each step.</p>
<h4 id="code-and-opcount">Code and Opcount</h4>
<p>R=A; Repeat: do symmetric elimination on submatrix \(R_{i+1,i+1}\); do \(R_{i}^{*}/\sqrt{r_{i,i}}\). Only Upper part of R stored.</p>
<p>Opcount: \(\sum_{k=1}^{m} \sum_{j=k+1}^{m} 2(m-j) \approx \frac{m^{3}}{3}\) flops.</p>
<h4 id="stability-2">Stability</h4>
<p>By SVD: \(\norm{R}<em>{2} = \norm{R^{*}}</em>{2} = \norm{A}_{2}^{1/2}\); \(so \norm{R} \leq \sqrt{m}\norm{A}\) \chk. So, R never grows large. So, backward stable : get \(\hat{R}<em>\hat{R}\) for perturbed A. Forward error in R large; but R and \(R^{</em>}\) diabolically correlated.</p>
<h2 id="aqr">A=QR</h2>
<h3 id="triangular-orthonormalization">Triangular orthonormalization</h3>
<p>Take the m*n matrix A; arrive at the matrix \(Q_n = A\hat{R}\). At step j, you have \(Q_{:,1:j-1} = Q_{j-1}\) find the direction of \(q_j\) by removing the component of \(a_j\) in the subspace spanned by \(Q_{j-1}\).</p>
<p>Thence, you arrive at reduced QR: \(A=Q_n\hat{R}\), where \(Q_n\) is a m*n matrix. Can extend Q thence to be a square matrix: this is full QR.</p>
<p>So, QR \(\exists \forall A\). If sign(\(R_{i,i}\)) is fixed to be +ve, QR unique.</p>
<h4 id="gram-schmidt-classical">Gram-Schmidt classical</h4>
<p>At step j: Take \(a_j-\hat{Q}<em>{j-1}\hat{Q^{*}}</em>{j-1}a_j\) and normalize it to get \(q_j\), with \(\hat{Q}_{j-1} = [q_1 .. q_j]\).</p>
<p>\(2mn^{2}\) flops.</p>
<h5 id="instability">Instability</h5>
<p>Even by the time it calculates \(q_{20}\), error becomes unbearable.</p>
<h5 id="double-gram-schmidt">Double gram-schmidt</h5>
<p>Get A = QR, then do Q = Q&rsquo;R&rsquo; to re-orthogonalize Q. A surer way of getting orthogonal basis for range(A).</p>
<h4 id="gram-schmidt-modified-mgs">Gram-Schmidt Modified (MGS)</h4>
<p>At step j: Remove the component of \(a_j\) first in the subspace \(\linspan{q_1}\), then remove the component of the residue from \(\linspan{q_2}\) and so on. Algebraically same as classical version: \(Q_{j-1} = \sum_1^{j-1}q_iq_i^{*}\). Computational cost same as classical version.</p>
<h5 id="round-off-error">Round off error</h5>
<p>very small angle twixt \(q_{1}, a_{2}\); so \(q&rsquo;_{2}=a_{2}-q_{1}q_{1}*a_{2}\) very small, with smaller error (maybe \(10^{-15}\) in \(q&rsquo;_{2,j}\) wrt \(q&rsquo;_{2,k}\)); err amplified maybe \(10^{10}\) times when \(q_{2}\) made after normalization. MGS has lesser roundoff error: \why.</p>
<h3 id="orthogonal-triangularization">Orthogonal triangularization</h3>
<p>(Householder) Do \(Q^{<em>}A = \hat{R}\), not \(A\hat{R}^{-1} = \hat{Q}\). Init: R=A; \(Q^{</em>} = Q_{n} .. Q_{1}\); \(Q_{k} = \mat{I_{k-1} &amp; 0 \ 0 &amp; F}\) leaves \(r_{1} .. r_{k-1}\) and \(r_{1}^{*} .. r_{k-1}^{*}\) alone, Householder reflector F reflects x (last m-k+1 entries in \(r_{k}\)) to \(\norm{x}e_{1}\); last entries in \(r_{k+1} .. r_{n}\) elsewhere. F reflects accross plane \(\perp v = x - \norm{x}e_{1}\) or \(\perp v&rsquo; = -x - \norm{x}e_{1}\); so by geometry, \(\frac{vv^{*}}{\norm{v}}x\) is projection on v; \(\norm{x}e_{1} = Fx = (I - 2\frac{vv^{*}}{\norm{v}})x\).</p>
<p>To avoid catastrophic cancellation when \(x - \norm{x}e_{1}\) very small, choose \(v = -sign(x_{1})x - \norm{x}e_{1}\). \(F^{*} = F, so Q_{k}^{*}=Q\). Needs: \(2mn^{2} - \frac{2}{3}n^{3}\) flops.</p>
<h4 id="stability-of-finding-q-r">Stability of finding Q, R</h4>
<p>Calculated \(\hat{Q}\) and \(\hat{R}\) can have large forward errors; but they&rsquo;re diabolically correlated; backward error or residual \(A -\hat{Q}\hat{R}\) very small. \(A + \Del A = \hat{Q}\hat{R}\) for \(\frac{\norm{\Del A}}{\norm{A}} = O(\eps)\). So, backward error analysis best way to proceed.</p>
<h2 id="find-eigenvalues">Find eigenvalues</h2>
<h3 id="hardness">Hardness</h3>
<p>Can&rsquo;t get directly for \(m\geq 5\): thm from Galois theory that roots can&rsquo;t be expressed as radicals etc.. So, iterative part necessary in alg. 2 phases: Preliminary reduction to structured form; then iterative part.</p>
<h4 id="usual-approach">Usual approach</h4>
<p>Easily reduce to almost-triangular form using similarity transformations. Then, use more similarity transformations to iteratively get close to triangular form.</p>
<h4 id="finding-a-single-ew">Finding a single ew</h4>
<p>(Dhillon) If you have an idea about the approximate size of the ew, you can find it in \(O(n^{2})\). Else, if ye need kth ew, it takes \(O(kn^{2})\) time.</p>
<p>For sparse A, it is much cheaper. \tbc</p>
<h3 id="use-characteristic-polynomial">Use characteristic polynomial</h3>
<p>Find roots using rootfinder. So, every ew has one non 0 ev. Ill conditioned. Eg: If coefficients in \(x^{2}+2x-1\) change by \(\eps\), x changes by \(O(\sqrt{\eps})\).</p>
<h3 id="reduction-to-upper-hessenberg-matrix-h">Reduction to Upper Hessenberg matrix H</h3>
<p>0&rsquo;s below first sub diagonal. If \(A=A^{*}\), get Tridiagonal matrix. Use Householder reflections: \<br>
\(H = (\prod_{i} Q_{m-1-i}^{*})A (\prod^{m-2}_{i=1} Q_{i})\).</p>
<p>These are similarity transformations, so \(\ew(H) = \ew(A)\).</p>
<p>For large sparse matreces: Use Arnoldi iteration.</p>
<h4 id="op-count">Op count</h4>
<p>Row ops, at 4 flops per num: \(\frac{4 m^{3}}{3}\); Col ops, at 4 flops per num: \(2 m^{3}\); total: \(\frac{10 m^{3}}{3}\). Reduced work if \(A=A^{*}\): \(\frac{4 m^{3}}{3}\).</p>
<h4 id="stability-3">Stability</h4>
<p>\(\tilde{Q}\tilde{H}\tilde{Q}^{*} = A + \del A\) for relatively small \(\del A\).</p>
<h3 id="approach-eigenvalue-revealing-factorizations">Approach Eigenvalue revealing factorizations</h3>
<p>$A = QUQ^{*}; \<br>
U = ..Q_{2}^{*}Q_{1}^{*}AQ_{1}Q_{2}..\(. If \)A=A^{*}$, this leads to unitary diagonalization.</p>
<h3 id="power-iteration-for-real-symmetric-a">Power iteration for real symmetric A</h3>
<p>The series \(v^{(i)} = \frac{A^{i}x}{\norm{A^{i}x}}\) and \(l^{(i)} = r(v^{(i)})\) converge to eigenpair corresponding to largest ew \(\ew_{1}, q_{1}\): as \(x = \sum a_{i}q_{i}\).</p>
<p>So, Applying A repeatedly takes x to dominant ev.</p>
<h4 id="convergence">Convergence</h4>
<p>Linear convergence of ev. $\norm{v^{(i)} - \pm q_{1}} = O(|\frac{\ew_{2}}{\ew_{1}}|^{i}),\<br>
\norm{\ew^{(i)} - \pm \ew_{1}} = \norm{v^{(i)} - \pm q_{1}}^{2}$.</p>
<h3 id="inverse-iteration">Inverse iteration</h3>
<p>ev of A and \((A-pI)^{-1}\) same, ew \(\ew_{i}\) shifted and inverted to get ew \((\ew_{i} - p)^{-1}\). If p near \(\ew_{j}\), using power iteration on \((A-pI)^{-1}\) gives fast convergence.</p>
<p>Good for finding ev if ew already known.</p>
<h4 id="convergence-1">Convergence</h4>
<p>Linear convergence of ev. \<br>
\(\norm{v^{(i)} - \pm q_{j}} = O(|\frac{p-\ew_{j}}{p-\ew_{k}}|^{i}),\ \norm{\ew^{(i)} - \pm \ew_{1}} = \norm{v^{(i)} - \pm q_{j}}^{2}\).</p>
<h4 id="alg">Alg</h4>
<p>Solve \((A-pI)w = v^{(k-1)}\); normalize to get \(v^{(k)}\).</p>
<h3 id="rayleigh-quotient-iteration">Rayleigh quotient iteration</h3>
<p>Inverse iteration, where \(\ew^{(i)} = R(v^{(i)})\) used as p (ew estimate).</p>
<h4 id="convergence-2">Convergence</h4>
<p>Cubic convergence of ev and ew. If \(\norm{v^{(k)} - q_{j}} \leq eps\) when \(|\ew^{(k)} - \ew_{j}| \leq O(\eps^{2})\). So \(\norm{v^{(k+1)} - q_{j}} = O(|\ew^{(k)} - \ew_{j}|\norm{v^{(k)} - q_{j}}) =\ O(\norm{v^{(k)} - (\pm q_{j})}^{3})\). \(|\ew^{(k+1)} - (\ew_{j})| = O(\norm{v^{(k+1)} - q_{j}}^{2}) = O(|\ew^{(k)} - (\pm q_{j})|^{3})\).</p>
<p>Gain 3 digits of accuracy in each iteration.</p>
<h3 id="simultaneous-iteration-for-real-symmetric-a">Simultaneous iteration for real symmetric A</h3>
<p>Aka Block power itern. \(\tuple{v_{i}}\) linearly independent; their matrix \(V^{(0)}\). \(\tuple{q_{i}}\) orth ev of A; cols of \(\tilde{Q}\).</p>
<p>Unstable. \why</p>
<h4 id="convergence-3">Convergence</h4>
<p>If \(|\ew_{1}| &gt; .. &gt; |\ew_{n}| \geq |\ew_{n+1}|..\), Orth basis of \(\linspan{A^{k}v_{1}^{(0)}, .. A^{k}v_{n}^{(0)}}\) converges to \(\linspan{q_{1}, .. q_{n}}\): take \(v_{i} = \sum_{j}a_{j}q_{j}\), do power iteration.</p>
<h4 id="alg-1">Alg</h4>
<p>Take some \(Q^{0} = I\) or other orth cols, get \(Z = AQ^{(k-1)}\); get \(Q^{(k)}R^{(k)} = Z\). Defn: \(A^{(k)} = (Q^{(k)})^{T}AQ^{(k)}\), \(R'^{(k)} = \prod R^{(k)}\).</p>
<p>\(A^{k} = Q^{(k)}R'^{(k)}\): By induction: \(A^{k} = AQ^{(k-1)}R'^{(k-1)} = Q^{(k)}R'^{(k)}\).</p>
<h3 id="qr-iteration">QR iteration</h3>
<p>Not QR factorization. Get \(Q^{(k)}R^{(k)} = A^{(k-1)}\);\<br>
\(A^{(k)}=R^{(k)}Q^{(k)} = (Q^{(k)})^{T}A^{(k-1)}Q^{(k)}\): Similarity transformation. Works for all A with distinct \(|\ew_{i}|\); easy analysis for \(A=A^{T}\).</p>
<p>Defn: \(R'^{(k)} = \prod R^{(k)}\), \(Q'^{(k)} = \prod_{k} Q^{(k)}\): same as \(Q^{(k)}\) in Simult itern alg.</p>
<p>\exclaim{Stable - finding \(\ew\) now routine!}</p>
<h4 id="convergence-for-real-symmetric-a">Convergence for real symmetric A</h4>
<p>Same as Simultaneous iteration starting with I. \<br>
\(A^{k} = Q^{(k)}R'^{(k)}\): So, finds orth bases for \(A^{k}\).</p>
<p>\(A^{(k)} = (Q'^{(k)})^{T}AQ'^{(k)}\); \(A^{(k)}<em>{i,i}\) are \(R(Q'^{(k)}</em>{i})\); as \(Q'^{(k)}<em>{i}\) converges, \(A^{(k)}</em>{i,i} \to \ew_{i}\), off diagonal entries tend to 0; so approaches Schur factorization.</p>
<p>Linear convergence rate: \(max_{j}\frac{\ew_{j+1}}{\ew_{j}}\).</p>
<h3 id="modified-qr-alg">Modified QR alg</h3>
<p>Tridiagonalize: \((Q^{(0)})^{T}A^{(0)}Q^{(0)} = A\). Pick shift \(p^{(k)}\); get \(Q^{(k)}R^{(k)}=A^{(k-1)} - p^{(k)}I\); get \(A^{(k)} = R^{(k)}Q^{(k)} + p^{(k)}I\); if any off diagonal element is close to 0, take \(\mat{A_{1} &amp; 0 \ 0 &amp; A_{2} }\), deflate and apply QR.</p>
<p>Needs O(m) iterations costing \(O(m^{2})\) iterations each. \why</p>
<h3 id="stability-4">Stability</h3>
<p>Aka Eigenvalue perturbation theory.</p>
<p>\part{Sparse, large matrix algebra}</p>
<h2 id="iterative-linear-algebra-methods">Iterative Linear algebra methods</h2>
<p>Unlike direct methods. To solve Ax=b and Ax=lx. Eg: Conjugate gradient, Lanczos, Arnoldi.</p>
<h3 id="exploiting-sparsity-use-only-ax">Exploiting sparsity: Use only Ax</h3>
<p>Use black box access to methods which find Ax and \(A^{*}x\); minimize these calls. Thus, take advantage of sparsity.</p>
<h4 id="flops">Flops</h4>
<p>Dense computations: O(m) steps, \(O(m^{2})\) ops per step, total flops \(O(m^{3})\): Also worst case for Iterative methods. But generally O(m) or \(O(m^{2})\).</p>
<h4 id="accuracy">Accuracy</h4>
<p>Converge geometrically to \(\eps_{mach}\); direct methods make no progress until all \(O(m^{3})\) are completed.</p>
<h3 id="krylov-sequences-and-subspaces-of-a-and-b">Krylov sequences and subspaces of A and b</h3>
<p>\(b, Ab, A^{2}b ..\). Krylov subspaces \(K_{r}(A, b)\) are spanned by successively larger groups of these. Can also form Krylov matrix. Orthogonalization (perhaps Gram Schmidt style) used between iterations in order to avoid erroneous linear dependence.</p>
<p>Convergence rate depends on spectral properties of A.</p>
<p>Analysis closely related to approximation of f(x) by polynomials of on subsets of the complex plane. \why</p>
<h3 id="projection-of-a-to-krylov-subspaces">Projection of A to Krylov subspaces</h3>
<p>Reduces the problem to problems in \(1, 2 ..\) dimensions. Look at the action of A in \(Ax=b\) or \(Ax = lx\), restricted to Krylov subspace. Approximation gets closer as number of iterations \(n \to m\).</p>
<h3 id="iteratively-find-ortho-basis-of-ka-b">Iteratively find ortho-basis of K(A, b)</h3>
<h4 id="triangular-orthogonalization">Triangular orthogonalization</h4>
<p>(Arnoldi).
Direct method used Orthogonal triangularization (Householder); Now use Triangular Orthogonalization (Gram Schmidt) : Can be stopped in middle for partial solution \(Q_n\). Also, use a trick: Take current orthogonal basis Q, set next column \(q_{i+1}\) = the normalized component of \(Aq_i \perp (q_1 .. q_i)\). Thus, \(q_{i+1} \perp [q_0\ Aq_0\ ..\ A^{i - 1}q_0]\) also.</p>
<h4 id="description-using-h">Description using H</h4>
<p>Start with arbit b; normalize to get \(q_{1}\); for any n, for \(j \leq n\) get: \(h_{j,n} = \dprod{q_{j},Aq_{n}}\); get: \(h_{n+1,n} = \norm{v} = \norm{Aq_{n} - \sum_{j=1}^{n}h_{j,n}q_{j}}\): Do the subtraction mgs style when each \(h_{j,n}\) is found; find \(q_{n+1} = \frac{v}{h_{n+1,n}}\). \(Aq_{n} = Q_{n+1}h_{n} = \sum_{i=1}^{n+1} q_{i}h_{i, n}\).</p>
<p>So, you have almost upper triangular/ upper hessenberg \((n+1)\times n: \hat{H}<em>n\). Get \(AQ</em>{n} = Q_{n+1}\hat{H}_{n}\).</p>
<h4 id="square-h">Square H</h4>
<p>Take \(\hat{H}\) cut to \(n\times n: H_{n}\), the Ritz matrix; get \(H_{n} = Q_{n}^{*}AQ_{n}\);  Also: \(H_{n} = Q_{n}^{*}Q_{n+1}\hat{H}_{n}\).</p>
<p>\exclaim{As if going towards Similarity transformation of A to \htext{\(H = QAQ^{<em>}\)}{..}!} Maybe \(m \to \infty\): So maybe only solving for first \(n&lt;m\) cols of Q in \(Q^{</em>}AQ=H\).</p>
<h5 id="htexth_n-as-projection-of-a-to-krylov-subspaces">\htext{\(H_{n\)}{..} as Projection of A to Krylov subspaces}</h5>
<p>Representation in basis \(\set{q_{1} .. q_{n}}\) of the orthogonal projection of operator A onto \(K_{n}\): The shadow of operation of A in \(K_{n}\). Consider operator \(K_{n} \to K_{n}\): Take \(v = Q_{n}x \in K_{n}\): so x in basis \(Q_n\); apply A: \(A Q_n x\); project Av back to \(K_{n}\): \(Q_n^{*} A Q_n x = H_n x\).</p>
<p>Can&rsquo;t get operator A back from \(H_{n}\).</p>
<h4 id="hermitian-case-tridiagonal-form">Hermitian case: tridiagonal form</h4>
<p>(Lanczos): Arnoldi iteration for \(A = A^{*}\). \(H_{n}\) becomes tridiagonal \(T_{n}\) with \(\set{a_{i}}\) in diagonal and \(\set{b_{i}}\) in 1st super and sub diagonals. 3 term recurrance: \(Aq_{n} = b_{n-1}q_{n-1} + a_{n}q_{n}+ b_{n}q_{n+1}\).</p>
<h4 id="reduction-of-arbit-a-to-tridiagonal-form">Reduction of arbit A to tridiagonal form</h4>
<p>If A symmetric, \(A = QTQ^{*}\) for tridiagonal T; if A assymetric gotto give up orthogonality or tridiagnoalness: so find \(A = VTV^{-1} = VTW^{T}\) for tridiagonal but non symmetric T with diagonals \(\set{d_{2}, ..}, \set{a_{1}, ..}, \set{b_{2}, ..}\). Solve AV = VT and \(W^{T}A = TW^{T}\): 3 term recurrances.</p>
<h2 id="eigenvalue-estimation">Eigenvalue estimation</h2>
<p>Aka Rayleigh Ritz procedure. \((H_{n})_{i,j} = q_{i}^{*}Aq_{j}\). As \(Aq_{j} \in K_{j+1}\), for \(i&gt;j+1\), \((H_{n})_{i,j} = 0\). \((H_{n})_{j,j} = r(q_{j}) = q_{j}^{*}Aq_{j}\).</p>
<p>ew of \(H_{n}\) called Arnoldi ew estimates of A, Ritz values wrt \(K_{n}\) of A.  ev of \(H_{n}\) are the stationary points of r(x) restricted to \(K_{n}\): \(r(x) = r(Q_{n}y) = \frac{y^{T}Q_{n}^{T}AQ_{n}y}{y^{T}y} = \frac{y^{T}H_{n}y}{y^{T}y}\). \(\gradient r(Q_{n}y) = r&rsquo;(y) = 0\) iff y is ev and \(r&rsquo;(y)\) is ew of \(H_{n}\).</p>
<h2 id="directly-solve-ax--b">Directly solve Ax = b</h2>
<p>Can use triangular triangularization: but sparsity could be spoilt due to row operations: so try to order elimination steps to minimize the number of non zeros added.</p>
<h3 id="for-symmetric-ve-definite-a">For symmetric +ve definite A</h3>
<p>A can be interpreted to be the precision matrix of a gaussian distribution \(Pr(x) \propto e^{-2^{-1}x^{T}Ax - \mean^{T}Ax}\), whose graphical model G is sparse. Then, solving \(A\mean = b\) is same as finding the mode of Pr(x). Can use loopy belief propogation to solve Ax = b. This is exact when G is a tree; the updates correspond to triangular triangularization.</p>
<h2 id="iteratively-solve-axb-1">Iteratively solve Ax=b</h2>
<h3 id="the-optimization-view">The optimization view</h3>
<p>Error \(e_{n} = \norm{x-x_{n}}\) vs residual \(r_{n} = \norm{Ax_{n}-b}\).</p>
<p>View solving Ax=b as an optimization problem: minimize something like this at every iteration.</p>
<h3 id="for-dense-a">For dense A</h3>
<p>See linear algebra ref.</p>
<h3 id="for-spd-a">For SPD A</h3>
<p>See later section.</p>
<h3 id="generalized-minimum-residuals-gmres">Generalized minimum residuals (GMRES)</h3>
<h4 id="residue-minimization-in-a-subspace">Residue minimization in a subspace</h4>
<p>At step n, approximate x by \(x_{n} \in K_{n}(A, r_{0})\) which minimizes residual \(\norm{.}_{2}\) of \(r_{n} = b - Ax_{n}\): minimize \(AQ_{n}y - b = Q_{n+1}\hat{H}_{n}y-b\): \(\norm{Q_{n+1}\hat{H}_{n}y-b}_{2} = \norm{\hat{H}_{n}y-Q_{n+1}^{*}b}_{2} = \norm{\hat{H}_{n}y-\norm{b}e_{1}}\). So oblique projection of problem.</p>
<h4 id="alg-2">Alg</h4>
<p>At each step of Arnoldi itern to find orthobasis of \(K_{n}(A, r_{0})\): Find \(y_{n} = \argmin_{y} \norm{\hat{H}_{n}y-\norm{b}e_{1}}\), \(x_{n}= Q_{n}y\).</p>
<h3 id="cgnr">CGNR</h3>
<p>Conjugate Gradients for minimizing residue using Normal eqns. Take Ax = b, get \(A^{<em>}Ax = A^{</em>}b\); then apply CG. Relationship with old r: \(\hat{r} = A^{*}r\). Ortho projection of problem under normal matrix.</p>
<p>But \(k(A^{*}A) = k(A)^{2}\): Slower convergence.</p>
<h3 id="cgne">CGNE</h3>
<p>Take \(A^{T}u = x\), get \(AA^{T}u = b\). \(AA^{T}\) is SPD; now apply CG. Residue r same; new direction vector \(\hat{p} = A^{-T}p\). Get monotonic decay in error vector. Ortho projection of problem under normal matrix.</p>
<h2 id="axb-spd-a">Ax=b, SPD A</h2>
<p>Many practical applications.</p>
<h3 id="reduction-to-quadratic-programming">Reduction to quadratic programming</h3>
<p>Consider \(A \succeq 0\). Then, any \(q(x) = (0.5) x^{T}Ax - b^{T}x + c\) is convex, has minimum when \(\gradient q(x) = Ax - b = 0\).</p>
<p>So, can now use ideas from quadratic programming to solve Ax = b! Can try to minimize q(x) iteratively! Move in the direction of \(\gradient q(x)\).</p>
<h3 id="conjugate-gradients-cg-for-spd-a">Conjugate gradients (CG) for SPD A</h3>
<h4 id="error-minimization-in-a-subspace">Error minimization in a subspace</h4>
<p>Consider iteration n: restricted to subspace \(K_n\). For \(x_n \in K_n\): Take \(e_{n} = x - x_{n}\); solve optimization problem: minimize \(\norm{e_{n}}_{A} = x_{n}^{T}Ax_{n} - 2x_{n}^{T}b + b^{T}b = 2f(x_{n}) + k\): note definition of f().</p>
<p>Find \(x_{n} = argmin_{x_{n} \in K_{n}} f(x_{n})\): so minimizing only the shadow of f in \(K_{n}\). f is convex, so is \(K_n\), so it has a unique minimum.</p>
<p>Actual problem was to minimize q(x). Here, solving the ortho projection of problem under A.</p>
<h4 id="iteration">Iteration</h4>
<p>\(x_{0} = 0, p_{0} = r_{0} = b\). \(x_{n} = x_{n-1} + a_{n}p_{n-1}\).</p>
<p>Step size \(a_{n} = \frac{r_{n-1}^{T}r_{n-1}}{p_{n-1}^{T}Ap_{n-1}}\): \(f(x_{n})\) minimum when \(\gradient f(x_{n}) = Ax_{n} -b = A(x_{n-1} + a_{n}p_{n-1}) - b  = a_{n}Ap_{n-1} - r_{n-1}= 0\).</p>
<p>Residual \(r_{n} = b - Ax_{n} = r_{n-1} - a_{n}Ap_{n-1}\); direction \(p_{n} = r_{n} + c_{n}p_{n-1}\); improvement in current step \(c_{n} = \frac{(r_{n}^{T}r_{n})}{(r_{n-1}^{T}r_{n-1})}\).</p>
<p>From iteration: \(K_{n} = \linspan{x_{1}, .. x_{n}} = \linspan{p_{0}, .. p_{n-1}}\).</p>
<p>\(\norm{e_{n-1}}_{A} \geq \norm{e_{n}}_{A}\). \why</p>
<p>Search directions are A conjugate: \(p_{n}^{T}Ap_{j} = 0\). \why \(r_{n}^{T}r_{j}=0\). \why</p>
<h3 id="conjugate-residues-cr-for-symmetric-a">Conjugate residues (CR) for symmetric A</h3>
<p>Minimize \(\norm{Ax-b}_{A}\): \<br>
oblique projection of the problem under A.</p>
<p>\tbc</p>
<h3 id="biconjugate-gradients-bcg-method-for-non-singular-a">Biconjugate gradients (BCG) method for non singular A</h3>
<p>Do CG on 2 systems together: Ax = b and \(A^{<em>}x^{</em>} = b^{*}\). If A is near-symmetric, get good convergence.</p>
<p>\tbc</p>
<h3 id="preconditioning">Preconditioning</h3>
<h4 id="uses">Uses</h4>
<p>Maybe k(A) very high in \(Ax = b\); so, get an equivalent, better conditioned problem. Or, maybe want to turn it into an equivalent problem which is easier to solve.</p>
<h4 id="left-preconditioning">Left preconditioning</h4>
<p>Take \(M \in S_{++}\); \(M^{-1}Ax = M^{-1}b\). \<br>
If \(M^{-1} \approx A^{-1}, k(M^{-1}A) \approx 1\).</p>
<h4 id="right-preconditioning">Right preconditioning</h4>
<p>Take \(AM^{-1}Mx = AM^{-1}u = b\).</p>
<h4 id="shift-preconditioning">Shift preconditioning</h4>
<p>Take $M_{L}^{-1}AM_{R}^{-1}M_{R}x = \<br>
M_{L}^{-1}b\(. Want \)\norm{M_{L}^{-1}AM_{R}^{-1}}$ small.</p>
<h4 id="traits-of-good-preconditioners">Traits of good preconditioners</h4>
<p>Work involved is actually in solving \(My = b\), so usually want M very sparse. But, at the same time, want M to be close to A, which may be quite dense.</p>
<h5 id="joint-condition-number-of-m-and-a">Joint Condition Number of M and A</h5>
<p>k(A,M)</p>
<h3 id="finding-left-preconditioning-matrix-m">Finding left preconditioning matrix M</h3>
<h4 id="using-alu">Using A=LU</h4>
<p>Get \(A \approx LU\), get \(A^{-1} \approx M = U^{-1}L^{-1}\).</p>
<p>Use Incomplete LU factorization (ILU) with 0 pattern P: \(A^{-1} = U^{-1}L^{-1}\) can kill sparsity, so sacrifice accuracy for sparsity. Alter LU alg to keep sparsity.</p>
<p>ILU(0): P same as 0&rsquo;s in A.</p>
<p>ILU(p): Keep level of fill \(l_{ij}: k: |a_{ij}| \approx \eps_{k}&lt;1\), drop items with \(l_{ij}&gt;k&rsquo;\). Heuristic which doesn&rsquo;t need finding log: init value: \(l_{ij} = 0\) if \(a_{ib}\neq 0\), else \(\infty\); updates while putting 0: \(l_{i,j} := \min (l_{ij}, l_{ik}+ l_{kj} + 1)\): \(l_{ik}+ l_{kj} \) corresponds to change in level. Same run for every matrix with same pattern.</p>
<p>ILU (Thresholding): No row op when wt is near 0, keep only p pre and post diagonal entries.</p>
<h4 id="use-support-graph-theory">Use support graph theory</h4>
<p>M is symmetric and +ve semidefinite.</p>
<p>\tbc</p>
<h4 id="find-sparse-m-to-min-htextnormma-i_f">Find sparse M to min \htext{\(\norm{MA-I_{F}\)}{..}}</h4>
<p>Start with random M, do gradient descent. Let \(F(M) = \norm{MA-I}_{F}\); Use Frechet derivative: \(\frac{F(M+E) - F(M)}{\norm{E}}\).</p>
<p>\tbc</p>
<h3 id="using-preconditioners">Using preconditioners</h3>
<h4 id="preconditioned-conjugate-gradient-pcg-for-spd-a">Preconditioned conjugate gradient (PCG) for SPD A</h4>
<p>Left preconditioning: \(M^{-1}A\) is SPD under \(\dprod{.,.}<em>{M}\). Can adapt CG to this: always use \(\dprod{.,.}</em>{M}\), residue \(z = M^{-1}r\). Replace new vars with old vars to get PCG alg.</p>
<p>Use \(M=LL^{T}\), \(\hat{p}<em>{j} = p</em>{j}\) etc.. to see that split PCG is equivalent to PCG.</p>
<p>Right preconditioning: \(AM^{-1}\) is SPD under \(\dprod{.,.}_{M^{-1}}\). Get alg equivalent to PCG.</p>
<p>Similarly, can apply PCG on CGNR and CGNE.</p>
<h4 id="preconditioning-gmres">Preconditioning GMRES</h4>
<p>Simply solve the preconditioned form of the eqn using GMRES.</p>
<p>Left and right preconditioning not equivalent. Right preconditioning minimizes actual, unskewed residue.</p>

  </div>
</article>







<aside class="card border" id="section-tree-item-">
    <div class="card-title bg-light-gray border d-flex justify-content-between">
        <a href="#ZgotmplZ">Dense algebra </a>
        <a data-toggle="collapse" href="#section-tree-item-body-" role="button" aria-expanded="false" aria-controls="section-tree-item-body-" >…<i class="fas fa-caret-down" class="collapsed"></i> </a>
    </div>
    <nav id="section-tree-item-body-" class="card-body p-0 collapse">
        
        <li>draft: false</li>
        
        <li>iscjklanguage: false</li>
        
        <li>title: Dense algebra</li>
        
    </nav>
</aside>


      </main>
    </div>
    <footer class="bg-yellow  p-1" role="contentinfo">
  <div id="disqus_thread"></div>
  <div id="div_footer_bar" class="container-fluid d-flex justify-content-between">
    <a class="btn btn-secondary" href="https://github.com/vvasuki/notes/issues/new" >
      प्रतिस्पन्दः
    </a>
    <a class="btn btn-secondary" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" /></a>
    <div class="btn small border">
      Built on 2020 Nov 22 10:54:55 UTC. (<a href="http://google.com/search?q=10%3a54%3a55%20UTC to IST">IST</a>)
    </div>
  <ul id="footer-bar-right-custom" class="list-group list-group-horizontal">
  </ul>
  </div>
</footer>

  </body>
  <script type='text/javascript'>
    
    
    
    module_main.default.onDocumentReadyTasks();
  </script>
</html>
