\documentclass[10pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}

%opening
\title{CS388R: Answer to Homework 2}
\author{vishvAs vAsuki}

\begin{document}

\maketitle

\section{Question}

Exercise 2.7 (c and d): Let X and Y be independent geometric random variables, where X has parameter p and Y has parameter q.

\begin{enumerate}

\item       What is Pr(min(X, Y)=k)?
\item       What is $E[X|X\leq Y]$?
\end{enumerate}

You may find it helpful to keep in mind the memoryless property of geometric random variables.

\subsection{Solution}

\subsubsection*{Part 1}

min(X,Y) = k in the following cases:

\begin{itemize}
\item $X=k, Y \geq k$
\item $X>k, Y=k$
\end{itemize}

Note:
\begin{eqnarray}
Pr(X = i) &=& (1-p)^{i-1}p\\
Pr(Y = i) &=& (1-q)^{i-1}q\\
&&\text{$X \geq i$ means there have been i-1 failures.}\\
So: Pr(X \geq i) &=& (1-p)^{i-1}\\
Also: Pr(Y \geq i) &=& (1-q)^{i-1}\\
&&\text{$X > i$ means there have been i failures.}\\
So: Pr(X > i) &=& (1-p)^{i}\\
Also: Pr(Y > i) &=& (1-q)^{i}\\
\end{eqnarray}

We use the above identities below:
\begin{eqnarray}
Pr(min(X, Y)=k)  &=& Pr(X=k)Pr(Y\geq k) + Pr(X>k)Pr(Y=k)\\
Pr(min(X, Y)=k)  &=& (1-p)^{k-1}p (1-q)^{k-1} + (1-p)^{k}(1-q)^{k-1}q\\\
Pr(min(X, Y)=k)  &=& (1-p)^{k-1}(1-q)^{k-1}(p+q-pq)\\\
Pr(min(X, Y)=k)  &=& (1-p)^{k-1}(1-q)^{k-1}-(1-p)^{k}(1-q)^{k}\\
\end{eqnarray}
Note that the last line indicates Pr(events corresponding to both X and Y fail k-1 times) - Pr(events corresponding to both X and Y fail k times), which makes intuitive sense.

\subsubsection*{Part 2}

\begin{eqnarray}
&&\text{From definition of expectations:}\\
E[X|X\leq Y] &=& \sum_{i=1}^{\infty} iP(X=i|X\leq Y)\\
&&\text{Applying Bayes Rule:}\\
E[X|X\leq Y] &=& \sum_{i=1}^{\infty} iP(X=i,X\leq Y)/P(X\leq Y)\\
&&\text{As noted in 1:}\\
Pr(Y \geq i) &=& (1-q)^{i-1}\\
P(X=i,Y \geq i) &=& Pr(X=i)Pr(Y \geq i)\\
P(X=i,Y \geq i) &=& (1-p)^{i-1}p(1-q)^{i-1}\\
P(X=i,Y \geq i) &=& (1-p)^{i-1}p(1-q)^{i-1}\\
P(X\leq Y) &=& \sum_{i=1}^{\infty} P(X=i,Y \geq i)\\
P(X\leq Y) &=& \sum_{i=1}^{\infty} (1-p)^{i-1}p(1-q)^{i-1}\\
P(X\leq Y) &=& p/(1-(1-p)(1-q))\\
So: \frac{P(X=i,Y \geq i)}{P(X\leq Y)} &=& (1-p)^{i-1}p(1-q)^{i-1}(1-(1-p)(1-q))/p\\
\frac{P(X=i,Y \geq i)}{P(X\leq Y)} &=& (1-p)^{i-1}(1-q)^{i-1}(1-(1-p)(1-q))\\
\frac{P(X=i,Y \geq i)}{P(X\leq Y)} &=& (1-p)^{i-1}(1-q)^{i-1}-(1-p)^{i}(1-q)^{i}\\
&&\text{Substituting this in the earlier equation:}\\
E[X|X\leq Y] &=& \sum_{i=1}^{\infty} [i(1-p)^{i-1}(1-q)^{i-1}-i(1-p)^{i}(1-q)^{i}]\\
\end{eqnarray}
\begin{eqnarray}
E[X|X\leq Y] &=& \sum_{i=1}^{\infty} i(1-p)^{i-1}(1-q)^{i-1}-\sum_{i=1}^{\infty}i(1-p)^{i}(1-q)^{i}\\
\text{But:}&&\sum_{i=1}^{\infty} i(1-p)^{i-1}(1-q)^{i-1} = \\
&& \sum_{i=1}^{\infty} (i-1)(1-p)^{i-1}(1-q)^{i-1} \\
&&+ \sum_{i=1}^{\infty} (1-p)^{i-1}(1-q)^{i-1}\\
&=& \sum_{i=1}^{\infty} (i)(1-p)^{i}(1-q)^{i} + \frac{1}{1-(1-p)(1-q)}\\
\text{So:}E[X|X\leq Y] &=& \frac{1}{1-(1-p)(1-q)}\\
\end{eqnarray}

\section{Question}

Exercise 2.9:

\begin{enumerate}

\item       Suppose that we roll twice a fair k-sided die with the numbers 1 through k on the die's faces, obtaining values X1 and X2. What is E[max(X1, X2)]? What is E[min(X1, X2)]?

\item       Show from your calculations in part (a) that (2.1) E(max(X1, X2)) + E[min(X1, X2)] = E[X1] + E[X2].

\item       Explain why Eqn. (2.1) must be true by using the linearity of expectations instead of a direct computation.

\end{enumerate}

\subsection{Solution}

\subsubsection*{Part 1}

\begin{eqnarray}
&&\text{From definition of expectations:}\\
E[max(X1, X2)] &=& \sum_{i=1}^{k} i Pr(max(X1, X2)=i)\\
E[max(X1, X2)] &=& \sum_{i=1}^{k} i (Pr(X1=i)Pr(X2 \leq i) \\
&&+ Pr(X1<i)Pr(X2=i))\\
E[max(X1, X2)] &=& \sum_{i=1}^{k} i (\frac{1}{k}\frac{i}{k} + \frac{i-1}{k}\frac{1}{k})\\
E[max(X1, X2)] &=& \sum_{i=1}^{k} i \frac{2i-1}{k^{2}}\\
E[max(X1, X2)] &=& \frac{1}{k^{2}}\sum_{i=1}^{k} (2i^{2}-i)\\
E[max(X1, X2)] &=& \frac{1}{k^{2}}(2\sum_{i=1}^{k} i^{2}-\sum_{i=1}^{k} i)\\
E[max(X1, X2)] &=& \frac{1}{k^{2}}2\frac{k(k+1)(2k+1)}{6}-\frac{k(k+1)}{2}\\
E[max(X1, X2)] &=& \frac{1}{k^{2}}k(k+1)\frac{2(2k+1)}{6}-\frac{3}{6}\\
E[max(X1, X2)] &=& \frac{1}{k^{2}}k(k+1)\frac{(4k-1)}{6}\\
E[max(X1, X2)] &=& \frac{(k+1)(4k-1)}{6k}\\
\end{eqnarray}

\begin{eqnarray}
&&\text{From definition of expectations:}\\
E[min(X1, X2)] &=& \sum_{i=1}^{k} i Pr(min(X1, X2)=i)\\
E[min(X1, X2)] &=& \sum_{i=1}^{k} i (Pr(X1=i)Pr(X2>=i) \\
&& + Pr(X1>i)Pr(X2=i))\\
E[min(X1, X2)] &=& \sum_{i=1}^{k} i (\frac{1}{k}\frac{k-i+1}{k} + \frac{k-i}{k}\frac{1}{k})\\
E[min(X1, X2)] &=& \frac{1}{k^{2}}\sum_{i=1}^{k} i (k-i+1 + k-i)\\
E[min(X1, X2)] &=& \frac{1}{k^{2}}\sum_{i=1}^{k} i (2k-2i+1)\\
E[min(X1, X2)] &=& \frac{1}{k^{2}}\sum_{i=1}^{k} ((2k+1)i-2i^{2})\\
E[min(X1, X2)] &=& \frac{1}{k^{2}}((2k+1)k(k+1)/2-2\sum_{i=1}^{k}i^{2})\\
E[min(X1, X2)] &=& \frac{1}{k^{2}}((2k+1)k(k+1)/2-2\frac{k(k+1)(2k+1)}{6})\\
E[min(X1, X2)] &=& \frac{k(k+1)(2k+1)}{k^{2}}(1/2-\frac{2}{6})\\
E[min(X1, X2)] &=& \frac{(k+1)(2k+1)}{6k}\\
\end{eqnarray}

\subsubsection*{Part 2}
\begin{eqnarray}
E[max(X1, X2)] + E[min(X1, X2)] &=& \frac{(k+1)(4k-1)}{6k}+\frac{(k+1)(2k+1)}{6k}\\
E[max(X1, X2)] + E[min(X1, X2)] &=& \frac{(k+1)}{6k}(4k-1+2k+1)\\
E[max(X1, X2)] + E[min(X1, X2)] &=& \frac{(k+1)}{6k}(6k)\\
E[max(X1, X2)] + E[min(X1, X2)] &=& (k+1)\\
&&\text{But note:}\\
E[X1]=E[X2] &=& \sum_{i=1}^{i=k}iPr(X=i)\\
E[X1]=E[X2] &=& \frac{1}{k}\sum_{i=1}^{i=k}i\\
E[X1]=E[X2] &=& \frac{1}{k}\frac{k(k+1)}{2}\\
E[X1]=E[X2] &=& \frac{(k+1)}{2}\\
So: E[X1]+E[X2] &=& (k+1)\\
So: E[X1]+E[X2] &=& E[max(X1, X2)] + E[min(X1, X2)]\\
\end{eqnarray}

\subsubsection*{Part 3}
We want to prove $E[X1]+E[X2] = E[max(X1, X2)] + E[min(X1, X2)]$ by the use of Linearity of Expectations.

Lemma: X1+X2=max(X1, X2)+min(X1, X2).
Proof:
\begin{eqnarray}
&&\text{When $X1<X2$:}\\
max(X1, X2) &=& X2\\
min(X1, X2) &=& X1\\
So: X1+X2 &=& max(X1, X2)+min(X1, X2)\\
\\
&&\text{When $X1=X2$:}\\
max(X1, X2) &=& X2\\
min(X1, X2) &=& X1\\
So, X1+X2 &=& max(X1, X2)+min(X1, X2)\\
\\
&&\text{When $X1>X2$:}\\
max(X1, X2) &=& X1\\
min(X1, X2) &=& X2\\
So, X1+X2 &=& max(X1, X2)+min(X1, X2)\\
So, \forall X1,X2: X1+X2 &=& max(X1, X2)+min(X1, X2)\\
\end{eqnarray}

We use the above lemma below:
\begin{eqnarray}
&&\text{Applying linearity of expectations:}\\
E[X1]+E[X2] &=& E[X1+X2]\\
&&\text{Using the lemma proved above:}\\
E[X1]+E[X2] &=& E[max(X1, X2)+min(X1, X2)]\\
&&\text{Applying linearity of expectations:}\\
E[X1]+E[X2] &=& E[max(X1, X2)] + E[min(X1, X2)]\\
\end{eqnarray}


\section{Question}

Exercise 2.18: The following approach is often called reservoir sampling. Suppose we have a sequence of items passing by one at a time. We want to maintain a sample of one item with the property that it is uniformly distributed over all the items that we have seen at each step. Moreover, we want to accomplish this without knowing the total number of items in advance or storing all of the items that we see.

Consider the following algorithm, which stores just one item in memory at all times. When the first item appears, it is stored in the memory. When the kth item appears, it replaces the item in memory with probability 1/k. Explain why this algorithm solves the problem.

\subsection{Solution}

We want to maintain a sample of one item with the property that it is uniformly distributed over all the items that we have seen at each step. To clarify, the sample must have the property that it is uniformally distributed over all the items that we have seen at each step. In other words, when we have seen k items, we want to show that, for each of the k items, the probability that the sample consists of that item is 1/k. We want to show that, after we have seen k items, $\forall i \in {1 .. k} Pr(S(k)=I_{i})=1/k$ (We call this the proposition $P_{k}$). Here, S(k) represents the Sample at the kth step of the algorithm, and $I_{i}$ represents the ith item we have seen.

We prove this by induction.

Basis step: From the definition of the algorithm in the question, which says 'When the first item appears, it is stored in the memory.', the proposition $P_{1}$, $\forall i \in {1 .. k} Pr(S(k)=I_{i})=1/k$ is true when k=1.

Induction step: We now prove that, when $P_{k}$ is true, $P_{k+1}$ is true.

Now, suppose that $P_{k}$ is true. That is, $\forall i \in {1 .. k} Pr(S(k)=I_{i})=1/k$ is true. Then, consider $Pr(S(k+1)=I_{k+1})$. According to the description of the algorithm, it replaces the item in memory with probability 1/(k+1). So, $Pr(S(k+1)=I_{k+1})=1/(k+1)$. [1]

Now, consider $Pr(S=I_{i})$ for some $i \in {1 .. k} $. According to our assumption, $Pr(S(k)=I_{i})=1/k$. According to the description of the algorithm, S(k)=S(k+1) with the probability k/(k+1). Hence, $\forall i \in {1 .. k} Pr(S(k+1)=I_{i})=(1/k)(k/(k+1))=1/(k+1)$. [2]

Hence, putting [1] and [2] together, we have : $\forall i \in {1 .. k+1} Pr(S(k+1)=I_{i})=(1/k)(k/(k+1))=1/(k+1)$.

Thus, above, we have proved that, when $P_{k}$ is true, $P_{k+1}$ is true. And, we have also proved that $P_{1}$. Hence, by the principle of mathematical induction, $P_{x}$ is true for all natural numbers x.

\section{Question}

Exercise 2.26: A permutation $\pi: [1, n] \rightarrow [1, n]$ can be represented as a set of cycles as follows. Let there be one vertex for each number i, i=1, ..., n. If the permutation maps the number i to the number $\pi(i)$, then a directed arc is drawn from vertex i to vertex $\pi(i)$. This leads to a graph that is a set of disjoint cycles. Notice that some of the cycles could be self-loops. What is the expected number of cycles in a random permutation of n numbers?

\subsection{Solution}

We define the x-cycle as a cycle with x verteces. 

First, we calculate the probability that vertex v is in an x-cycle. Now, consider the case where vertex $v_{1}$ is in an x-cycle. For this to happen, $v_{2}=\pi(v_{1})$ should not be $v_{1}$. (The probability of this happening in a random permutation is $\frac{(n-1)}{n}$.) Also, $v_{3}=\pi(v_{2})$ should not be $v_{1}$ or $v_{2}$. (The probability of this happening in the same random permutation is $\frac{(n-2)}{(n-1)}$. This is because we are dealing with the case where 1 vertex out of n is already the permutation of some other vertex.) This should hold true for all verteces in the x-cycle, except for the vertex $v_{x-1}$. For any such verteces $v_{y}$, which is the yth vertex in the partially formed x-cycle, the probability that $\pi(v_{y})$ is not a vertex from $v_{1}$ to $v_{y}$ is $\frac{(n-y)}{n-y+1}$. (This is because we are dealing with the case where y-1 verteces out of n are already the permutations of some other verteces.) Finally, to complete the x-cycle, $\pi(v_{x-1})=v_{1}$ must be true. The probability of this happening in a random graph is $\frac{1}{(n-x+1)}$.

Hence:
\begin{eqnarray}
Pr(v_{i} \in x-cycle) &=& \frac{(n-1)}{n}\frac{(n-2)}{(n-1)}..\frac{(n-x)}{(n-x+2)}\frac{1}{(n-x+1)}\\
Pr(v_{i} \in x-cycle) &=& \frac{1}{(n)}\\
\end{eqnarray}

Now, we find the expected number of x-cycles. When y verteces are found to be part of an x-cycle, we can safely say that there exist y/x x-cycles. So, E[number of x-cycles]=E[(verteces in x-cycle)/x]=E[verteces in x-cycle]/x (by linearity of expectations). We define an indicator random variable, $X_{i,x}$ as follows:  $X_{i,x}=1$ if vertex i is part of an x cycle, and $X_{i,x}=0$ otherwise. Then, E[verteces in x-cycle] = $E[X_{i,x}]$ =$\sum_{i=1}^{n}X_{i,x}P(X_{i,x}=1)$ = $\sum_{i=1}^{n}1/n$ = 1. Hence, finally, E[number of x-cycles] = $E[X_{i,x}]/x=1/x$.

Now, E[total number of cycles] = $\sum_{i=1}^{n}$E[number of x-cycles] = $\sum_{i=1}^{n}1/i \simeq ln n$. (We have applied linearity of expectations in the first step here.) Thus, the expected number of cycles is the harmonic number H(n).

\section{Question}

Exercise 2.32: You need a new staff assistant, and you have n people to interview. You want to hire the best candidate for the position. When you interview a candidate, you can give them a score, with the highest score being the best and no ties being possible. You interview the candidates one by one. Because of your company's hiring practices, after you interview the kth candidate, you either offer the candidate the job before the next interview or you forever lose the chance to hire that candidate. We suppose the candidates are interviewed in a random order, chosen uniformly at random from all n! possible orderings.

We consider the following strategy. First, interview m candidates but reject them all; these candidates give you an idea of how strong the field is. After the mth candidate, hire the first candidate you interview who is better than all of the previous candidates you have interviewed.

\begin{enumerate}
\item Let E be the event that we hire the best assistant, and let Ei be the event that ith candidate is the best and we hire him. Determine Pr(Ei), and show that
      $Pr(E)=\frac{n}{m}\sum_{j=m+1}^{n}\frac{1}{j-1}$

\item Bound $\sum_{j=m+1}^{n}\frac{1}{j-1}$ to obtain
      $\frac{m}{n}(ln(n) - ln(m)) \leq Pr(E) \leq \frac{m}{n}(ln (n-1) - ln (m-1))$

\item Show that m(ln n-ln m)/n is maximized when m=n/e, and explain why this means $Pr(E)\geq 1/e$ for this choice of m.
\end{enumerate}

\subsection{Solution}

\subsubsection*{1} First we define Best[i] to be the even where the ith candidate is the best among all n. As we deal with a random permutation, we know that Best[i] is equal to 1/n. If $i\leq m$, the ith candidate is rejected. Hence, for all i in \{1, ... m\}, Pr(Ei)=0. For all i in \{m+1, ... n\}, Ei is true when Best[i] is true, and all candidates in the set \{m+1, .. i-1\} were found to be inferior to the best among the first k candidates.

The latter proposition is false only when all candidates in \{m+1, .. i-1\} are better than any candidate in \{1, ... m\}. In other words, that proposition is false only when the candidates in \{m+1, .. i-1\} are the best among the first i-1 candidates, and this happens with the probability (i-1-m)/(i-1). Hence, for any i in \{m+1, ... n\}, all candidates in the set \{m+1, .. i-1\} will be found inferior to the best among the first k candidates with the probability 1-(i-1-m)/(i-1) = m/(i-1).

So, for all i in the set \{m+1, .. i-1\}, we have: $Pr(E_{i}) = \frac{m}{n(i-1)}$.

E, the event that we hire the best assistant, happens only when one of the disjoint events: E1, E2, E3... En happens. 
\begin{eqnarray}
P(E) &=& \sum_{i=1}^{n} P(E_{i})\\
&& \text{As the first m candidates are selected with probability 0:}\\
P(E) &=& \sum_{i=m+1}^{n} P(E_{i})\\
P(E) &=& \sum_{i=m+1}^{n} \frac{m}{n(i-1)}\\
P(E) &=& \frac{m}{n}\sum_{i=m+1}^{n} \frac{1}{(i-1)}\\
\end{eqnarray}

\subsubsection*{2} 

As 1/x is a monotonically decreasing function, we can say: $ln(n)=\int_{x=1}^{n}1/x \leq \sum_{k=1}^{n}1/k$ and $\sum_{k=2}^{n}1/k \leq \int_{x=1}^{n}1/x =ln(n)$. This is the same argument used in \cite{mitzenmacherUpfal}, which includes nice figures to aid the intuitive understanding of these inequalities. For similar reasons, we can say for all $m>1$: $\int_{x=m}^{n}1/x \leq \sum_{k=m}^{n-1}1/k$ and $\sum_{k=m}^{n-1}1/k \leq \int_{x=m-1}^{n-1}1/x$.

Applying these inequalities, we get: $\int_{x=m}^{n}1/x \leq \sum_{j=m}^{n-1}\frac{1}{j} \leq \int_{x=m-1}^{n-1}1/x$. This implies the following inequality: $(ln(n) - ln(m)) \leq \sum_{j=m+1}^{n}\frac{1}{j-1} \leq (ln (n-1) - ln (m-1))$. This inturn implies the following bound: $\frac{m}{n}(ln(n) - ln(m)) \leq \frac{m}{n}\sum_{j=m+1}^{n}\frac{1}{j-1} \leq \frac{m}{n}(ln (n-1) - ln (m-1))$. Hence, we get the bound: $\frac{m}{n}(ln(n) - ln(m)) \leq Pr(E) \leq \frac{m}{n}(ln (n-1) - ln (m-1))$.

\subsubsection*{3} m(ln (n)-ln (m))/n is maximized when its derivative, with respect to m, is 0. So:
\begin{eqnarray}
\frac{ln(n)}{n} - \frac{ln(m)}{n} - \frac{1}{n} &=& 0\\
ln(n) - ln(m) - 1 &=& 0\\
ln(n) - ln(m) - ln(e) &=& 0\\
ln(n) - ln(e) &=& ln(m)\\
ln(n/e) &=& ln(m)\\
(n/e) &=& (m)\\
\end{eqnarray}
So, when m=n/e, m(ln (n)-ln (m))/n is maximized. But, $\frac{m}{n}(ln(n) - ln(m)) \leq Pr(E) \leq \frac{m}{n}(ln (n-1) - ln (m-1))$. So, substituting the maximizing value of m, $1/e \leq Pr(E)$!

\section{Question}

Exercise 3.7: A simple model of the stock market suggests that, each day, a stock with price q will increase by a factor $r>1$ to qr with probability p and will fall to q/r with probability 1-p. Assuming we start with a stock with price 1, find a formula for the expected value and the variance of the price of the stock after d days.

\subsection{Solution}

Let $X_{d}$ be the expectation of the stock in d days. Suppose that in d days, the stock price, q, increased c times, and decreased d-c times, then the stock price at the end of d days will be $\frac{qr^{c}}{r^{d-c}}$. The probability that the stock price increases for c days and decreases for d-c days is $\binom{d}{c}p^{c}(1-p)^{d-c}$. Hence:
\begin{eqnarray}
E[X_{d}] &=& \sum_{c=1}^{d} \frac{qr^{c}}{r^{d-c}}\binom{d}{c}p^{c}(1-p)^{d-c} \\
&=&  \sum_{c=1}^{d} q\binom{d}{c}(rp)^{c}(\frac{1-p}{r})^{d-c}\\
&=& q(rp+\frac{1-p}{r})^{d}\\
\end{eqnarray}

Now, we find the variance:
\begin{eqnarray}
E[X_{d}^{2}] &=& \sum_{c=1}^{d} (\frac{qr^{c}}{r^{d-c}})^{2}\binom{d}{c}p^{c}(1-p)^{d-c} \\
&=&  \sum_{c=1}^{d} q^{2}\binom{d}{c}(r^{2}p)^{c}(\frac{1-p}{r^{2}})^{d-c}\\
&=&  q^{2}\sum_{c=1}^{d} \binom{d}{c}(r^{2}p)^{c}(\frac{1-p}{r^{2}})^{d-c}\\
&=& q^{2}(r^{2}p+\frac{1-p}{r^{2}})^{d}\\
Var[X_{d}] &=& E[X_{d}^{2}] - E[X_{d}]^{2}\\
Var[X_{d}] &=& q^{2}(r^{2}p+\frac{1-p}{r^{2}})^{d} - q^{2}(rp+\frac{1-p}{r})^{2d}\\
\end{eqnarray}

Hence, if the stock price is initially 1, $E[X_{d}]=(rp+\frac{1-p}{r})^{d}$, and $Var[X_{d}] = (r^{2}p+\frac{1-p}{r^{2}})^{d} - (rp+\frac{1-p}{r})^{2d}$.

\section{Question}

Exercise 3.15: Let the random variable X be representable as a sum of random variables $X=\sum_{i=1}^{n}Xi$. Show that, if E[XiXj]=E[Xi]E[Xj] for every pair of i and j with $1\leq i<j \leq n$, then $Var[X] = \sum_{i=1}^{n}Var[Xi]$.

\subsection{Solution}
We prove this by mathematical induction.

\textbf{Basis case}: For any two distinct variables mentioned above, Xi and Xj, we prove that $Var[X_{i}+X_{j}]=Var[X_{i}]+Var[X_{j}]$.

\begin{eqnarray}
&&\text{From Theorem 3.2:}\\
Var[X_{i}+X_{j}]&=&Var[X_{i}]+Var[X_{j}]+2E[(X_{i}-E(X_{i}))(X_{j}-E(X_{j}))]\\.
Var[X_{i}+X_{j}]&=&Var[X_{i}]+Var[X_{j}]\\
&&+2E[X_{i}X_{j}-X_{i}E(X_{j})-X_{j}E(X_{i})+E(X_{i})E(X_{j})]\\.
&&\text{Using linearity of expectations and E[XiXj]=E[Xi]E[Xj]:}\\
0&=&E[X_{i}X_{j}-X_{i}E(X_{j})-X_{j}E(X_{i})+E(X_{i})E(X_{j})]\\
&&\text{Hence, for all \textit{distinct} i and j between 1 and n:}\\
&&\text{The following results hold:}\\
0&=&E[(X_{i}-E(X_{i}))(X_{j}-E(X_{j}))]\\
Var[X_{i}+X_{j}]&=&Var[X_{i}]+Var[X_{j}]\\
\end{eqnarray}

\textbf{Induction step}: Now, assume that $Var[\sum_{i=1}^{m}X_{i}] = \sum_{i=1}^{m}Var[X_{i}]$ is true. Then, we prove below that $Var[\sum_{i=1}^{m+1}X_{i}] = \sum_{i=1}^{m+1}Var[X_{i}]$ will be true.

\begin{eqnarray}
Var[\sum_{i=1}^{m+1}X_{i}] &=& Var[\sum_{i=1}^{m}X_{i}+X_{i+1}]\\ 
&&\text{From Theorem 3.2:}\\
Var[\sum_{i=1}^{m+1}X_{i}] &=& Var[\sum_{i=1}^{m}X_{i}]+Var[X_{i+1}]\\
&&+2E[(\sum_{i=1}^{m}X_{i}-E(\sum_{i=1}^{m}X_{i}))(X_{m+1}-E(X_{m+1}))]\\ 
&&\text{From linearity of expectations:}\\
&&E[(\sum_{i=1}^{m}X_{i}-E(\sum_{i=1}^{m}X_{i}))(X_{m+1}-E(X_{m+1}))]\\
&=& E[(\sum_{i=1}^{m}X_{i}-\sum_{i=1}^{m}E[X_{i}])(X_{m+1}-E(X_{m+1}))]\\
&=& E[\sum_{i=1}^{m}(X_{i}-E[X_{i}])(X_{m+1}-E(X_{m+1}))]\\
&&\text{From linearity of expectations:}\\
&=& \sum_{i=1}^{m}E[(X_{i}-E[X_{i}])(X_{m+1}-E(X_{m+1}))]\\
&&\text{From a result shown earlier:}\\
&=& 0\\
&&\text{Applying this in an earlier equation:}\\
Var[\sum_{i=1}^{m+1}X_{i}] &=& Var[\sum_{i=1}^{m}X_{i}]+Var[X_{i+1}]\\
Var[\sum_{i=1}^{m+1}X_{i}] &=& \sum_{i=1}^{m+1}Var[X_{i}]\\
\end{eqnarray}

Hence, by the \textbf{principle of mathematical induction}, for any m between 2 and n, $Var[X] = \sum_{i=1}^{m}Var[Xi]$.

\section{Question}

Exercise 3.21: A fixed point of a permutation $\pi: [1, n]\rightarrow[1, n]$ is a value for which $\pi(x)=x$. Find the variance in the number of fixed points of a permutation chosen uniformly at random from all permutations. (Hint: Let Xi be 1 if $\pi(i)=i$, so that $\sum_{i=1}^{n}Xi$ is the number of fixed points. You cannot use linearity to find $Var[\sum_{i=1}^{n}Xi]$, but you can calculate it directly.)

\subsection{Solution}

A fixed point happens in a permutation exactly when a vertex participates in a cycle of length 1. Let X = number of fixed points, and let $X_{i}$ be an indicator random variable which is true when vertex $v_{i}$ in the corresponding permutation graph is in a cycle of length 1. So, $X=\sum_{i=1}^{n}X_{i}$.

We had shown, using a permutation graph, in our answer to Exercise 2.26, that E[number of cycles of length x] = 1/x. So, \textbf{E[X]=1}. We had also proved that $Pr(v_{i} \in x-cycle) = \frac{1}{(n)}$. So, $Pr(X_{i}=1)=1/n$. From this, it follows that $E(X_{i})=E(X_{i}^{2})=1/n$

Now, we find $E[X^{2}]$.

\begin{eqnarray}
E[X^{2}] &=& E[(\sum_{i=1}^{n}X_{i})^{2}]\\
E[X^{2}] &=& E[\sum_{i=1}^{n}(X_{i})^{2}+2\sum_{1 \leq i<j\leq n}X_{i}X_{j}]\\
&&\text{By linearity of expectations:}\\
E[X^{2}] &=& \sum_{i=1}^{n}E[(X_{i})^{2}]+2\sum_{1 \leq i<j\leq n}E[X_{i}X_{j}]\\
\end{eqnarray}

But, $X_{i}X_{j}$ is an indicator random variable, which is 1 only when $\pi(i)=i$ and $\pi(j)=j$. This happens with probability $\frac{1}{n}\frac{1}{n-1}$ in a random permutation. So, $E[X_{i}X_{j}]=\frac{1}{n(n-1)}$.
\begin{eqnarray}
E[X^{2}] &=& \sum_{i=1}^{n}E[(X_{i})^{2}]+2\sum_{1 \leq i<j\leq n}\frac{1}{n(n-1)}\\
&&\text{There are $\binom{n}{2}$ combinations of i and j. So:}\\
E[X^{2}] &=& \sum_{i=1}^{n}E[(X_{i})^{2}]+2\frac{n(n-1)}{2}\frac{1}{n(n-1)}\\
E[X^{2}] &=& \sum_{i=1}^{n}E[(X_{i})^{2}]+1\\
E[X^{2}] &=& \sum_{i=1}^{n}(1/n)+1\\
E[X^{2}] &=& 2\\
Var[X] &=& E[X^{2}]-E[X]^{2}\\
Var[X] &=& 2-1=1\\
\end{eqnarray}

\section{Question}

Exercise 3.22: Suppose that we flip a fair coin n times to obtain n random bits. Consider all $m = \binom{n}{2}$ pairs of these bits in some order. Let Yi be the exclusive-or of the ith pair of bits, and let $Y=\sum_{i=1}^{m}Y_{i}$ be the number of Yi that equal 1.

\begin{enumerate}
\item      Show that each Yi is 0 with probability 1/2 and 1 with probability 1/2.
\item      Show that the Yi are not mutually independent.
\item      Show that the Yi satisfy the property that E[YiYj]=E[Yi]E[Yj].
\item      Using Exercise 3.15, find Var[Y].
\item      Using Chebyshev's inequality, prove a bound on $Pr(|Y-E[Y]|\geq n)$.

\end{enumerate}

\subsection{Solution}

\subsubsection*{1} As the coin-flips are independent, the following outcomes are equally likely: (0,0), (1,0), (0,1) and (1,1). So, the value of the corresponding random variable Yi will be 0, 1, 1 and 0 respectively. As all these outcomes are equally likely, P(Yi=1)=P(Yi=0)=1/2.

\subsubsection*{2} I prove the fact that the Yi are not mutually indepenent. Consider the case where all the coin flips result in the all the n random bits being equal to 1, or in all the n random bits being equal to 0. In these two cases, and in no other cases, are *all* the random variables Yi equal to 0. Thus, Pr(all Yi=0)=Pr(all n random bits are equal) = $2^{-n}+2^{-n}=2^{-n+1}$. But, this value is not equal to the product of the probabilities of all Yi being equal to 0, which is $\prod_{1}^{\binom{n}{2}}Pr(Y_{i}=0)=\prod_{1}^{\binom{n}{2}}2^{-1}=2^{-\binom{n}{2}}$. If the variables Yi were indeed mutually indpendent, these values should have been equal. Hence, by proving that Pr(all Yi=0) $\neq$ $\prod_{1}^{\binom{n}{2}} Pr(Y_{i}=0)$ we have proved that the Yi are not mutually independent.

\subsubsection*{3} First we prove that any pair of variables Yi and Yj are pairwise independent, even though we have already seen that mutual independence among them does not hold. Suppose that Yi and Yj depend on random bits (ai, bi) and (aj, bj) respectively.

In the case where ai, bi, aj and bj are mutually distinct, the random variables Yi and Yj are independent, because the coin-flips which generate the 4 random bits are independent.

Now, consider the case where bi=aj always, that is, when both bi and aj are generated by the same coin flip. In this case, if we enumerate all possible values for the random bits, we get the following values for (ai, bi=aj, bj): (0,0,0), (0,0,1), (0,1,0), (0,1,1), (1,0,0), (1,0,1), (1,1,0), (1,1,1). The corresponding values of the random variables (Yi, Yj) will be: (0,0), (0,1), (1,1), (1,0), (1,0), (1,1), (0,1) and (0,0) respectively. We find that every possible value of (Yi =u, Yj=v) has the probability 1/4, which is equal to 0.5 * 0.5, which is inturn equal to Pr(Yi=u)Pr(Yi=v).

Hence, by considering the possible cases discussed above, we see that any pair of distinct variables (Yi, Yj) are pairwise independent.

Then, from Theorem 3.3 of \cite{mitzenmacherUpfal}, we conclude that E[Yi]E[Yj]=E[Yi.Yj].

\subsubsection*{4} First, we observe that $E[Y_{i}]=E[Y_{i}^{2}]=0.5*1+0.5*0=0.5$. Hence, $Var[Y_{i}]=E[Y_{i}^{2}]-E[Y_{i}]^{2}=.5-.25=0.25$. Using our answer to Exercise 3.15, we know that Var[Y] = $\sum_{i=1}^{\binom{n}{2}} Var[Y_{i}]=\frac{n(n-1)}{2*4}=\frac{n(n-1)}{8}$.

\subsubsection*{5} By Chebyshev's inequality, $Pr(|Y-E[Y]|\geq n) \leq \frac{Var[Y]}{n^{2}}$. Hence, $Pr(|Y-E[Y]|\geq n) \leq \frac{n(n-1)}{8n^{2}}=\frac{(n-1)}{8n}$.

\bibliographystyle{plain}
\bibliography{randomizedAlgorithms}

\end{document}
