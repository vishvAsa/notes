\documentclass[10pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

%opening
\title{CS388R: Answer to Homework 4}
\author{vishvAs vAsuki}

\begin{document}

\maketitle

\section{Question}
Exercise 5.17: Theorem 5.7 shows that any event that occurs with small probability in the balls-and-bins setting where the number of balls in each bin is an independent Poisson random variable also occurs with small probability in the standard balls-and-bins model. Prove a similar statement for random graphs: Every event that happens with small probability in the $G_{n,p}$ model also happens with small probability in the $G_{n,N}$ model for $N=\binom{n}{2}p$.

\subsection{Solution}
[Acknowledgement: I benefitted from discussions with Yuchul Kim in solving this.]

\paragraph{Notations}: $N=\binom{n}{2}p$. Let Yi be the probability that the ith edge is present in a graph chosen from $G_{n,p}$. Let Xi be the probability that the ith edge is present in a graph chosen from $G_{n,N}$.

\paragraph{Note}: The elementary event in the probability space of some thing happening when a graph is drawn from $G_{n,N}$ or $G_{n,p}$ is the event where a single graph G is drawn from those models. Any event happening in either model is then a collection of these elementary events. The events of these probability spaces can also be characterized by the set of edges they require a graph of n verteces to have.

\paragraph{Notations}: Let draw(G, $G_{n,N}$) represent the event where the particular graph G is drawn from $G_{n,N}$. Let draw(G, $G_{n,p}$) represent the event where the particular graph G is drawn from $G_{n,p}$.

\newtheorem{lem}{Lemma}
\begin{lem}\label{Lemma}
$Pr(draw(G, G_{n,N})) = Pr(draw(G, G_{n,p})|\sum_{i=1}^{\binom{n}{2}}Y_{i}=N)$
\end{lem}

\begin{proof}

Note that the graph G has N edges. So, $Pr(draw(G, G_{n,p}) \wedge \sum_{i=1}^{\binom{n}{2}} Y_{i}=N) = Pr(draw(G, G_{n,p}))$, as events are the same in either case.

\begin{eqnarray}
Pr(draw(G, G_{n,p})|\sum_{i=1}^{\binom{n}{2}}Y_{i}=N) &=& \frac{Pr(draw(G, G_{n,p}))}{Pr(\sum_{i=1}^{\binom{n}{2}}Y_{i}=N)}\\
&=& \frac{p^{N} (1-p)^{\binom{n}{2}-N}}{\binom{\binom{n}{2}}{N} p^{N} (1-p)^{\binom{n}{2}-N}}\\
&=& Pr(draw(G, G_{n,N}))\\
\end{eqnarray} 
\end{proof}

\newtheorem{thm}{Theorem}
\begin{thm}\label{Theorem}
Let f(x1,... xm), where m = $\binom{n}{2}$. Then, $E[f(X1,... Xm)] < \Theta(\binom{n}{2}N^{.5}) E[f(Y1,... Ym)]$.
\end{thm}
\begin{proof}
\begin{eqnarray}
E[f(Y1,... Ym)] &=& \sum_{k=0}^{\binom{n}{2}}E[f(Y1,... Ym)|\sum_{i=1}^{\binom{n}{2}}Y_{i}=k]Pr(\sum_{i=1}^{\binom{n}{2}}Y_{i}=k) \\
&\geq& E[f(Y1,... Ym)|\sum_{i=1}^{\binom{n}{2}}Y_{i}=N]Pr(\sum_{i=1}^{\binom{n}{2}}Y_{i}=N)\\
&&\text{Applying the lemma:}\\
&=& E[f(X1,... Xm)]Pr(\sum_{i=1}^{\binom{n}{2}}Y_{i}=N)\\
&=& E[f(X1,... Xm)]\binom{\binom{n}{2}}{N} p^{N} (1-p)^{\binom{n}{2}-N}\\
&\geq& E[f(X1,... Xm)](\frac{\binom{n}{2}-N+1}{\binom{n}{2}}) \frac{N^{N}}{N!} (1-p)^{\binom{n}{2}-N}\\
&\geq& E[f(X1,... Xm)](\frac{\binom{n}{2}-N+1}{\binom{n}{2}}) \frac{e^{N}}{eN^{.5}} (1-p)^{\binom{n}{2}-N}\\
&\geq& E[f(X1,... Xm)]\Theta(\frac{1}{\binom{n}{2}N^{.5}}e^{N}e^{-N+1/\binom{n}{2}})\\
&\geq& E[f(X1,... Xm)]\Theta(\frac{1}{\binom{n}{2}N^{.5}})\\
\end{eqnarray} 
\end{proof}

\newtheorem{cor}{Corollary}
\begin{cor}\label{The main result}
Every event that happens with small probability in the $G_{n,p}$ model also happens with small probability in the $G_{n,N}$ model for $N=\binom{n}{2}p$.
\end{cor}
\begin{proof}
We note that the selection of a particular graph G corresponds to a certain assignment of values to an indicator function f(x1,... xm), where m = $\binom{n}{2}$. This function maps a list of indicator random variables x1,... xm (which correspond to edges of a graph) to 0 or 1 depending on whether the graph is selected using a certain random graph distribution. Using the above theorem, we note that $E[f(X1,... Xm)] < \Theta(\binom{n}{2}N^{.5}) E[f(Y1,... Ym)]$.. But, for the indicator function described above, $E[f(X1,... Xm)]=Pr(f(X1,... Xm)=1)$ and $E[f(Y1,... Ym)]=Pr(f(Y1,... Ym)=1)$, which inturn correspond to the probabilities of the elementary events in the models $G_{n,N}$ and $G_{n,p}$. So, $Pr(f(X1,... Xm)=1) < \Theta(\binom{n}{2}N^{.5}) Pr(f(Y1,... Ym)=1)$.

Thus, every event that happens with small probability in the $G_{n,p}$ model also happens with small probability in the $G_{n,N}$ model for $N=\binom{n}{2}p$.
\end{proof}

Note: Taking an example where $2N=\binom{n}{2}$ will illustrate the effectiveness and limitations of this result. The result is useful only for inverse polynomial probabilities.

\section{Question}
Exercise 5.18: An undirected graph on n vertices is disconnected if there exists a set of $k<n$ vertices such that there is no edge between this set and the rest of the graph. Otherwise, the graph is said to be connected. Show that there exists a constant c such that if $N\geq cn \log n$ then, with probability 1-o(1), a graph randomly chosen from $G_{n,N}$ is connected.

\subsection{Solution}

\paragraph{Notations}: Let draw(G, $G_{n,N}$) represent the event where the particular graph G is drawn from $G_{n,N}$. Let draw(G, $G_{n,p}$) represent the event where the particular graph G is drawn from $G_{n,p}$.

We define the event D(G,k): In a randomly chosen graph with n verteces, there exists a set of $k<n$ vertices such that there is no edge between this set and the rest of the graph G. Let $D(G)=\sum_{k=1}^{n/2}D(G,k)$.

Let $N=\binom{n}{2}p$.

\begin{proof}
When the event D(G,k) happens, none of k(n-k) edges between two sets of verteces exists. Consider the graph G where D(G,k) is true. Note that the graph G has N edges. $Pr(D(G,k)|draw(G, G_{n,p})) = (1-p)^{k(n-k)}\binom{n}{k}$. By the union bound, $Pr(D(G)|draw(G, G_{n,p})) \leq \sum_{k=1}^{n/2}(1-p)^{k(n-k)}\binom{n}{k}$. $\forall k, (1-p)^{k(n-k)} \leq (1-p)^{nk/2}$. Also, $\binom{n}{k} \leq n^{k}$. So, $Pr(D(G)|draw(G, G_{n,p})) \leq \sum_{k=1}^{n/2}(1-p)^{nk/2}n^{k} \approx \sum_{k=1}^{n/2}e^{-c(\ln n)k/2}n^{k} = \sum_{k=1}^{n/2}n^{-dk+k} = n^{-fk}/2$, where d and f are constants used to simplify the calculation. (Note that we have used the approximation $(1-x^{-1})^{x}=e^{-x}$, as we are concerned with asymptotics in this proof.)

Note that the above derivation implies that for all values of $N>c\ln n$, $Pr(D(G,k)|draw(G, G_{n,p}))$ is an arbitrary inverse polynomial. The probability with which the event happens in the $G_{n,p}$ model is o(1). Hence, using the result from the answer to the first question, we conclude that the probability with which it happens in $G_{n,N}$ is o(1).

Thus, there exists a constant c such that if $N\geq cn \log n$ then, with probability 1-o(1), a graph randomly chosen from $G_{n,N}$ is connected.
\end{proof}

\section{Question}
Exercise 5.21: In hashing with open addressing, the hash table is implemented as an array and there are no linked lists or chaining. Each entry in the array either contains one hashed item or is empty. The hash function defines, for each key k, a probe sequence h(k, 0), h(k, 1), ... of table locations. To insert the key k, we first examine the sequence of table locations in the order defined by the key's probe sequence until we find an empty location; then we insert the item at that position. When searching for an item in the hash table, we examine the sequence of table locations in the order defined by the key's probe sequence until either the item is found or we have found an empty location in the sequence. If an empty location is found, this means the item is not present in the table.

An open-address hash table with 2n entries is used to store n items. Assume that the table location h(k, j) is uniform over the 2n possible table locations and that all h(k, j) are independent.

\subsubsection{1.} Show that, under these conditions, the probability of an insertion requiring more than k probes is at most $2^{-k}$.

\subsubsection{2.} Show that, for i=1, 2, ..., n, the probability that the ith insertion requires more than 2 log n probes is at most $1/n^{2}$.

Let the random variable $X_{i}$ denote the number of probes required by the ith insertion. You have shown in part (b) that $Pr(X_{i}>2 \log n)\leq 1/n^{2}$. Let the random variable X= $max_{1\leq i \leq n} X_{i}$ denote the maximum number of probes required by any of the n insertions.

\subsubsection{3.} Show that $Pr(X>2 \log n)\leq 1/n$.
\subsubsection{4.} Show that the expected length of the longest probe sequence is $E[X]=O(\log n)$.

\subsection{Solution}

\subsubsection{1.} Consider a random probe sequence. A probe, or an attempt to insert an item into the hash table, fails with the probability p. The highest failure rate happens when there are n-1 items already present in the hash table. In this case, as every probe is uniformly and independently chosen, $p = (n-1)/2n \leq 1/2$. Hence, for any probe, the probability of failure is at most 1/2.

An insertion requiring more than k probes happens exactly when the first k probes fail. As all probes are mutually independent, this happens with the probability at most $(1/2)^{k}=2^{-k}$.

\subsubsection{2.} 
Let the random variable $X_{i}$ denote the number of probes required by the ith insertion. We want $Pr(X_{i}>2\log n)$. But we have shown, in our answer to the previous subquestion, that $Pr(X_{i}>k) \leq 2^{-k}$. Hence, $Pr(X_{i}>2 \log n) \leq 2^{-2 \log n}=n^{-2}$


\subsubsection{3.} 
Let the random variable X= $max_{1\leq i \leq n} X_{i}$ denote the maximum number of probes required by any of the n insertions. We have shown in part (b) that $Pr(X_{i}>2 \log n)\leq 1/n^{2}$. We know that $X>2 \log n$ happens exactly when, for some i, $X_{i} > 2 \log n$. Hence, by applying the union bound, we see that $Pr(X>2 \log n) \leq \sum_{i=1}^{n} Pr(X_{i}>2 \log n) \leq n/n^{2}$. Hence, $Pr(X>2 \log n)\leq 1/n$.

\subsubsection{4.} We want to show that the expected length of the longest probe sequence is $E[X]=O(\log n)$.

First, we observe that $X \in \{0,1,...n\}$. So, the maximum value of X is n. We have already shown that $Pr(X>2 \log n)\leq 1/n$. From this, we know: $Pr(X \leq 2 \log n)\geq (n-1)/n$.

\begin{eqnarray}
\therefore E[X] &\leq& 2 \log n Pr(X \leq 2 \log n) + n Pr(X>2 \log n)\\
&&\text{As $n \geq 2\log n$:}\\
&\leq& 2 \log n \frac{n-1}{n} + n (1/n)\\
&=& 2 \log n -\frac{2 \log n}{n} + 1\\
\end{eqnarray} 

Thus, $E[X]=O(\log n)$.

\section{Question}
Exercise 5.22: Bloom filters can be used to estimate set differences. Suppose you have a set X and I have a set Y, both with n elements. For example, the sets might represent our 100 favorite songs. We both create Bloom filters of our sets, using the same number of bits m and the same k hash functions. Determine the expected number of bits where our Bloom filters differ as a function of m, n, k, and $|X \cap Y|$. Explain how this could be used as a tool to find people with the same taste in music more easily than comparing lists of songs directly.

\subsection{Solution}
[Acknowledgement: I benefitted from discussions with Qi Li and Yuchul Kim in solving this.]

$|X|=|Y|=n$. Number of bits in each bloom filter=m. Number of hash functions for each person=k. Let $|X \cap Y|=C$, and $|C|=c$. Let B1 and B2 be the two bloom filters.

Let $U_{1,i}$ be an indicator random variable, which is 1 if the ith bit in B1 is 1, and the ith bit in the B2 is 0. Similarly, we define $U_{2,i}$ for the case where only the ith bit in B2 is 1.

Pr(none of the songs in C set bit i in B1 or B2) $=(1-m^{-1})^{kc}$. This is because 1-1/m is the probability with which any hash function would leave the ith bit unset. Also, the bits corresponding to the songs in C hold identical values in both bloom filters. 

Due to similar reasons, Pr(none of the songs in Y-C set bit i in B2) = $(1-m^{-1})^{k(n-c)}$.

Pr(songs in X-C set bit i in B1) = 1 - Pr(none of the songs in X-C set bit i in B1) = $(1-(1-m^{-1})^{k(n-c)})$.

Then, $Pr(U_{1,i}=1)=$ Pr(none of the songs in C set bit i in B1 or B2) * Pr(none of the songs in Y-C set bit i in B2) * Pr(songs in X-C set bit i in B1)= $(1-m^{-1})^{kc}  (1-m^{-1})^{k(n-c)} (1-(1-m^{-1})^{k(n-c)}) =(1-m^{-1})^{kn}(1-(1-m^{-1})^{k(n-c)})$. For symmetrical reasons, $Pr(U_{2,i}=1)$ also has this value. This is also the value of $E[U_{1,i}]=E[U_{2,i}]$.

Furthermore, let $U_{1} = \sum_{i=1}^{m} U_{1,i}$ and let $U_{2} = \sum_{i=1}^{m} U_{2,i}$. Then, by linearity of expectations, $E[U_{1}]=E[U_{2}]= m(1-m^{-1})^{kn}(1-(1-m^{-1})^{k(n-c)})$.

Thus, in order to determine the expected number of bits where our Bloom filters differ, we want the to find $E[U_{1}+U_{2}]$. By linearity of expectations, this is: $2m(1-m^{-1})^{kn}(1-(1-m^{-1})^{k(n-c)})$.

\textbf{Explanation of utility:} This could be used as a tool to find people with the same taste in music more easily than comparing lists of songs directly. The lesser the number of bits which are different in the bloom filters, the more likely it is that the corresponding people have similar taste in music. Comparison of the bits in the bloom filters can be done in time linear in the number of bits, O(m). But, if one were to compare two unsorted lists of n songs, one would require, at best, O(n) or O(nlogn) time, which is significantly higher. Furthermore, if the bloom filters are used as addresses of a hash table, one could answer a wider variety of queries about the differences in music tastes of people.
.
\section{Question}
Exercise 6.3: Given an n-vertex undirected graph G=(V, E), consider the following method of generating an independent set. Given a permutation a of the vertices, define a subset $S(\sigma)$ of the vertices as follows: for each vertex i, $i\in S(\sigma)$ if and only if no neighbor j of i precedes i in the permutation $\sigma$.

\subsubsection{1.} Show that each $S(\sigma)$ is an independent set in G.
\subsubsection{2.} Suggest a natural randomized algorithm to produce $\sigma$ for which you can show that the expected cardinality of $S(\sigma)$ is $\sum_{i=1}^{n}\frac{1}{d_{i}+1}$, where $d_{i}$ denotes the degree of vertex i.
\subsubsection{3.} Prove that G has an independent set of size at least $\sum_{i=1}^{n}\frac{1}{d_{i}+1}$.

\subsection{Solution}
\subsubsection{1.} We want to show that each $S(\sigma)$ is an independent set in G. Consider the construction of $S(\sigma)$. For each vertex i, $i\in S(\sigma)$ if and only if no neighbor j of i precedes i in the permutation $\sigma$. This means that, given a vertex $i \in S(\sigma)$, we can be sure that no neighbor of i preceeds i in $\sigma$, and that every neighbor of i, which can only come after i in $\sigma$, is not added to $S(\sigma)$. Hence, we can conclude that every vertex $i\in S(\sigma)$ has no neighbor in $S(\sigma)$. In other words, $S(\sigma)$ is an independent set.

\subsubsection{2.} A natural randomized algorithm to produce $\sigma$ would be the following:

\begin{itemize}
\item \textbf{Input:} A set of verteces V.
\item \textbf{Output:} Permutation s
\item \textbf{Algorithm:}
\subitem Initialize s, a new permutation.
\subitem Do the following until s includes all verteces in V:
\subsubitem Select a vertex in V which is not in s at random.
\subsubitem Append this vertex to s.
\subitem Return s.
\end{itemize}

With the above algorithm, all permutations are equally likely.

Consider the process of creation of S(s). What is the probability that the vertex i is added to S(s)? Let the indicator random variable for this event be $X_{i}$. We note that the vertex i is added to s iff it is the first among $d_{i}+1$ verteces ($X_{i}$ and neighbors of $X_{i}$) in the permutation s, where $d_{i}$ denotes the degree of vertex i. As all permutations are equally likely, we see that the probability of this happening is $\frac{1}{d_{i}+1}$. So, $E[X_{i}]=P(X_{i}=1)=\frac{1}{d_{i}+1}$. The cardinality of S(s) is given by the random variable $X=\sum_{i=1}^{n}X_{i}$. Due to linearity of expectations, we see that $E[X]=\sum_{i=1}^{n}\frac{1}{d_{i}+1}$. Hence, the expected cardinality of S(s) is $\sum_{i=1}^{n}\frac{1}{d_{i}+1}$.

\subsubsection{3.} We want to prove that G has an independent set of size at least $\sum_{i=1}^{n}\frac{1}{d_{i}+1}$.

We have already proved that the expected cardinality of S(s) is $E[X]=\sum_{i=1}^{n}\frac{1}{d_{i}+1}$. But, we know that, for any X, $Pr(X\geq E[X])>0$. Hence, $Pr(X \geq \sum_{i=1}^{n}\frac{1}{d_{i}+1})>0$. Hence, we conclude that there exists atleast one independent set in G with cardinality atleast $\sum_{i=1}^{n}\frac{1}{d_{i}+1}$.

\section{Question}
Exercise 6.8: Prove that, for every integer n, there exists a way to 2-color the edges of Kx so that there is no monochromatic clique of size k when $x=n-\binom{n}{k}2^{1-\binom{k}{2}}$.

(Hint: Start by 2-coloring the edges of Kn, then fix things up.)

\subsection{Solution}

[Acknowledgement: I benefitted from discussions with Jing-Tang Jang in solving this.]

Consider the complete graph Kn, and an ordering of all the $K_{k}$ cliques in Kn. Color this graph by coloring each edge with a uniformly and randomly chosen color. Let $X_{i}$ be an indicator random variable which is 1 if the ith $K_{k}$ is monochromatic. Then, we see that $E[X_{i}]=Pr(X_{i}=1)=2^{1-\binom{k}{2}}$. Then, due to linearity of expectations, the expected number of monochromatic cliques of size k in Kn is: $E[X=\sum_{i=1}^{n}X_{i}]=\binom{n}{k}2^{1-\binom{k}{2}} = n-x$.

But we know that $Pr(X \leq E[X])>0$. Hence, the probability that there exists a coloring of Kn, such that at most n-x k-sized monochromatic cliques exist is $Pr(X \leq n-x)>0$. In other words, there exists a coloring of Kn, such that at most n-x k-sized monochromatic cliques exist. Now, if we take this particluar coloring of Kn and remove n-x verteces such that one vertex in each monochromatic clique is removed, we end up with a complete graph Kx with no monochromatic $K_{k}$.

Thus, we have proved that, for every integer n, there exists a way to 2-color the edges of Kx so that there is no monochromatic clique of size k when $x=n-\binom{n}{k}2^{1-\binom{k}{2}}$.

\section{Question}
Exercise 6.14: Consider a graph in Gn,p, with p=1/n. Let X be the number of triangles in the graph, where a triangle is a clique with three edges. Show that
$Pr(X\geq 1)\leq 1/6$.
and that
$\lim \limits_{n \to \infty } Pr(X \geq 1)\geq 1/7$.

(Hint: Use the conditional expectation inequality.)

\subsection{Solution}
Let the set of verteces of G be V. Consider an enumeration of all subsets of V which are of the size 3. There are $\binom{n}{3}$ such sets. Let $X_{i}$ be an indicator random variable which is 1 if the corresponding set of 3 verteces is a triangle. Then, $X=\sum_{i=1}^{\binom{n}{3}} X_{i}$. But, $E[X_{i}]=Pr(X_{i})=p^{3}=\frac{1}{n^{3}}$. So, by linearity of expectations, $E[X]=\binom{n}{3}\frac{1}{n^{3}}=\frac{(n-1)(n-2)}{6n^{2}} $.

Now, we apply the Markov inequality.

\begin{eqnarray}
Pr(X>a) &\leq& E[X]/a\\
&&\text{Let $a = \frac{n^{2}}{(n-1)(n-2)}$}\\
&&\text{Substituting values:}\\
Pr(X > \frac{n^{2}}{(n-1)(n-2)}) &\leq& 1/6\\
But: \frac{n^{2}}{(n-1)(n-2)} &>& 1\\
\therefore Pr(X \geq 1) &\leq& 1/6\\
\end{eqnarray}

Thus, we have shown that $Pr(X\geq 1)\leq 1/6$.


\begin{eqnarray}
E[X|X_{i}=1] &=& \sum_{j=1}^{\binom{n}{3}} E[X_{j}|X_{i}=1]\\
&&\text{Accounting for sets which are influenced by $X_{i}$:}\\
&=& 1+3p^{2}+\sum_{j=1}^{\binom{n}{3}-3} E[X_{j}|X_{i}=1]\\
&&\text{As the remaining $X_{j}$ are not influenced by $X_{i}$:}\\
&=& 1+3p^{2}+\sum_{j=1}^{\binom{n}{3}-3} E[X_{j}]\\
&=& 1+3p^{2}+\sum_{j=1}^{\binom{n}{3}-3} p^{3}\\
&=& 1+3p^{2}+(\binom{n}{3}-3) p^{3}\\
&=& 1+3n^{-2}+(\binom{n}{3}-3) n^{-3}\\
&=& 1+3n^{-2}+(\frac{n(n-1)(n-2)}{6}-3) n^{-3}\\
\therefore \lim \limits_{n \to \infty } E[X|X_{i}=1] &=& 6/7\\
\end{eqnarray}

Applying the conditional expectation ineqality described in \cite{mitzenmacherUpfal}:
\begin{eqnarray}
Pr(X>0) &\geq& \sum_{i=1}^{\binom{n}{3}} \frac{Pr(X_{i}=1)}{E[X|X_{i}=1]}\\
&=& \frac{1}{E[X|X_{i}=1]}\sum_{i=1}^{\binom{n}{3}} Pr(X_{i}=1)\\
&=& \frac{1}{E[X|X_{i}=1]}\sum_{i=1}^{\binom{n}{3}} Pr(X_{i}=1)\\
&=& \frac{1}{E[X|X_{i}=1]} \binom{n}{3} n^{-3}\\
But: \lim \limits_{n \to \infty } \binom{n}{3} n^{-3}&=& \frac{1}{6}\\
\therefore \lim \limits_{n \to \infty } Pr(X>0) &\geq& \frac{1/6}{6/7} = 1/7\\
\end{eqnarray}

Hence, as X's range is $ {0,1,2,...}$, $\lim \limits_{n \to \infty } Pr(X \geq 1)\geq 1/7$.

\section{Question}
Exercise 6.15: Consider the set-balancing problem of Section 4.4. We claim that there is an n*n matrix A for which $||Ab||_{\infty}$ is $\Omega(\sqrt{n})$ for any choice of b. For convenience here we assume that n is even.

\subsubsection{1.} We have shown in Eqn. (5.5) that $n! \leq e \sqrt{n}(\frac{n}{e})^{n}$. Using similar ideas, show that $n! \geq a \sqrt{n}(\frac{n}{e})^{n}$ for some positive constant a.
\subsubsection{2.} Let $b_{1}, b_{2}, ..., b_{m/2}$ all equal 1, and let $b_{m/2+1}$, $b_{m/2+2}, ..., b_{m}$ all equal -1. Let Y1, Y2, ...,Ym each be chosen independently and uniformly at random from {0, 1}. Show that there exists a positive constant c such that, for sufficiently large m,
$Pr(|\sum_{i=1}^{m}b_{i}Y_{i}|>c\sqrt{m})>1/2$. (Hint: Condition on the number of $Y_{i}$ that are equal to 1.)
\subsubsection{3.} Let b1, b2, ..., bm each be equal to either 1 or -1. Let Y1, Y2, ..., Ym each be chosen independently and uniformly at random from ${0, 1}$. Show that there exists a positive constant c such that, for sufficiently large m, $Pr(|\sum_{i=1}^{m}b_{i}Y_{i}|>c\sqrt{m})>1/2$.
\subsubsection{4.} Prove that there exists a matrix A for which $||Ab||_{\infty}$ is $\Omega(\sqrt{n})$ for any choice of b.

\subsection{Solution}

\subsubsection{1.}
Acknowledgement: I benefitted from discussions with Aditya Saurabh in solving this.

\begin{proof}
\begin{eqnarray}
\int_{i-1}^{i} \ln x dx &\leq& \ln(i-0.5)\\
&=& \ln i + \ln(1-0.5/i)\\
&& \text{Considering the logarithmic series:}\\
&<& \ln i - \frac{1}{2i}\\
\therefore \int_{1}^{n}\ln x dx &\leq& \sum_{i=2}^{n}\ln i - \sum_{i=2}^{n}\frac{1}{2i}\\
n\ln n -n +1 &\leq& ln(n!) - \frac{\ln n}{2} -\frac{1}{2}\\
&& \text{Taking exponents:}\\
n! &\geq& \sqrt{e}\sqrt{n}(\frac{n}{e})^{n}\\
\end{eqnarray}
\end{proof}

Thus, $n! \geq a \sqrt{n}(\frac{n}{e})^{n}$ for some positive constant a.

\subsubsection{2.}
Variance of the binomial distribution is m/2. Furthermore, the binomial distribution is symmetric.

\subsubsection{3.}


\subsubsection{4.}

We want to prove that there exists a matrix A for which $||Ab||_{\infty}$ is $\Omega(\sqrt{n})$ for any choice of b.

Consider any choice of values for the vector b. Pick the elements of A uniformly and randomly from {0,1}. Consider row j of A. From subquestion 3, we know that there exists a positive constant c such that, for sufficiently large n, $Pr(|\sum_{i=1}^{n}A_{ji}b_{i}|>c\sqrt{m})>1/2$. Thus, there is a non zero probability that the product of A's ith row with b is at least $c\sqrt{m}$. As $||Ab||_{\infty}$ is the maximum of these products, there is a non zero probability that $||Ab||_{\infty}$ is $\Omega(\sqrt{n})$.

Hence, we conclude that there exists a matrix A for which $||Ab||_{\infty}$ is $\Omega(\sqrt{n})$ for any choice of b.


\bibliographystyle{plain}
\bibliography{../randomizedAlgorithms}

\end{document}
