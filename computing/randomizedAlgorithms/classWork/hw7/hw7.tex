\documentclass[10pt]{amsart}
\usepackage{amssymb}


\newtheorem{thm}{Theorem}[subsection]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}

\theoremstyle{remark}
\newtheorem{defn}[thm]{Definition}
\newtheorem*{ack}{Acknowledgement}
\newtheorem{alg}[thm]{Algorithm}
\newtheorem{rem}[thm]{Remark}

%opening
\title{CS388R: Answer to Homework 7}
\author{vishvAs vAsuki}

\begin{document}

\maketitle

\section{Question}
1. Exercise 12.5, page 309. Exercise 12.5: Consider the gambler's ruin problem, where now the independent games are such that the player either wins one dollar with probability $p<1/2$ or loses one dollar with probability 1-p. As in Exercise 12.4, the player continues until either losing l1 dollars or winning l2 dollars. Let Xn be 1 if the player wins the nth game and -1 otherwise, and let Zn be the player's total winnings after n games.

\subsubsection{1.}

Show that
$A_{n}=(\frac{1-p}{p})^{Z_{n}}$

is a martingale with mean 1.

\subsubsection{2.}Determine the probability that the player wins l2 dollars before losing l1 dollars.

\subsubsection{3.}Show that
$B_{n}=Z_{n}-(2p-1)n$
is a martingale with mean 0.
\subsubsection{4.}Let T be the stopping time when the player finishes playing. Determine $E[Z_{T}]$ and use it to determine E[T]. (Hint: You can use what you already know about the probability that the player wins.)


\subsection{Solution}

\subsubsection{1.}
\begin{thm}
$A_{n}=(\frac{1-p}{p})^{Z_{n}}$ is a martingale with mean 1.
\end{thm}
\begin{proof}
Given a value of p, $A_{n}$ depends only on $Z_{n}$. But, $Z_{n} = \sum_{i=1}^{n} X_{i}$. Hence, $A_{n}$ depends only on $X_{1}, \dots X_{n}$.

As $p<1/2$, we note that $\frac{1-p}{p} > 1$. $E[|A_{n}|] \leq E[(\frac{1-p}{p})^{Z_{n}}] \leq (\frac{1-p}{p})^{l_{2}}$.

\begin{eqnarray*}
E[A_{n+1}|X_{1}, \dots X_{n}] &=& E[A_{n+1}|Z_{n}] \\
&=& E[(\frac{1-p}{p})^{Z_{n+1}}|Z_{n}] \\
&=& p(\frac{1-p}{p})^{Z_{n}+1} + (1-p)(\frac{1-p}{p})^{Z_{n}-1}\\
&=& (1-p)(\frac{1-p}{p})^{Z_{n}} + (p)(\frac{1-p}{p})^{Z_{n}}\\
&=& (\frac{1-p}{p})^{Z_{n}}\\
&=& A_{n}\\
\end{eqnarray*} 

Hence, observing the above properties, the sequence $A_{0} \dots A_{n}$ is a martingale with respect to $X_{1}, \dots X_{n}$.

So, $E[A_{n}] = E[A_{1}] = p(\frac{1-p}{p}) + (1-p)(\frac{1-p}{p})^{-1} = 1$. In other words, the mean of this martingale is 1.
\end{proof}

\subsubsection{2.}
Let T be the first time when the player wins l2 dollars or looses l1 dollars. As T depends only on $Z_{1}, \dots Z_{n}$, it is a valid stopping time. $A_{n} \leq (\frac{1-p}{p})^{l_{2}}$. So, we can apply the Martingale stopping theorem from \cite{mitzenmacherUpfal}.

So, using another result shown earlier, $E[A_{T}] = E[A_{1}] = 1$. At time T, the player either wins l2 dollars with probability q or looses l1 dollars with probability (1-q). So, $1 = q(\frac{1-p}{p})^{l_{2}} + (1-q)(\frac{1-p}{p})^{-l_{1}}$. So, $q((\frac{1-p}{p})^{l_{2}} - (\frac{1-p}{p})^{-l_{1}}) = 1 - (\frac{1-p}{p})^{-l_{1}}$. So, the probability of winning is l2 dollars is :

\begin{eqnarray*}
q &=& \frac{1 - (\frac{1-p}{p})^{-l_{1}}}{(\frac{1-p}{p})^{l_{2}} - (\frac{1-p}{p})^{-l_{1}}}\\
&=& \frac{(\frac{1-p}{p})^{l_{1}} - 1}{(\frac{1-p}{p})^{l_{2}+l_{1}} - 1}\\
\end{eqnarray*}

\subsubsection{3.}
\begin{thm}
$B_{n}=Z_{n}-(2p-1)n$ is a martingale with mean 0.
\end{thm}
\begin{proof}
Given a value of p, $B_{n}$ depends only on $Z_{n}$. But, $Z_{n} = \sum_{i=1}^{n} X_{i}$. Hence, $B_{n}$ depends only on $X_{1}, \dots X_{n}$.

As $p<1/2$, we note that $(2p-1) < 0$. $E[|B_{n}|] leq E[|Z_{n}|]-(2p-1)E[n] \leq max(l_{1}, l_{2})-(2p-1)n$.

\begin{eqnarray*}
E[B_{n+1}|X_{1}, \dots X_{n}] &=& E[B_{n+1}|Z_{n}] \\
 &=& E[Z_{n+1}]-(2p-1)E[(n+1)]\\
 &=& p(Z_{n}+1) + (1-p)(Z_{n}-1)-(2p-1)(n+1)\\
 &=& Z_{n} + (2p-1) -(2p-1)(n+1)\\
 &=& Z_{n} -(2p-1)(n)\\
 &=& B_{n}\\
\end{eqnarray*}

Hence, observing the above properties, the sequence $B_{0} \dots B_{n}$ is a martingale with respect to $X_{1}, \dots X_{n}$.

So, $E[B_{n}] = E[B_{1}] = p(1-(2p-1)) + (1-p)(-1-(2p-1)) = 0$. In other words, the mean of this martingale is 0.
\end{proof}

\subsubsection{4.}
Let T be the first time when the player wins l2 dollars or looses l1 dollars. As T depends only on $Z_{1}, \dots Z_{T}$ and inturn on $X_{1}, \dots X_{T}$, it is a valid stopping time. At time T, the player either wins l2 dollars with probability q or looses l1 dollars with probability (1-q). We have already determined the value of q.

$Z_{n}$ is not a martingale! Note that $Z_{T} = \sum_{i=1}^{T}X_{T}$. So, $E[Z_{T}] = E[\sum_{i=1}^{T}X_{i}]$. Due to Wald's equation, $E[T]E[X] = E[Z_{T}]$. But, we know that $E[X] = p - (1-p) = 2p-1$.

\begin{eqnarray*}
E[T]E[X] &=& E[Z_{T}]\\
E[T](2p-1) &=& ql_{2} + (1-q)l_{1}\\
E[T] &=& (2p-1)^{-1}(\frac{(\frac{1-p}{p})^{l_{1}} - 1}{(\frac{1-p}{p})^{l_{2}+l_{1}} - 1}l_{2} + \frac{((\frac{1-p}{p})^{l_{2}+l_{1}} - (\frac{1-p}{p})^{l_{1}}}{(\frac{1-p}{p})^{l_{2}+l_{1}} - 1}l_{1})\\
\end{eqnarray*} 

\section{Question}
2. Exercise 12.10, page 310. Exercise 12.10: A parking-lot attendant has mixed up n keys for n cars. The n car owners arrive together. The attendant gives each owner a key according to a permutation chosen uniformly at random from all permutations. If an owner receives the key to his car, he takes it and leaves; otherwise, he returns the key to the attendant. The attendant now repeats the process with the remaining keys and car owners. This continues until all owners receive the keys to their cars.

Let R be the number of rounds until all car owners receive the keys to their cars. We want to compute E[R]. Let Xi be the number of owners who receive their car keys in the ith round. Prove that
$Y_{i}=\sum_{j=1}^{i}(X_{j}-E[X_{j}|X_{1},\dots X_{j-1}])$ is a martingale. Use the martingale stopping theorem to compute E[R].

\subsection{Solution}
\begin{ack}
Clues from Padmadevan Chettiar and Jing-Tang helped me solve this problem.
\end{ack}

\begin{thm}
$Y_{i}=\sum_{j=1}^{i}(X_{j}-E[X_{j}|X_{1},\dots X_{j-1}])$ is a martingale with mean 0.
\end{thm}
\begin{proof}
$Y_{i}$ depends only on $X_{1},\dots X_{i}$. Also, $E[Y_{i}] < ni$

\begin{eqnarray*}
Y_{i+1} &=& \sum_{j=1}^{i}(X_{j}-E[X_{j}|X_{1},\dots X_{j-1}])\\
&=& Y_{i} + X_{i+1}-E[X_{i+1}|X_{1},\dots X_{i}]\\
E[Y_{i+1}|X_{1},\dots X_{i}] &=& E[Y_{i}|X_{1},\dots X_{i}] + E[X_{i+1}|X_{1},\dots X_{i}]\\
&& -E[E[X_{i+1}|X_{1},\dots X_{i}]|X_{1},\dots X_{i}]\\
&=& E[Y_{i}|Y_{i}] + E[X_{i+1}|X_{1},\dots X_{i}]\\
&& -E[X_{i+1}|X_{1},\dots X_{i}]\\
&=& Y_{i}\\
\end{eqnarray*}
Hence, observing the above properties, we conclude that the sequence $Y_{1} \dots Y_{n}$ is a martingale with respect to $X_{1}, \dots X_{n}$.

Hence, we see that $E[Y_{i}] = E[Y_{1}] = E[X_{1}]-E[E[X_{1}]] = 0$.
\end{proof}

Let R be the number of rounds until all car owners receive the keys to their cars. R is a valid stopping time, as it does not depend on values $X_{i}$ for $i>R$.

Suppose that at a certain point in time, k keys have not been returned to their owners. Let Aj be an indicator random variable which is 1 when the jth person gets his key. Then, applying linearity of expecatations, we see that the expected number of people who get their key at that point in time is $E[X_{j}|X_{1},\dots X_{j-1}] = \sum_{j=1}^{k} E[A_{j}] = k(1/k) = 1$.

Suppose that at round i, there are k unreturned keys. $Pr(X_{i}=0)=\frac{k-1}{k}\frac{k-2}{k-1}\dots1 \\
= \frac{1}{k} \leq \frac{1}{2}$. So, $Pr(R > 2f(n)) < 2^{-2f(n)}$ for any $f(n) = \Omega(n)$. So, $E[R] = O(n) < \infty$. We have already shown that $Y_{i+1} = Y_{i} + X_{i+1}-E[X_{i+1}|X_{1},\dots X_{i}]$. Also, $E(|Y_{i+1}-Y_{i}|) = E(|X_{i+1}-E[X_{i+1}|X_{1},\dots X_{i}]|) \leq E(X_{i+1}+E[X_{i+1}|X_{1},\dots X_{i}]) \leq 2$. So, the martingale stopping theorem can be applied.

Applying the martingale stopping theorem, we see that $E[Y_{R}] = E[Y_{1}] = 0$. So, applying Wald's equation, $E[\sum_{j=1}^{R}X_{j}] - E[\sum_{j=1}^{R}E[X_{j}|X_{1},\dots X_{j-1}]] = n - E[R]E[X_{j}|X_{1},\dots X_{j-1}] = n - E[R] = 0$. Hence, $E[R] = n$.

\section{Question}
3. Exercise 12.11, page 310. Exercise 12.11: Alice and Bob play each other in a checkers tournament, where the first player to win four games wins the match. The players are evenly matched, so the probability that each player wins each game is 1/2, independent of all other games. The number of minutes for each game is uniformly distributed over the integers in the range [30, 60], again independent of other games. What is the expected time they spend playing the match?

\subsection{Solution}
Let $X_{i}$ be +1 if Alice wins the game, -1 otherwise. Let $Y_{i} = \sum_{j=1}^{i} X_{j}$ be the number of games Alice has won after i games. Let G be the number of games played upto and including the point when some player wins four games.

Let $T_{i}$ be the time spent playing the ith game. As T is uniformly distributed over the integers in the range [30, 60], $E[T] = 30 + \sum_{i=0}^{30}i/31 = 45$.

Note that the range of G is the set of integers in [4,7]. The procedure to count the number of ways in which (G=g) can happen is as follows:
\begin{itemize}
\item Consider the number of ways to pick the person, P, who wins the match. There are 2 ways of doing this.
\item For G=g, P must loose exactly g-4 games. P can loose in any one of the first g-1 games. The number of ways this can happen is $\binom{g-1}{g-4} = \binom{g-1}{3}$.
\end{itemize}

Using the above, we see that $Pr(G=g) = 2\binom{g-1}{g-4}(2)^{-g}$. So, $Pr(G=4) = 2\binom{3}{0}(2)^{-4} = (2)^{-3}$. $Pr(G=5) = 2\binom{4}{1}(2)^{-5} = 2(2)^{-3}$. $Pr(G=6) = 2\binom{5}{2}(2)^{-6} = 5(2)^{-4}$. $Pr(G=7) = 2\binom{6}{3}(2)^{-7} = 5(2)^{-4}$. Hence, $E[G] = (2)^{-3}(4 + 10 + 15 +35/2) = (2)^{-3}(46.5)$.

Note that G is a valid stopping time, as it does not depend on any $T_{i}$. Due to Wald's equation, we know that $E[\sum_{i=1}^{G}T_{i}] = E[G]E[T] \\
= (2)^{-3}(46.5)(45) = 261.56$.

\section{Question}
4. Exercise 12.15, page 312. Exercise 12.15: A subsequence of a string s is any string that can be obtained by deleting characters from s. Consider two strings x and y of length n, where each character in each string is independently a 0 with probability 1/2 and a 1 with probability 1/2. We consider the longest common subsequence of the two strings.

\subsubsection{1.}
Show that the expected length of the longest common subsequence is greater than $c_{1}n$ and less than $c_{2}n$ for constants $c_{1}>1/2$ and $c_{2}<1$ when n is sufficiently large. (Any constants c1 and c2 will do; as a challenge, you may attempt to find the best constants c1 and c2 that you can.)
\subsubsection{2.}
Use a martingale inequality to show that the length of the longest common subsequence is highly concentrated around its mean.

\subsection{Solution}

Let Ai denote the ith bit in the first string. Let Bi denote the ith character in the second string. Let L be the length of the longest common subsequence.

\subsubsection{1.}
\begin{thm}
$c_{1}=9/16$, $c_{1}n \leq E[L]$.
\end{thm}
\begin{proof}
Let L' be the length of a special type of longest common subsequence defined as follows: Let i be an even number from the domain [0, n-1]. Let $L_{i}$ be the length of the longest common subsequence between the 2-letter strings, $(A_{i}, A_{i+1})$ and $(B_{i}, B_{i+1})$. Then, $L = \sum L'_{i}$, for the n/2 values of i.

We see that when $(A_{i}, A_{i+1}) = (0,0)$, $(B_{i}, B_{i+1})$ can be (0,0), (0,1), (1,0), (1,1) with equal probability. $L'_{i}$ is 2, 1, 1, 0 respectively in these cases. We see that when $(A_{i}, A_{i+1}) = (0,1)$ or (1,1), $(B_{i}, B_{i+1})$ can be (0,0), (0,1), (1,0), (1,1) with equal probability. So, $L'_{i}$ is 1, 2, 1, 1 respectively in these cases. By such enumeration, we see that $E[L'_{i}] = 16^{-1}(4 + 5 + 4 + 5) = 8^{-1}(4 + 5) = 9/8$.

By linearity of expectations, $L = 9n/16$.

Clearly $L' \leq L$, as there are restrictions about bit positions are placed on L' besides the order of the bits. Hence, for $c_{1}=9/16$, $c_{1}n \leq E[L]$.
\end{proof}

\begin{thm}
$c_{2}=.99n$, $c_{2}n > E[L]$ for large n.
\end{thm}
\begin{proof}
For $L > .98n$, atleast .98n bits in A and B should agree.
\begin{eqnarray}
Pr(L > .98n) &\leq& \binom{n}{.98n} \binom{n}{.98n} 2^{-.98n}\\
&=& \binom{n}{.02n}^{2} 2^{-.98n}\\
&\leq& \frac{en}{.02n}^{.04n} 2^{-.98n}\\
&=& (50e)^{.04n} 2^{-.98n}\\
&\leq& 2^{.28n} 2^{-.98n}\\
&=& 2^{-.7n}\\
E[L] &\leq& .98n + 2^{-.7n} n\\
&=& .98n + 2^{-.7n + log n} \\
&\leq& .98n + 1\\
&\leq& .99n\\
\end{eqnarray} 
\end{proof}

\subsubsection{2.}
Consider a stochastic process. For i from 0 to n-1, reveal pairs Pi of bits Ai and Bi at one time step. Let Xi be the longest common subsequence in the i bits revealed so far. Consider the following Doob martingale. $Xi = E[L|P0, P1, ... Pi]$.

\begin{thm}
Xi is a doob martingale on the sequence defined by Pi.
\end{thm}
\begin{proof}
Xi is a function of P0, P1, ... Pi. $E[|X_{i}|] \leq n$.

$E[X_{i+1}|P0, P1, \dots Pi] = E[E[L|P0, P1, \dots Pi+1]| P0, P1, \dots Pi] \\
= E[L|P0, P1, ... Pi] = X_{i}$.

So, Xi is a doob martingale on the sequence defined by Pi.
\end{proof}

When a new pair Pi of bits is revealed, it can increase or decrease \\
$E[L|P0, P1, ... Pi]$ by at most 1. (This is because the longest common subsequence among the bits revealed so far can change by at most 1 when a new pair of bits are revealed.) In other words, $|X_{i+1} - X_{i}| \leq 1$. So, due to Azuma's inequality, we see that $Pr(|X_{n} - X_{0}| \geq l\sqrt{n}) \leq 2e^{-l^{2}/2}$. But, as we argued earlier, when a new pair Pi of bits is revealed, $|E[L]-X_{0}| \leq 1$. Also, we know that $X_{n-1} = L$. So, $Pr(|L - E[L]| \geq l\sqrt{n} + 1) \leq 2e^{-l^{2}/2}$.

Let $c<1$ be a small constant. So, considering the case where $l=\frac{cE[L]}{\sqrt{n}}$, we see that $Pr(|L - E[L]| \geq cE[L] + 1) \leq 2e^{-\frac{(cE[L])^{2}}{2n}} = 2e^{-\Theta(1)}$. Hence, the length of the longest common subsequence is highly concentrated around its mean.

\section{Question}
5. Exercise 13.7, page 333. Exercise 13.7:

\subsubsection{1.} We have shown that the maximum load when n items are hashed into n bins using a hash function chosen from a 2-universal family of hash functions is at most $\sqrt{2n}$ with probability at least 1/2. Generalize this argument to k-universal hash functions. That is, find a value such that the probability that the maximum load is larger than that value is at most 1/2.
\subsubsection{2.} In Lemma 5.1 we showed that, under the standard balls-and-bins model, the maximum load when n balls are thrown independently and uniformly at random into n bins is at most 3 ln n/ln ln n with probability 1-1/n. Find the smallest value of k such that the maximum load is at most 3 ln n/ln ln n with probability at least 1/2 when choosing a hash function from a k-universal family.

\subsection{Solution}
\subsubsection{1.} 
Let X be the total number of k-way collisions. There are m balls (items being hashed), and n bins (hash table positions). Enumerate all possible k-sized subsets of the items being hashed. Let $X_{i}$ be an indicator random variable which is 1 if there is a collision involving all items in the kth subset. So, $X = \sum X_{i}$, and $E[X] = \sum E[X_{i}]$ due to linearity of expectations.

Hash function is chosen from a k-universal family. Let Y be the maximum load.

\begin{eqnarray}
E[X_{i}] = Pr(X_{i}=1) &\leq& \frac{1}{n^{k-1}} \\
E[X_{i}] &\leq& \binom{m}{k} \frac{1}{n^{k-1}} < \frac{m^{k}}{k!n^{k-1}}\\
Pr(X \geq \frac{2m^{k}}{k!n^{k-1}}) &\leq& 2^{-1}\\
Pr(\binom{Y}{k} \geq {2m^{k}}{k!n^{k-1}}) &\leq& Pr(X \geq \frac{2m^{k}}{k!n^{k-1}}) \leq 2^{-1}\\
But: \binom{Y}{k} &\leq& \frac{Y^{k}}{k!}\\
Pr(\frac{Y^{k}}{k!} \geq {2m^{k}}{k!n^{k-1}}) &\leq& 2^{-1}\\
Pr(Y \geq m\sqrt[k]{\frac{2}{n^{k-1}}}) &\leq& 2^{-1}\\
\end{eqnarray} 

When m=n, we see that $Pr(Y \geq \sqrt[k]{2n}) \leq 2^{-1}$.

\subsubsection{2.} 
In Lemma 5.1 we showed that, under the standard balls-and-bins model, the maximum load when n balls are thrown independently and uniformly at random into n bins is at most 3 ln n/ln ln n with probability 1-1/n. Considering our result in the response to the previous subquestion, we see that the 1/2 bound for this maximum load can be proved whenever:
\begin{eqnarray}
\frac{3\ln n}{\ln \ln n} &\geq& \sqrt[k]{2n}\\
\ln (\frac{3\ln n}{\ln \ln n}) &\geq& k^{-1}\ln 2n\\
k &\geq& \frac{\ln 2n}{\ln (\frac{3\ln n}{\ln \ln n})}\\
\therefore k &=& \Omega(\frac{\ln n}{\ln \ln n})\\ 
\end{eqnarray} 

\section{Question}
6. Exercise 13.9, page 334. Exercise 13.9: Suppose we are given m vectors $v1, v2, ..., vm \in {0, 1}^{l}$ such that any k of the m vectors are linearly independent modulo 2. Let $v_{i}=(v_{i,1}, v_{i,2},..., v_{i,l})$. Let u be chosen uniformly at random from ${0, 1}^{l}$, and let $X_{i}=\sum_{j=1}^{l}v_{i,j}u_{j} \mod 2$. Show that the Xi are uniform, k-wise independent bits.

\subsection{Solution}
All addition or multiplication in this answer is conducted in modulo 2.

Note that as $X_{i}=\sum_{j=1}^{l}v_{i,j}u_{j} \mod 2$, $X_{i} = f(v_{i},u)$.

\begin{thm}
The Xi are uniform bits.
\end{thm}
\begin{proof}
Note that every vector vi contains atleast one entry with the value 1. (If this were not the case, 2-wise independence of the vi would not hold, as then: vi = 0(vj), for any other vector vj.)

We know that $X_{i} = f(v_{i},u)$. Suppose that vi contains m entries with values 1. Let $v_{k}$ be one of them. We use the principle of deferred decisions. For all $v_{i,j} = 0$, $v_{i,j}u_{j}=0$. Suppose that we already have the values of the entries in the vector u corresponding to the m-1 of the entries in $v_{i}$ ($i\neq k$) with the value 1. Then, irrespective of the value of $\sum v_{i,j}u_{j}$ for those m-1 values of j, the value of Xi depends entirely on the value of the $u_{k}$. So, $Pr(X_{i}=0) = Pr(u_{k}=0) = 1/2$.
\end{proof}

\begin{thm}
The Xi are k-wise independent bits.
\end{thm}
\begin{proof}
If we show that $Pr(X_{1}=x_{1}, \dots X_{k}=x_{k}) = 2^{-k}$, we will have shown that $X_{1}, ... X_{k}$ are k-wise independent.

$X_{1}, \dots X_{k}$ are generated by taking the inner product the vector u with v1,... vk. We know that v1,... vk are linearly independent. Construct a matrix, M whose rows are v1,... vk. Then, the rank of M is k. So, there must be k linearly independent columns in this matrix. Let set of the ordinals of these columns be called C.

$X_{i}=\sum_{j \in C}v_{i,j}u_{j} + \sum_{j \notin C}v_{i,j}u_{j}\mod 2$. Let $X'_{i}= \sum_{j \in C}v_{i,j}u_{j}$. Let $Y'_{i}= \sum_{j \notin C}v_{i,j}u_{j}$. Consider the equations for $X'_{1}, \dots X'_{k}$. These are k independent equations with k unknowns (u' = <$u_{j}$ for every j in C>). So, for each value of the vector $X'_{1}, \dots X'_{k}$, there is exactly one value for the subvector u'. In other words, there is a one to one mapping between X' and u'. Let t be the unique solution for u' for a given value of X'. As the bits $u_{j}$ are chosen unifromly at random, $Pr(X'_{1}=x'_{1}, \dots ,X'_{k}=x'_{k})= Pr(u'=t) = 2^{-k}$.

We had defined earlier that $X_{i}= X'{i} + Y'_{i}$. Now, we apply the principle of deferred decisions. Whatever the value of the vector $Y'$, the value of $X'$ determines the value of X. Hence, $Pr(X_{1}=x_{1}, \dots X_{k}=x_{k}) = Pr(X'_{1}=x'_{1}, \dots ,X'_{k}=x'_{k}) = 2^{-k}$.

Hence, $X_{1}, ... X_{k}$ are k-wise independent. Then, due to symmetry, we can claim the same about any arbitrarily chosen k values of Xi.
\end{proof}

\section{Question}
7. Exercise 13.11, page 334. Exercise 13.11: In a multi-set, each element can appear multiple times. Suppose that we have two multi-sets, S1 and S2, consisting of positive integers. We want to test if the two sets are the "same"-that is, if each item appears the same number of times in each set. One way of doing this is to sort both sets and then compare the sets in sorted order. This takes O(n log n) time if each multi-set contains n elements.
\subsubsection{1.} Consider the following algorithm. Hash each element of S1 into a hash table with cn counters; the counters are initially 0, and the ith counter is incremented each time the hash value of an element is i. Using another table of the same size and using the same hash function, do the same for S2. If the ith counter in the first table matches the ith counter in the second table for all i, report that the sets are the same, and otherwise report that the sets are different.

Analyze the running time and error probability of this algorithm, assuming that the hash function is chosen from a 2-universal family. Explain how this algorithm can be extended to a Monte Carlo algorithm, and analyze the trade-off between its running time and its error probability.

\subsubsection{2.} We can also design a Las Vegas algorithm for this problem. Now each entry in the hash table corresponds to a linked list of counters. Each entry holds a list of the number of occurrences of each element that hashes to that location; this list can be kept in sorted order. Again, we create a hash table for S1 and a hash table for S2, and we test after hashing if the resulting tables are equal.

Argue that this algorithm requires only linear expected time using only linear space.

\subsection{Solution}
\subsubsection{1.}

Let the hash function h be chosen from a 2-universal family. The given algorithm can fail only by claiming that S1 and S2 are equal when they are not. In other words, it can fail only due to false positives.

As there are only n elements in S1, the algorithm utilizes at most n counters in the hash table. So, the algorithm fails if at any one of these n locations, there is a collision (2 elements are mapped to that location). As h belongs to the 2-universal family, for every z=h(x) in this set of locations, $Pr(z = h(y)) = Pr(h(x) = h(y)) \leq (nc)^{-1}$. So, due to the union bound, the probability that the algorithm fails is 1/c.

The algorithm requires linear or sublinear time to select the hash function uniformly at random from the 2-universal family. The time required to hash S1 and S2 into the hash tables is O(n). The time required for comparison is O(n). Hence, the running time for the algorithm is O(n).

The algorithm can be extended into a Monte Carlo algorithm by repeating the above algorithm c' times. Then, for constant c', the running time remains O(n). The error probability, however, is now $(c)^{-c'}$.

\subsubsection{2.} 
Consider the case where each entry in the hash table corresponds to a linked list of counters. Each entry holds a list of the number of occurrences of each element that hashes to that location; this list can be kept in sorted order. Again, we create a hash table for S1 and a hash table for S2, and we test after hashing if the resulting tables are equal.

\begin{thm}
The Las Vegas algorithm requires linear expected running time.
\end{thm}
\begin{proof}
The time required to select a hash function from a family of 2-universal family is linear or sublinear.

As shown in Lemma 13.9 of \cite{mitzenmacherUpfal}, the expected number of elements at any location is at most n/(nc) = 1/c. In other words, the expected number of elements in a certain list in the hash table is constant. Hence, the expected time required to compare two lists is constant. Hence, the expected time required to compare lists at n hash locations is linear, due to linearity of expectations.

By the same reasoning, the expected time required to construct the hash tables of lists is linear.

Hence, the Las Vegas algorithm requires linear expected running time.

\end{proof}

\begin{thm}
The Las Vegas algorithm requires linear space.
\end{thm}
\begin{proof}
We already know that the size of the hash table is O(cn). The total size of all the lists in the hash table is also linear, as a link list can only be as large as the number of elements in it, which is O(n). According to the definition of our algorithm, these are the only non-constant sized data structures used. Hence, the las vegas algorithm requires only linear space.
\end{proof}
\bibliographystyle{plain}
\bibliography{../randomizedAlgorithms}

\end{document}
