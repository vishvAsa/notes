\documentclass[oneside, article]{memoir}

\input{../packages}
\input{../packagesMemoir}
\input{../macros}

%opening
\title{Randomized algorithms: Quick reference}
\author{vishvAs vAsuki}

\begin{document}
\maketitle

Based on \cite{mitzenmacherUpfal}. Also see the probabilistic analysis quick reference.

\tableofcontents

\chapter{Notation}
whp: With high probability. PPT: Probabilistic polynomial time. Monte carlo algorithm: uses repeated random sampling for computation.

\chapter{Generate random bits}
Random bit generator is usually a physical device. Usually, $Pr(X = 1) = p$; from this, easily get random generator with $Pr(X = 1) = 1/2$: flip every alternate bit. Flippling every t-th bit, get Pr(X = 1) = 1/t.

\section{2-wise independent bits generation}
$2^{b}-1$ 2-ind bits $Y_{i}$ from b ind bits $X_{i}$: For each subset, $Y_{i}=\oplus X_{i}$. In GF(p), p 2-ind elements from 2 ind elements: $Y_{i}=(X_{1}+iX_{2}) \mod p$ for every i in GF(p). 2-universal ($Pr(h(x) = h(x')) \leq n^{-1}$) hash function family: H has $h:U \to V; |V| = n; a, b \in GF(p), a \neq 0$; $h_{a,b}(x) = ((b+xa) \mod p) \mod n$. If a can be 0, strongly 2-universal ($Pr(h(x) = y, h(x')=y') = n^{-2}$).

\section{Pseudorandom generator G}
Based on operational meaning of randomness: Better than early definitions, which were based on statistical properties of bit string.

$G:\set{0,1}^{l} \to \set{0,1}^{n}$, computable in time $2^{O(l)}$, is an $(\eps, s(n))$ pseudorandom generator if $\forall$ ckts c of size s(n) [strength parameter], Indistinguishability / unpredictability property holds: $Pr_{y \in \set{0,1}^{n}}[c(y)=1] - Pr_{x \in \set{0,1}^{l}}[c(G(x))=1] \leq \eps$ [error parameter].

Distinguishers c usually try to learn from string and guess if G generated it. For example of distinguisher, see colt ref.

Notion similar to PFF: If ye pick y, you've picked random functions f; if ye pick x, you've picked a subset of pseudorandom functions.

\subsection{Hard function f}
f is (a(n), s(n)) hard if $\forall$ ckts of size $\leq s(n)$, \\
$Pr_{y \in \set{0,1}^{n}}[c(y)=f(y)] \leq 2^{-1} + a(n)$. (a(n), s(n), D) hardness: a generalized notion: take Pr wrt D.

Worst case hardness: $a(n) = 2^{-1} + 2^{-n}$: otherwise, c can memorize +ve inputs.

Any pseudorandom generator is also hard. \why

Any hard function is also a pseudorandom generator. \why

\subsection{Hard core function f}
f is (a(n), s(n), M) hard core wrt \\
measure M (defining distribution $D_{M}$) if f is $(a(n), s(n), D_{M})$ hard.

If f is hard wrt distribution D which is uniform over set S, it is (a(n), s(n), S) hard core. Then S is a (a(n), s(n), f) hard core set. If (a(n), s(n), M) hard, then (a(n), s(n), S) hard for S of a certain size.

(Impagliazzo): If f is (a(n), s(n), U) hard, it is $(b(n), b(n)^{2}a(n)^{2}, M)$ hard core where $D_{M}$ is close to U: if not hard core, could smoothly boost the good guessing ckt to get a ckt which violates hardness.

\subsection{BBS pseudorandom generator}
(Blum, Blum, Shub). Input: N=pq: p, q are primes $= 2 \mod 4$; Initial seed $s_{0}$ of n bits. Output: A stream of poly(n) bits which look random. $s_{i} = s_{i-1}^{2} \mod N  = s_{0}^{2^{i}} \mod N$; ith bit output: $b_{i} \dfn LSB(s_{i})$.

If factoring is hard, you cannot distinguish between a truly random m bit string and an m bit string obtained by choosing $s_{0}$ at random and running BBS gen. \why

\section{Pseudo random function family (PFF)}
\subsection{Function family F}
Set of polynomial sized boolean circuits with input length n, of size $O(e^{n})$ with samplable index set I; $\exists$ alg A to input $i \in I$, return $f_{i} \in F$, simulate its input/ output behavior by accepting x and returning $f_{i}(x)$ in poly time.

\subsection{Distinguisher D}
Any Poly time alg; inputs function f: black box access to it; outputs 1 if it thinks f is random.

\subsubsection{Indistinguishability}
Let f be truly random function. F is PFF if for every D, $Pr_{f \in rand}(D^{f}=1)-Pr_{i \in_{U} I}(D^{f_{i}}=1) < O(e^{-n})$.

\subsection{Existance}
Goldwasser, Goldreich, Micali (GGM): If one way functions exist (if factoring is hard), then PFF's exist.

\tbc


\chapter{Sampling from distribution}
\section{Sampling a random variable X}
\subsection{Problem}
The pseudo random number generator yields a sequence of almost independent random bits: see randomized alg ref. How do you use them to sample from a given distribution?

\subsubsection{Visualization}
Want to 'cover' the entire range(X) by the sampling: visualize as throwing darts in a oval: dart density corresponds to probability; the shape formed by the darts corresponds to the Pr(X) contour.

\subsubsection{Challenges}
Can't sample from $\Re$: computer can't even store all possible $\Re$. So, must sample from Q.

Or, Pr(X=x) or $Pr(X=x|Y=y)$ may be difficult to calculate.

\subsection{Sampling some distributions}
\subsubsection{Uniform distribution}
Sampling from U[a,b] is easy: sample 1 bit at a time form the range.

\subsubsection{Discrete probability distribution}
Take a line segment [0, 1], partition it into ranges $\set{R_i}$ corresponding to probabilities of various values; Sample x from U[0, 1]; identify its range j; output value j corresponding to $R_j$.

\subsubsection{Normal distribution}
Use rejection sampling: accept x drawn from uniform distribution with probability proportional to $e^{(x-\mean)^{T}\covmatrix^{-1}(x-\mean)}$. Can similarly sample from any distribution.

Or, sample from discrete probability distribution which approximates N: but this is costly as the number of rational numbers in the range is large.

\paragraph*{Multidimensional Normal distribution}
Sampling from a spherical normal distribution is easy: just apply a linear transformation to the inputs before sampling.

\section{Problems solved by sampling X}
\subsection{The integration/ summation problem}
Maybe you want to $E[f(X)|T]$ where T is an event (maybe 'true'), but ye find it difficult to do this analytically. The expectation could involve a combinatorial sum: the RV f(X) can take on many different values with different probabilities.

\subsubsection{Find normalization const Z of Pr(X)}
Given prob distribution $Pr(x) = Z^{-1}f(x)$ specified only by f(x) to find Z.

\subsubsection{Finding Pr(T) for hard to analyze T}
Same as finding the expectation of an indicator random variable.

Visualize sample space as an oval, and sampling as putting dots in the oval; you estimate the size of a smaller oval where the event is true.

Aka counting if every sample point is equally probable.

\subsection{Optimization}
Sampling with special attention being paid towards finding an optimum is considered elsewhere.

Maybe you want to find $\min_{X} f(X)$. As seen in optimization ref, in general it is hard to find a global minimum; so repeated sampling / random restarts with gradient descent can be a useful strategy.

\subsubsection{Optimization after summation}
Maybe want $\min_{X} E[f(X)|T]$.

\subsection{Simulation}
Maybe you want to simulate roll of dice, or want AI's which behave realistically in games.

\section{Sampling Random vectors}
\subsection{Arbitrary randomg vectors}
Suppose random vector $X$ is 3-dimensional, and suppose that we are given the joint potential $\gf_X$. One can first deduce and sample $x_1$ from $f_{X_1}$, then $x_2$ from $f_{X_2|X_1 = x_1}$, and finally $x_3$ from $f_{X_3|X_1 = x_1, X_2 = x_2}$. The cost mostly comes from having to sum over $|ran(X_2)||ran(X_3)|$ terms in deducing the distributions.

\subsection{From graphical models}
\subsubsection{Forward sampling for DAG's}
Consider a directed graphical model with fully specified conditional probability distributions for non-root nodes and distributions for root nodes.

One simply samples successive each level of the DAG, starting at level 0, utilizing the conditional probability distributions given samples drawn from the previous level at each step.

This is an exact sampling technique.

\subsubsection{Undirected trees}
Sampling from undirected tree-structured graphical models can be accomplished by converting them efficiently to tree structured directed graphical models. Computing marginal probabilities by summing over pairwise potentials  is efficient when $range(X_i)$ is finite, which then makes computation of conditional probabilities for the DAG efficient.

\subsubsection{Factor trees}
One can forward-sample tree structured factor graphs by conversion to equivalent directed trees whose nodes correspond to factor-nodes.

\subsection{Arbitrary undirected graphs}
Arbitrary undirected graphs may be sampled by conversion to corresponding factor trees.

\section{Repeated Sampling}
Aka Monte Carlo sampling. Repeated sampling is done for several purposes, like inference. It is usually much simpler when Pr(X) is modelled by a graphical model.

Here we consider various techniques for repeated sampling themselves, rather than their special applications to problems such as inference (as in rejection sampling).

\subsection{Markov chain Stationary distribution}
Aka Markov Chain Monte Carlo (MCMC).

Consider a stochastic process whose state space is $ran(X)$. For any distribution $f_X$, we can usually construct markov chain whose stationary distribution is $f_X$. So, doing appropriate random walks on the state transition graph of this Markov chain (see probabilistic models for details), one can sample various states from $f_X$.

But before each sample is picked, one must 'throw away some samples' in order to let the state distribution approximate the stationary distribution.

The art is in finding a fast mixing one. If it is time reversible, can get very fast mixing Markov chain.

\subsubsection{Sampling from discrete distribution}
Aka Metropolis alg.

Want to sample from distribution over \\
$range(X) = \set{v_{i}}$. Make ergodic Markov chain with right stationary distribution: Pick $M \geq maxDegree$, set $Pr(v_{i} \to v_{j}) = (1/M)min(1, \pi_{v_{j}}/\pi_{v_{i}})$ otherwise self-loop. Note that finding $\pi_{v_{j}}/\pi_{v_{i}}$ usually easy : ye don't need to know normalization constant Z.

\subsection{Repeated conditional sampling}
Aka Gibbs sampling.

\subsubsection{Algorithm}
Start with random assignment to $(X_i)$. For each i, sample $X_i \distr Pr(X_i|X_{j\neq i})$; repeat.

The conditional probability distribution is computed using \\$f_{X_i|X_{-i}} = \gf_{X}/\gf_{X_{-i}} = \gf_{X}/\sum_{x_i} \gf_{X}$, where the joint potential $\gf_X$ is not necessarily normalized.

\subsubsection{Correctness}
Why is this same as sampling from Pr(X)? \why

\subsubsection{Efficiency}
Gibbs sampling is efficient as long as the $\gf_X$ can be efficiently computed. This is because of the fact that the normalization constant cancels out, and need not be computed; and becuase $range(X_i)$ is limited.

\subsubsection{Modifications}
This is akin to a 'random walk on a graph' behavior, so often takes long to cover the entire sample space well. Consider $(X_1, X_2)$ with them being very correlated: visualize sample space as elongated ellipse around some line; so going L units away on $X_i$ requires $O(L^{2})$.

So, maybe use random restarts.

\subsection{Block conditional sampling}
Aka block-Gibbs sampling.

\subsubsection{Algorithm}
Here, we sample whole blocks of variables at a time. Consider a partition of variables $X$ into $A, B$. Starting with a random assignment, we sample from $f_{A|B}$ and $f_{B|A}$ in turn. $f_{A|B} = \gf_X/\gf_B$, where $\gf$ is the potential function which is not necessarily normalized.

Finding $\gf_B(b) = \sum_a \gf_X(a, b)$ requires summing over $\prod_i ran(B_i)$ items in general, which is not efficient.


\subsubsection{Advantages}


It is better than Gibbs sampling when the subgraphs can be more efficiently or accurately (perhaps even exactly) sampled from, and when summing over them is easy. This is true for example when the partitions correspond to tree-structured subgraphs of directed graphical models.

\section{Physics-inspired sampling}
\subsection{Energy temperature analogy}
Can write $Pr(x) = Z^{-1}e^{-\frac{E(x)}{T}}$, where E is energy function, T is the temperature. \tbc

\chapter{Inference}
Here we only consider inference by sampling. Inference techniques particular to various model classes are considered elsewhere.

\section{Inference problems}
Find Pr(X=x) or $Pr(X=x|Y=y)$. Inference algorithms may be exact or approximate.

Or find E[X] or $E[X|Y=y]$.

Or find the mode of $Pr(X=x|Y=y)$: as done in case of Maximum likelihood estimation.

\subsection{Difficulty in calculating Pr(X=x)}
Pr(X=x) can be difficult to calculate analytically. One could specify $Pr(X=x) = Z^{-1}f(x)$ by leaving the normalization constant Z unspecified, and describing only $f(x)$.

Similarly, $Pr(X=x|Y=y)$ can involve solving the 'summation problem'.

Can be difficult due to not knowing the normalization constant.

Often probability of an event T cannot be estimated analytically, even when probability of an atomic event is known. Eg: Consider uniform density over a bounded space in $R^{d}$, consider an irregular shape in it representing event T; now find Pr(T). Ye can throw darts/ sample to find the area (Monte carlo!). Or construct a grid to calculate the volume of the irregular shape: but here the computation will be $N^{d}$, where N is number of grid points in each dimension.

\section{Inference by sampling}
\subsection{Rejection sampling}
To find $E[f(X)|E_1]$, Sample from distribution of X, reject samples for which $\lnot E_1$, then use remaining points to find avg f(X).

\subsection{Set counting techniques}
Design sample space, sample m times to estimate event probability.

Maybe iteratively constrict sample space and multiply probabilities, derive number of events: Thus, one finds \\$Pr(E_{1}), Pr(E_{2}| E_{1}) ..$ finally finding $Pr(Y|E_{1} .. )$; thence finding Pr(Y).

\section{Inference quality}
\subsection{FPRAS for (eps, d) approx.}
RAS: Randomized approximation scheme. Suppose $Pr(|m^{-1}\sum^{m}_{i=1} (X-\mu)|\geq \epsilon \mu) \leq d$. If $m$ is FP (Fully Polynomial) in $\ln(\frac{2}{d}), \epsilon^{-2}, |x|$, we have a FPRAS. From Chernoff bound : $m \geq \frac{3\ln(\frac{2}{d})}{\epsilon^{2}\mu}$.

\subsection{Fully poly almost uniform samplers (FPAUS)}
\tbc

\chapter{Ways of making randomized algorithms}
\section{Algs for problems in the class RP}
One sided error; success probability $p = Pr(f(x) = 1|x\in L) \geq 2^{-1}$.

\subsection{Amplification of confidence of RP alg}

\subsubsection{'Monte Carlo' search alg}
Sample a solution, check correctness, repeat t times, lower bound p. If trials n-wise independent: tn random bits; failure probability: $(1-p)^{t} \approx e^{-tp}\leq 2^{-tp} \to 0$ for large t. So, even if $p = (poly(n))^{-1}$; for t = poly'(n), success whp.

If trials 2-wise independent: 2n random bits: $t^{-1}$ using Chebyshev ineq. \chk

\subsubsection{Negligible success probability }
Anything asymptotically \\
smaller than an inverse polynomial. Eg: $O(n^{-\ln n}), e^{-n}$. Else, could boost success rate.

\subsection{'Las Vegas' search alg}
Repeat MC alg till confirmed success.

\subsection{Disperser}
Uneven bipartite graph ($V=L \union R, E$); Min degree of v in L: D; N(v): nighbors of v; $\forall S \subseteq L, |S|=k$, expand: connect to $\geq \frac{|R|}{2}$ verteces. How small be D? Solving Pr( G not disperser) $< 1$: $D \geq \log |L| + 1$.

\subsubsection{RP amplification} Make disperser with D=t; pick v in L; use N(v) as random bits. \tbc

\section{BPP alg}
2-sided error. Repeat many times, take majority, use Chernoff.

\section{Random projections}
Solve problem in $\mathbb{R}^{D}$ by projecting points in V to random $\mathbb{R}^{d}$, $d<D$; distance preserved approx whp! Eg: Approx nearest neighbor, clustering; proj points in $\mathbb{R}^{2}$ to most lines. Take $x = (a-b) \in \mathbb{R}^{D}$, change bases to get $y =(a-b) \in \mathbb{R}^{D}$, project to first d directions: drop D-d coords, get $z \in \mathbb{R}^{d}$, renormalize (mult by $(\frac{D}{d})^{0.5}$) as projection reduces length.

(Johnson, Lindenstrauss): Let $|V|=n$; $\epsilon \in (0,0.5), d = \frac{18}{\epsilon^{2}}\ln n: \exists \texttt{ whp } f:\mathbb{R}^{D} \to \mathbb{R}^{d}\ \forall (a,b) \in V, \frac{\norm{f(a)-f(b)}-\norm{a-b}}{\norm{a-b}} \leq \epsilon$ : Proj to rand plane $\equiv$ proj rand unit vector y to get z. As $Pr(\norm{z}^{2} \leq k(\frac{d}{D})) \leq exp(\frac{d}{2} (1-k+\log k))$: Chernoff style proof; show f(a)-f(b) close to a-b; then use union bound to cover all pairs in V.

Tight up to a factor $\log(1/\eps)$: $\exists$ points V: $\Omega(\frac{\log n}{\eps^{2} \log (\frac{1}{\eps})})$ to preserve distances.

\section{Fingerprinting}

BigObj1 = BigObj2 $\Leftrightarrow$ SmallObj1 = SmallObj2.

\subsection{Matrix multiplication check}
Problem: AB ?= C. Ordinary algorithm: Finding AB costs $O(n^{2.4})$.

Randomized algorithm: Sample r from $\set{0, 1}^{n}$, check if $ABr = Cr$: $O(n^{2})$.

Analysis: Let D = AB-C; $AB \neq C \implies [Pr(Dr = 0) \leq 2^{-1}]$ using principle of deferred decisions. Also from Schwartz Zippel Thm (see Algebra ref).

\subsection{Codes and communication complexity}
See Information and coding theory ref.

\subsection{String search}
Find pattern vector $y \in \set{0, 1}^{m}$ in text vector $x \in \set{0, 1}^{n}$; x(j) is $x_{j} .. x_{j+m-1}$.

Trivial alg: O(nm). Optimal det alg: (Boyer Moore) O(n+m) flops. Rand alg: compare fingerprints. Take rand prime p; use $F_{p}(x) = x mod p$. Find $F_{p}(x(j+1))$ from $F_{p}(x(j))$ in O(1) flops: precompute $F_{p}(2^{m-1})$; $x(j+1) = 2(x(j) - x_{j}2^{m-1}) + x_{j+m}$; So O(n+m) flops. From $\union$ bound, prime num theorem, Pr(false match) $\leq (n-m+1)\frac{m}{\Pi(mn^{3})} = O(\frac{\log n}{n^{2}})$. \tbc



\chapter{Derandomization, explicit construction}
\section{Derandomization}
Design algorithm which uses random choices; Replace random choice with choice which maximizes conditional expectation; Eg: find $\geq \frac{|E|}{2}$ cut in G=(V, E). Devise rand alg which uses n 2-wise ind bits; enumerate all values for $\log n$ truly ind bits, make n 2-wise ind bits and run rand alg.

\section{Explicit construction algorithms}
Sample values for some variables, show that desired object still exists, use exhaustive search for other variables.

\section{Sampling uniformly from unit sphere}
Pick each point according to $N(0, n^{-1})$. \why

\chapter{Approximation algorithms}
\section{LP based Approximation algorithms}
Rephrase (maybe NP hard) problem as Integer Programming problem; Make LP relaxation; solve in polytime; translate solution by rounding; make $(\del, \eps)$ approximation guarantees. Rounding choices: To closest integer, or randomized rounding.

\subsection{Vertex cover problem}
G=(V,E). IP: Vars $v_{i}$ = 0 or 1, $\forall (i,j) \in E, v_{i}+v_{j} \geq 1$, $\min \sum v_{i} = ?$; LP relaxation: $\hat{v}_{i} \in [0,1]$; solution $\sum \hat{v}_{i} \leq opt \leq \sum v_{i}$; round to nearest int to get approx soln $\set{v_{i}}$; as $v_{i} \leq 2 \hat{v}_{i}$: $\sum v_{i} \leq 2 opt$.

Max SAT.

\tbc

\section{Semidefinite programming based approximation algs}

\tbc

\chapter{Applications}
For number theory applications, see number theory ref.

\section{Perfect matchings}
\subsection{Bipartite graph}
G = (U, V, E); $|U| = |V| = n$. Matching: $\set{e} \in E$ not sharing endpoints. Perfect matching (pm) has size n. Naive alg takes $O((n!)n)$ time.

Make symbolic matrix with $A_{i,j} = x_{i,j}$ if $(u_{i}, v_{j})\in E$, else 0. Let Q(x) = det(A) : $n^{2}$-nomial, 'symbolic determinant'. G has pm iff $\exists r: Q(r) \neq 0$. $deg(Q) \leq n$. Take prime $p > 2n$, pick r from $Z_{p}$; by Schwartz Zippel Thm, $Pr(Q(r) = 0 | \exists r'|Q(r') \neq 0) < 2^{-1}$.

If pm unique, find pm in polytime: Repeat: $E = E - \set{e}$; see if G still has pm. (Parallelizable.)

\subsubsection{If pm not unique}
Take uniformly random weight fn $w:[1,m] \to [1,2m]$; let $W(S) = \sum_{d \in S} w(d)$.

\paragraph*{Isolation lemma}
Let $S_{i} \in [1,m]$; take k $S_{i}$. $Pr(\exists minWt\ S_{i}, S_{j} | e \in S_{i} , e \notin S_{j}) \leq (2m)^{-1}$: by principle of deferred decisions: Pick W(e) last to $W(S_{j}) - W(S_{i} - \set{e})$. So, $\union$ bound: Pr($\exists$ unique $S_{i}$ with $W(S_{i})$ min) $\leq 2^{-1}$.

Get w(e) $\forall e = (u_{i}, v_{j}) \in E$. Get matrix $B_{i,j} = 2^{w((u_{i}, v_{j}))}$. Then $det(B) = \sum_{pm\ M} (\pm 2^{W(M)})$. So find pm by finding highest power of 2 dividign det(B).

\subsection{General graph}
(Tutte): G = (V, E); Make Tutte matrix: $\forall i<j$ if $(u_{i}, v_{j})\in E$, $A_{i,j} = x_{i,j}$ and $A_{j,i} = -x_{i,j}$, else 0. Let $|A| = Q(x)$ multinomial; G has pm iff $\exists x Q(x) \neq 0$: $det(A) = \sum_{\pi} sgn(\pi) \prod A_{i, \pi(i)}$; if some $\forall \pi: \prod A_{i, \pi(i)} = 0 $, no pm. Else, Take prime $p > 2n$, pick r from $Z_{p}$, take Q over $Z_{p}$; by Schwartz Zippel, $\exists x$ where $Q(x) \neq 0$.

\chapter{Analysis of randomized algorithms}
See probabilistic analysis ref.

\section{General Tricks}
Running time of \textbf{MDP-algorithm}: Make Markov chain, make RV $Z_{i}$ steps for going to absorbing state from i, get and solve recurrance eqns for $E[Z_{i}]$.

\section{Results}
2-SAT rand alg needs $O(n^{2})$ time. 3-SAT rand alg (without restart) needs $O(n^{3/2}(\frac{4}{3})^{n})$ time as Pr(moving forward in Markov chain) $< 1/2$. Max load Y when hash function from k-universal family used: $Pr(Y > \sqrt[k]{2n})<2^{-1}$ (bounding expected number of collisions, use Markov).



\bibliographystyle{plain}
\bibliography{randomizedAlgorithms}

\end{document}
