\documentclass[oneside, article]{memoir}

\input{../packages}
\input{../packagesMemoir}
\input{../macros}

%opening
\title{Optimization: Quick reference}
\author{vishvAs vAsuki}

\begin{document}
\maketitle
\tableofcontents

\part{Themes}
Aka Mathematical programming.

Notation: Vector space V, field F.

\chapter{Solver design}
Make fast solvers for specific classes of problems.

Make specification frameworks (see later section).

\chapter{Applications of Constrained optimization}
Convex optimization problems found widely in nature.

Untrained intuition is not very good at recognizing and formulating proper optimization problems: easy problems appear hard.

\part{Problem structure}

\chapter{The problem}
$\min f_{0}(x): V \to F$ subject to constraints $f_{i}(x) \leq b_{i}$. Vector $x$ is optimization variable of d dimensions. $f_0$ is optimization fn. $b_i$ are limits/ bounds for constraints.

\section{Standard (primal) form}
$\min_x f_{0}(x): \set{f_{i}(x) \leq 0}, \set{h_{i}(x) = 0} $: all are $V \to F$ functionals.

\subsection{Explicit constraints}
$f_i$ and $h_i$ are explicit constraints. Problem is unconstrained if it has no explicit constraints.

\subsection{Implicit constraints}
$x \in D  = \inters dom(f_i) \inters dom(h_i)$.

\subsection{Optimal value}
$p^{*} = \inf \set{f_{0}(x): f_{i}(x) \leq 0, h_j(x) \leq 0 \forall i, j}$. $p^{*} = \infty$ if problem is infeasible, or no $x$ satisfies constraints. $p^{*} = - \infty$ if it is unbounded below.

\section{Equivalent formulations}
\subsection{Epigraph formulation}
$\min_{x,t} t: f_{0}(x) \leq t, \set{f_{i}(x) \leq 0}, \set{h_{i}(x) = 0} $: all are $V \to F$ functionals.

\subsection{Linear equiality constraints}
Got $\min_x f_{0}(x): \set{f_{i}(x) \leq 0}, Ax-b = 0$. Take H which spans N(A), particular solution to Ax -b =0, $x_0$. Then, can rewrite as: $\min_v f_{0}(Hv + x_0): \set{f_{i}(Hv + x_0) \leq 0}$.


\subsection{Augmented/ Slack form}
Replace inequality constraints $\set{f_{i}(x) \leq 0}$ with $f_i(x) + s_i \leq 0; s_i \geq 0$.

\section{Conic (in)equalities constraints}
$\min_x f_{0}(x): \set{f_{i}(x) \preceq_{K_i} 0}, \set{h_{i}(x) = 0} $. Constraints are specified using conic (in)equalities.

\section{Special cases}
\subsection{Feasibility problem}
$\min 5.5$: $h(x) = 0, f(x) \leq 0$.

\section{Perturbed problem}
Replace constraints $f(x) \leq 0, h(x) = 0$ with $f(x) \leq u, h(x) = v$ to get problems parametrized by u, v; Denote optimum by $p^{*}(u, v)$. Now can study sensitivity of problem to perturbations.

Large $u_i$ implies loosening constraint $f_i$; similarly constraints can be tightened. How will the optimum change in response?

\chapter{The solution}
\section{Feasible region}
Feasible region D satisfies all constraints.

\subsection{Active, inactive constraints at x}
Active constraints: $\set{h_i(x) = 0}$, and $\set{f_i: f_i(x) = 0}$ at $x$. Inactive constraints: $f_i(x) < 0$ at $x$.

\subsubsection{Strict feasibility}
x is strictly feasible if $x \in int(D)$: ie all ineqalities $f_i(x) < 0$ hold strictly.

\subsection{Active constraints and their gradient}
\subsubsection{Constraint surface, gradient}
The level set $h_i(x) = 0$ is a d-1 dimensional (possibly closed) surface. For any $x$ in this constraint surface, $\gradient h_i(x) \perp$ the tangent to $\set{x: h_i(x) = 0}$.

\subsubsection{Direction of the gradient}
Consider level-set $f_i(x) = 0$. $\gradient f_i(x)$ will be oriented towards increasing $f_i(x)$, that is, away from the interior of $\set{x: f_i(x) \leq 0}$. See derivatives of functionals section in vector spaces survey.

\subsubsection{Intersection of constraint surfaces}
So, for any \\
$x:h_i(x) = 0 \forall i$, $\sum l_i \gradient h_i(x) \perp$ the tangent to $\set{x: h_i(x) = 0 \forall i}$.

Want $\min f_0(x)$ in this contour (constraint surface); or find $\inters$ of constraint surface with the smallest contour of $f_0$.

\section{Optimality criteria}
\subsection{Optimal x}
$x^{*}$ is optimal if $f_0(x^{*}) = p^{*}$. Similarly, locally optimal $x$ is also defined: optimality when $x$ constrained to some small ball in feasible set.

\subsection{Connection with gradient}
x optimal iff it is feasible and $\gradient f_0(x)^{T}(y-x) \geq 0 \forall y$ feasible.

\subsection{Unconstrained fn}
\subsubsection{Differentiable f}
If f differentiable, $x$ is a local minimum only if, $\forall y \in N_\eps(x): \gradient_{(y-x)}f(x) = \dprod{(y-x), \gradient f(x)} \geq 0$. Similar case for local maxima.

So, $x$ is a local extreme value only if $\gradient f(x) = 0$: else, could move a little bit and decrease/ increase f(x). $x: \gradient f(x) = 0$ is a critical point.

\paragraph*{Solving for x}
Conditions like $\gradient f(x) + l\frac{b+x}{\norm{b+x}} = 0$ need some clever algebra. Example, try the change of variables $e = b+x$ to simplify the above.

\subsubsection{Local extreme point}
Using 2nd order expression for $f(x + u) = f(x) + u^{T}D^{2} f(x+su)u$; so if $D^{2} f(x) \succeq 0$, $x$ is minimum; if $D^{2} f(x) \preceq 0$, $x$ is a maximum; else $x$ is a saddle point.

\subsubsection{Extension to convex non-differentiable f}
Take subdifferential $\subdifferential f$; $x$ is an extreme point only if $0 \in \subdifferential f$, as it implies that $f(x+d) \geq f(x)$.

\subsection{Gradient of objective, the active constraint surface}
Suppose that optimal $x$ occurs on the active constraint surface. What is the relationship of $\gradient f_0(x)$ with $\gradient h_i, \set{\gradient f_j}$, where the latter is the set of active constraints?

$\gradient f_0(x)$ $\perp$ to constraint surface at optimal $x$: else could move short distance along contour and decrease $f$. In other words, if you take any curve $g:R \to R^{n}$ in the constraint surface, passing through $x$, $f_0(g(t))$ would attain a minimum when $g(t) = x$; so can apply $\gradient f_0(g(t)) = \gradient f_0(x) Dg(t)= 0$ for any such curve $g$; so $\gradient f_0(x) \perp$ tangent to the active constraint surface at $x$.

So, want to achieve $\gradient f_0(x) + \sum_i m_i \gradient h_i(x) + \sum_j l_j \gradient f_j(x) = 0$.

\paragraph{Extension to  non-differentiable f}
The above reasoning and optimality condition can be generalized to non differentiable $f_i(x)$: we just need to interpret $\gradient f_i \in \subdifferential f_i(x)$. If this condition were violated, we could still move slightly along $\gradient f_0(x)$ and reduce the objective, while staying in the feasible region.

\subsubsection{Direction in case of active ineq constraints}
If $x$ is optimal, as $f_0(x)$ is being minimized, $\gradient f_0(x)$ - the direction of increasing $f_0$- points towards the interior of the constraint surface; unlike $\gradient f_i(x)$.

So, $l_j \geq 0$ for active inequality constraints. $\gradient f_0$ is not constrained in this manner by inactive inequality constraints: So, for all inactive $f_i(x)$: can use corresponding $l_i = 0$.

As earlier, this condition and the reasoning behind it can be generalized to non-differentiable convex $f_i$.

\subsubsection{General condition}
$\gradient f_0(x) + \sum_i m_i \gradient h_i(x) + \sum_i l_i \gradient f_i(x) = 0$, with $l_i \geq 0$, $h(x) = 0, f(x) \leq 0$.

Equivalently, $D f_0(x) + \sum_i m_i Dh_i(x) + \sum_i l_i Df_i(x) = 0$. \exclaim{This can be used when $f$, h are matrix functionals, as in SDP's!}

\paragraph{Extension to convex non-differentiable f}
Take subdifferential $\subdifferential f_i$; $x$ is an extreme point in the feasible region only if the general condition holds, using the notation $\gradient f_i(x) \in \subdifferential f_i(x)$. Reasons for this were explained elsewhere.

\subsubsection{Optimality in special cases}
If problem is constrained only by linear equalities $Ax = b$, you get the condition: $\gradient f_0(x) + A^{T}m = 0$.

\subsection{Sufficiency for local optima}
Consider $x'$, a local optimum. $x'$ is a local optimum iff the optimality condition holds. So, can enumerate feasible $x'$ which satisfy the optimality conditions, and pick the lowest to be the global optimum!


\subsection{Conic inequality constrained problems}
Some constraint qualifications, like Slater's conditions for convex programs, guarantee strong duality. So, $\exists l^{*} \succeq_{K^{*}} 0, m^{*}: g(l^{*}, m^{*}) = \inf_x L(x, l^{*}, m^{*}) = f_0(x^{*})$. If L is differentiable, this yields the optimality criterion: $\gradient_x L(x, l, m) = 0, h(x) = 0, f(x) \prec 0$.

\section{General strategies}
For solution strategies for particular cases, like linear, quadratic, convex, non-convex programming: see elsewhere.

\subsection{Hardness of finding global optimum}
In general, there are many local minima. Even when you have an expression for f(x), listing its minima is often not possible with analytic methods. Eg: f'(x) may be a quintic polynomial, for which there is no closed form expression for the roots.

In general, cannot find a global optimum in reasonable time: So, one must be satisfied with a local optimum or search for a long time.

Only in certain special cases, you can get arbitrarily close to the global optimum in reasonable time.

\subsection{Bisection method: using feasability solvers}
Take problem in epigraph form. $\min_{x,t} t: f_{0}(x) \leq t, \set{f_{i}(x) \leq 0}, \set{h_{i}(x) = 0} $. For any fixed t, this becomes a feasability problem. Can use binary search to find the lowest value of $t$ for which the feasability problem is satisfied.

Can do multi-parameter bisection similarly, when many parameters specify the objective.

\chapter{Constraints in the objective}
\section{Constrained and unconstrained formulations: equivalence}
\subsection{Lagrangian functional}
The Lagrangian functional \dfn $L(x, l, m) = f_{0}(x) + \sum m_{i}h_{i}(x) + \sum l_{j}f_{j}(x)$.

\subsubsection{Definition motivations}
you can rewrite the constrained optimization problem in the standard form as an unconstrained optimization problem. Also, you can define the dual problem.

\subsection{Unconstrained program from constrained problem}
As seen while considering the geometry of the problem, optimality criterion: $\gradient f_0 + \sum l_i' \gradient f_i + \sum m_i' \gradient h_i = 0$ for certain $l'\geq 0, m'$, where $\gradient f_i$ are represent subgradients in the case of convex non-differentiable $f_i$.

So, take $L(x, l', m') = f_{0}(x) + \sum m_{i}'h_{i}(x) + \sum l_{j}'f_{j}(x)$ for this $l', m'$; see that the optimality criterion is satisfied when $\gradient_x L(x, l', m') = 0$.

So, there exists an equivalent unconstrained problem $\min_x L(x, l', m')$ for every constrained optimization problem. 

This analysis holds for conic (in)equality constrained problems too.

\subsubsection{Objective - constraint tradeoff}
$L(x, l, m)$ looks like the scalarized form of a vector optimization problem, with $l$ and $m$ representing the tradeoff between the various parts of the objective.

\subsection{Lagrangian multipliers and tightness of constraints}
$l, m$ are called lagrangian multipliers. 

Suppose that $f_i(x) \leq 0$ is changed to $f_i(x) \leq 5$: $f_i$ now allows a greater degree of freedom, so minimizing $f_i$ is now a less critical part of the objective. Similarly, Consider any inactive constraint $f_i$: the corresponding multiplier will be 0.

Also explains why $l_i \geq 0$ should hold for the problem $\min_x L(x, l, m)$ to make sense: higher values of $f_i$ should be penalized.

\subsection{Constrained program from unconstrained problem}
Consider the problem $\min_x L(x, l, m)$ for $l\geq 0$. Considering the continuous relationship $c()$ between $l_i$ and the tightness of constraints $f_i(x) \leq c(l_i)$, we can conclude that there is an constrained problem equivalent for every unconstrained problem.

\chapter{Dual problem}
\section{Dual functional}
\subsection{Definition}
Take $g(m, l) = \inf_{x} L(x, m, l)$. For convex L(), do this by setting $\gradient_x L(x, m, l) = 0$.

$g$ is always concave: inf of linear functions in $m$ and l.

\subsubsection{Uniqueness}
Observe that optimization problems differing only in RHS of equality and inequality constraints have distinct lagrangian functionals, and thence distinct dual functionals.

\subsection{Connection with conjugate of the objective}
\subsubsection{Linear constraints case}
Consider $\min f_0(x): Ax \leq b, Cx = d$. $g(m, l) = -f_0^{*}(-A^{T}l - C^{T}m) - b^{T}l - d^{T}m$.

This holds in many other cases too. So, simplifies derivation of dual if conjugate is known. Also, simplifies derivation of the conjugate if the dual is known.

\subsection{As intercepts of supporting hyperplane to an image of the domain}
Consider $G = \set{(f(x), f_0(x)) \forall x \in dom (f_0) }$. Then, $g(l, m) = \inf_{(u, t) \in G} (t + l^{T}u) = \inf_x L(x, l, m)$. $t + l^{T}u = k$, for fixed k is a hyperplane. So, for fixed l, m: the hyperplane $l^{T}u + t = g(m,l)$ is the supporting hyperplane for G; and g(m, l) is the intercept of this hyperplane on the $f_0$ axis. \exclaim{Interpretation fails if hyperplane is vertical!} This is used in proof of slater's condition.

As we are interested in $f(x) < 0$, we consider only supporting hyperplanes which support G in that half-space. The convex dual problem tries to find the top intercept of all hyperplanes.

This view of dual variables is useful in proving strong duality from constraint qualifications like $x \in relint(F)$, where F is feasible set.

\subsubsection{Epigraph view}
Can consider $G' = \set{(u, v): \exists x \in dom(f_0): f(x) \leq u, f_0(x) \leq v}$; so $G \subseteq G'$.

\subsection{For perturbed problem}
$g'(m, l) = g(m, l) - u^{T}l - m^{T}v$.

\subsection{Lower bounds on solution to the primal}
$g(m, l) \leq L(x, m, l) \leq f_0(x)$ for all dual feasible $m$ and l and primal feasible $x$; as \\
$\sum m_{i}h_{i}(x) = 0; \sum l_{j}f_{j}(x) \leq 0$: as $l \geq 0, f(x) \leq 0$.

So, as $x^*$ is primal feasible, $\forall l \geq 0$: $g(m, l) \leq f_0(x^{*}) = p^{*}$.

Good way of showing if the problem is infeasible: $d^{*} = \infty$.

\exclaim{Applies to the case of conic inequalities}: just use the fact that $l \succeq_{K^{*}}0$ to see $l^{T}f_i(x) < 0$.

\section{Convex Dual problem}
$\max g(m, l) : l \geq 0$. Often, implicit constraint $m, l \in dom(g)$ made explicit.

Generalizable to primals constrained by conic inequalities: just use $l \succeq_{K^{*}} 0$, using dual of the cone K used to specify conic inequality constraints f().

Gives information about the sensitivity of the solution to perturbations in the input.

Solution notation: $m^{*}, l^{*}$, optimal value: $d^{*}$.

\subsection{Picking the right primal}
Equivalent formulations of the primal can lead to very different duals. Some duals can be hard to analyze, or useless (eg: constant); so reformulation of the primal (eg: by making implicit constraints explicit or vice versa) is a good trick.

\subsection{As finding top intercept of supporting hyperplane}
See dual function section.

\section{Duality gap b/w primal, dual solutions}
$p^{*} - g(l^{*}, m^{*})$ is the duality gap. As shown while considering the properties of the dual function, this quantity is non-negative.

\subsection{Strong duality}
When this is 0, strong duality holds: Eg: Many cases in Convex optimization.

Strong duality can also hold for non-convex problems.

\subsection{Strong duality: optimality conditions}
\subsubsection{Optimality conditions using strong duality}
Aka KKT conditions. Necessary conditions. Also sufficient when solving convex optimization: Certificate of optimality.

\paragraph{Primal and dual feasability}
Primal feasibility: $f(x^{*}) \leq 0, h(x^{*}) = 0$.

Dual feasiblity: $l^{*} \geq 0$.

\paragraph{Complimentary slackness}
$\forall j: l_{j}^{*}f_{j}(x^{*}) = 0$. Pf: $f_0(x^*) = g(l^{*}, m^{*}) \leq f_0(x^*) + \sum m_{i}^{*}h_{i}(x^{*}) + \sum l_{j}^{*}f_{j}(x^{*}) \leq f_0(x^*)$. So, as $l\geq 0, f(x) \leq 0$: \\
$\sum l_{j}^{*}f_{j}(x^{*}) = 0;\ \forall j: l_{j}^{*}f_{j}(x^{*}) = 0$.
 
If $l_i ^{*}> 0, f_i(x^{*}) = 0$, if $f_i(x^{*})<0, l_i^{*} = 0$: one of these is 0, hence the name.
 
At this point, $f_0(x^{*}) = L(x^{*}, l^{*}, m^{*}) = g(l^{*}, m^{*})$.
 
\paragraph{Optimality/ stationarity}
As $p^{*} = g(l^{*}, m^{*})$, $x^{*} = \argmin_x L(x, l^{*}, m^{*})$: \\ we have $\gradient_{x^{*}}L(x, l^{*}, m^{*}) = 0$. See geometric view of relationship between gradients of $f, h, f_0$ at optimal $x$ for why this should hold for some $l, m$ in general, when some nice conditions are met. With this condition, we are identifying $l, m$ as being none other than the solution to the dual problem.

This can be extended to the case where $L(x, l^{*}, m^{*})$, that is $f_0$ or $f$ or $h$ are not differentiable: in such a case, we just use some sub-gradients from the corresponding subdifferential set instead of gradients while writing the conditions.

\subparagraph{Extension to nondifferentiable convex f}
Applying the optimality criterion for convex non-differentiable functions we see that the same optimality criterion applies to problems involving non-differentiable convex constraint functionals: one just needs to interpret $\gradient f_i(x) \in \subdifferential f_i(x)$.

\subsubsection{Primal dual solvers}
They try to solve both the primal and dual problems simultaneously - maybe one of them gets solved quickly!

\subsection{Strong duality: Meaning of dual variables}
\subsubsection{As weights in lagrangian form}
Take problem in standard form, get dual function $g(l, m) = \min_x L(x, l, m)$; from strong duality, get: $\max_{l, m} g(l, m) = \max_{l, m} \min_x L(x, l, m) = p^*$. Note that $L(x, l, m) = f_0(x) + \sum_i l_i(f_i(x) - u_i) + \sum_i m_i(h_i(x) - v_i)$ if constraints are of the form $f(x) \leq u, h(x) = v$ : the constants u and v are included.

This gives a way to get the 'constraints in the objective' form of the optimization problem, by solving the hard-to-solve problem of finding the saddle-point corresponding to $\max_{l, m} \min_x L(x, l, m)$ in order to get $l^*, m^*$.

\subsubsection{Sensitivity analysis}
Using weak duality, for perturbed problem described earlier, get: \\
$p^{*}(u, v) \geq g(l^{*}, v^{*}) - l^{T}u - m^{T}v = p^{*}(0, 0)- l^{*T}u - m^{*T}v$, using strong duality.

So, for +ve $l_i$, large -ve perturbations in $u_i$ cause $p^{*}$ to increase.

Also if $p^{*}$ differentiable: $\gradient_u p^{*}(u, v) = -l, \gradient_v p^{*}(u, v) = -m$.


\subsection{Constraint qualifications to ensure strong duality}
See Slater conditions in convex optimization section. Others exist. Can derive some from the geometric/ supporting hyperplanes view of the dual problem.

\chapter{Vector optimization}
The objective function is a vector to vector function. Comparison among vectors could be wrt any pointed cone.

Objective often written as $\min (f_{01}(x), f_{02}(x) ..)$.

\section{Multicriterion optimization}
Special case where inequalities are wrt the non-negative orthant cone.

\section{Solutions}
\subsection{Optimality}
Feasible $x$ is pareto optimal if $f_0(x)$ is the minimal value of feasible region. If $f_0(x)$ is the minimum value, then $x$ is optimal.

\subsection{Scalarization}
Replace the objective with $l^{T}f_0(x)$: defines a set of trade-offs among the various measures of goodness which form the vector $f_0$.

\subsection{Visualiztion: tradeoff curve}
Visualize the image of the feasible region in the range of $f_0(x)$, where the axes are various dimensions of the image of $f_0$.

Pareto optimal points lie in the lower border of this image; for them, no dimension of $f_0(x)$ can be reduced without hurting other dimensions of $f_0(x)$: these are the minimal points of this set.

These correspond to different scalarizations of $f_0$. The endpoints correspond to the case where 0 weight is assigned to some dimension(s) of $f_0(x)$.

This curve is actually helpful for the enduser to pick a good scalarization.

\chapter{Prove properties of solution}
\section{Motivation, Notation}
Suppose that one wants to solve a problem or some special case of it, $Q$. One often wants to claim that a solution to $Q$ can be obtained by looking at the solutions of the optimization problem $P$ which is often easier to solve.

One then needs to be able to rigorously show that the solution to $P$ is also the solution to $Q$.

\subsection{Example}
In compressive sensing, \\
$Q \dfn \argmin \norm{x}_0 : Ax = b$ and $P \dfn \argmin \norm{x}_1 : Ax = b$.

\subsubsection{Uniqueness}
If one can either show that there is a unique solution to $P$, or if we can show that one can efficiently search the small number of solutions to $P$ in order to find a solution to $Q$, we have now have an efficient algorithm to solve $Q$.

\section{General proof technique}
Specify the 'optimality' properties $C_P(x)$ (certificate/ witness) which are satisfied exactly by solutions to $P$. Do the same for $Q$ and specify $C_Q(x)$. The idea is to show that $(\exists x: C_Q(x) \land C_P(x))$.

Construct a candidate solution $x$ to $P$ which simultaneously satisfies some subset of $C_Q(x) \union C_P(x)$. Show that this candidate then satisfies the remaining properties in $C_P(x) \union C_Q(x)$.

\subsection{Possible computational intractability}
Even though construction of the solution to $P$ and $Q$ described above is a valid proof technique, it often does not imply that we have an efficient algorithm to solve $Q$ using just the conditions $C_Q$ and $C_P$. This is because, to completely specify $C_Q$, one needs to know the solution to $Q$!

Eg: In the compressive sensing example, for $C_Q(x)$ to be completely specified, one needs to know not just $\norm{x}_0$, but also the coordinates $\set{i: x_i \neq 0}$.

\part{Problem formulation, solution}
\chapter{Formulating the problem}
\section{Focus on easily solved forms}
Modellers of natural phenomena and engineers formulate optimization problems. They know what classes of problems are easy to solve. So, they often draw on a mental library of functions, composition rules to create, for example a convex program.

\section{Dealing with strict inequality constraints}
Replace with non-strict inequalities.

\section{Use equivalent formulations (for convexity?)}
You can always replace $f_0(x)$ or $f_i(x)$ \\
with a $p(f_i(x))$ if p is monotonically increasing with $f_i$, and if you can translate between $\argmin f_0(x)$ and $\argmin p(y)$ or between $f_i(x) \leq 0$ and $p(y) \leq c$.

Some formulations are more easily solved than others. In fact, it can turn a non convex problem to an equivalent convex problem.

Eg: $\min \norm{x}_2 + t\norm{x}_1 \equiv \min \norm{x}_2^{2} + t\norm{x}_1$.

Also, can use the dual if strong duality holds.

\chapter{Problem parameters}
\section{Not variables}
These are different from variables. For example, in $\min \norm{Ax - b}^{2} + l \norm{x}^{2}$, while $x$ is a variable, l is a parameter to the optimization problem.

\section{Picking the best parameter}
With problem parameters unspecified,\\
 you have a variety of optimization probems. Then, pick the best problem to solve, ie pick the best parameter.

\subsection{Qualitative judgement}
Set various values for the problem parameters, solve them, and pick the parameter whose solution appears best, from the perspective of the application domain.

\subsection{Theoretical constraints}
Theoretical analysis of the problem domain sometimes connect the parameter values with desirable traits in the solutions.

\chapter{Specification framework engineering}
Specification frameworks abstract away from the \\
mathematical details, methodological details (which underlying solver to call), make specification easy. They lead to wide application.

It is a good idea to mimic the way people go about naturally formulating optimization problems for a certain class: eg by drawing on a library of functions known to be convex and rules of composition. Eg: cvx for disciplined convex programming.

\part{Linear programming}
\chapter{The problem}
Minimize linear/ affine function over a polyhedron: ie subject to affine/ linear constraint functions.

\section{Canonical form}
$\vec{x} \in R^{d}$. $\min c^{T}x: Ax \leq b$.

\subsection{Nonnegative optimization form}
$\min c^{T}x': Ax' \leq b$, with $x' = \mat{x^{+}\\ x^{-}}$; new constraints: $x' \geq 0$, $x^{+} - x^{-} = 0$.

\subsection{Equality constrained form}
Writable as $\min c^{T}x: Ax = b; $x$ \geq 0$, using slack variables.

\section{Constraints and the polyhedron}
Every constraint is a halfspace. Intersection of halfspaces is a polyhedron; this is the \textbf{feasable region}.

\section{The solution}
Want to find a point in the feasible region making the least angle/ inner product with $f$.

\subsection{Vertex in feasible region}
Consider minimizing $f^{T}x$ over a line segment (a, b): for any point in this segment, you have: $f^{T}(ta + (1-t)b) \leq \min \set{f^{T}a, f^{T}b}$; so sufficient to consider end points while trying to minimize $f^{T}x$ over any line segment in the feasible region. By induction, sufficient to consider vertices of the feasible polyhedron.


\subsection{Pathological cases}
$\min x$, A is empty; and $\min $x$: x\geq 50$: often ruled out by added resource constraints: $x$ allowed to be only so small.

\chapter{Related problems}
\section{Linear fractional program}
Minimize linear fractional function over a polyhedron. Actually quasi-convex programming, but reducible to LP. Can rewrite $\min t: \frac{a^{T}x + b}{c^{T}x + d} \leq t$ using linear constraints.

Similar is the case with generalized linear fractional program:\\ $\min t: (\max_i \frac{a_i^{T}x + b_i}{c_i^{T}x + d_i}) \leq t$.

\section{1, infty norm approximations}
$\min_x \norm{Ax - b}_\infty \equiv \min c: Ax - b \leq c1, Ax - b \geq -c1$.

$\min_x \norm{Ax - b}_1 \equiv \min c : Ax - b = t_+ + t_-; t_- \leq 0; t_+ \geq 0; 1^{T} t_+ - 1^{T}t_- \leq c$. This tends to yield sparse solutions: consider the functioning of least angles regression to see why.

Similarly, can replace 1, $\infty$ norm constraints in any LP.

\section{Robust linear programming}
Often, there is uncertainty in constraints like $a_i^{T}x \leq b_i$.

\subsection{Deterministic model}
x must be feasible $\forall a_i \in S$. If S is an ellipse, this can be handled using SOCP.

\subsection{Stochastic model}
Feasible $x$ must satisfy $Pr(a_i^{T}x \leq b_i) \geq t$. If distribution is Gaussian, this can be handled using SOCP.

\chapter{LP Algorithms}
\section{Exhaustive search alg}
Find intersection of every set of d hyperplanes (constraints); see if it satisfies other constraints: $O(\binom{n}{d})$.

\section{Simplex method}
(Dantzig). Theoretically exponential, highly efficient in practice.

\section{Interior point projective method}
(Karmarkar) LP solvable in time \\
$poly(n, d)$.

\part{Convex Quadratic programming}
For ideas about its importance - both in solving general convex optimization problems and in other domains, look at the 'Least squares' chapter.

\chapter{Linear Constrained QP}
Aka Quadratic programming. Minimize convex quadratic functional over a polyhedron (see vector spaces survey). Generalizes linear programming, special case of QCQP.

\section{Visualization}
If optimal value occurs when a constraint is active: Think of contours of the objective hitting an edge of the polyhedron. Slightly harder than in the LP case, where the minimum always occured in a vertex.

\section{Dual problem}
This is also a quadratic program; strong duality always holds! \chk

\chapter{Quadratically constrained QP (QCQP)}
Minimize convex quadratic functional, subject to convex quadratic constraints: so all equality constraints are specified by linear inequalities. Feasible set is an intersection of hyperellipes and a polyhedron.

\chapter{Least squared error problem}
\section{Problem}
Aka least squares.

\subsection{Unregularized problem}
$\min \norm{Aw - b}^{2}$, $A \in R^{m \times n}, m>n, rank(A) = m$.

\subsubsection{For the weighted least squares}
More weight to one observation or correlated observations:\\ Use Weighted inner product to find projection.\\
$WAx = Wb; \min \norm{WAx - Wb} = \min \norm{Ax-b}_{M}$ for $M=W^{*}W$.

\subsubsection{Solution}
See numerical analysis survey for geometry and solution.

\subsection{The regularized problem}
Aka weight decay.

Want to balance love of sparsity or smallness with desire for a good fit. Pick a regularizer which is small for good w, and large for bad w.

Take $\min f(w) + l\norm{w}_n$. As n decreases, feasible set decreases: consider progression of 1 balls from $\norm{.}_\infty$ to $\norm{}_0$.

\section{Application}
\subsection{Convex Optimization}
A common technique of solving convex optimization problems is to  approximate the objective with a quadratic function and then solve it.

\subsection{Other domains}
See regression problem in statistics survey for motivation. Same as finding maximum likelihood solution while fitting linear model with Gaussian noise.

Interpolation of functions linear in parameters is also a specific case: see numerical analysis survey.

\subsection{Regularized problems}
If solving a regression problem, this amounts to the least squares solution, when a prior probability distribution is imposed on w to favor smaller w values.

$w$ could be the coefficients of polynomial used to fit data in $A$, $b$. To avoid overfitting, we want $w$ to be small.

\subsubsection{Penalties and priors}
Different regularizers (usually norms) correspond to different priors on w. Shape of the prior distribution looks like the unit spheres corresponding to the norms used: sphere, diamond etc..

$\norm{}_{q}$ penalty for $q> 2$ usually not worth the effort. Sometimes, $\norm{}_{1.x}$ penalty used as compromize between lasso and quadratic regularizers.

\section{Standard formulations}
\subsection{Normalizing columns of A}
By solving $A'w' = b$, can solve $ADD^{-1}w \approx b$ using diagonal matrix D.

\section{Quadratic regularizer}
Aka ridge regression. \\
$\min f(w) + lw^{T}I'w= \min \norm{Aw-b}_{2}^{2}+ lw^{T}I'w$ instead, to control size of w too.

This objective is the lagrangian fn corresponding to the problem of finding $\min f(w) = \min \norm{Aw-b}_{2}^{2}$ subject to $w^{T}I'w < c$.

\subsection{The solution form}
$(A^{T}A + lI)\hat{w} = A^{T}b$: by setting $\gradient (f(w) + l\norm{w}_{2}^{2})= 0$. Take $A = U\SW V^{*}$, get $X\hat{w} = U \SW (\SW^{2} + lI)^{-1} \SW U^{T}y = UDU^{T}y$, where $D_{i,i} = \sw_{i}' = \frac{\sw_{i}^{2}}{\sw_{i}^{2} + l}$.

So, $X\hat{w} = \sum \sw_{i}' u_{i}^{T} u_{i}y$; observe action of $l \in [0,\infty]$ in $\sw_{i}'$: the shrinkage parameter; or the prior distribution on w.

\subsection{The geometry}
Consider hypersphere $\norm{w}_{2}^{2} < c$ and the paraboloid $f(w)$. The objective is to find w within the hypersphere, having the minimum $f(w)$ value. Visualize by taking contour map of $f(w)$ : successively larger ellipsoids. optimal $w$ is where the minimal contour touches the hypersphere, when the center of $f(w)$ lies outside the hypersphere.

Compare with geometry of $\min f(w)$ problem: see linear alg survey.

\subsection{Effect}
Shrinks components of $w$ towards each other in magnitude. Also takes care of correlated features.

Does not penalize small $w_i$, so does not lead to sparsity.

\section{l2 and linf regularizers}
\subsection{l2 regularizer}
\subsubsection{Problem}
Solve $\min_w \norm{Aw - b}^{2} + \gl \norm{w}_2$, with $\gl > 0$. An alternate formulation is $\min_w w^{T}Hw + v^{T}w + \gl \norm{w}_2$, with $H \succeq 0$.

\subsubsection{Solution}
The obtimality condition involves taking the subdifferential of $\norm{w}_2$. For diagonal $H$, this has a closed form solution. The solution technique involves some nifty variable substitution.

\subsubsection{Relation with Quadratic regularizer}
Rewriting as $\min_w \norm{Aw - b}^{2} : \norm{w}_2 < C(\gl)$; we see that this is equivalent to using the quadratic regularizer and solving: $\min_w \norm{Aw - b}^{2} : \norm{w}_2^{2} < C(\gl)^{2}$. So, the tradeoff curve, is for practical purposes, identical.

The latter formulation has a simpler solution as subgradient calculations are not involved in specifying the optimality conditions for analytically finding a solution.

But, sometimes we are given the problem where an $l_2$ regularizer is used. Such problems occur in doing block coordinate descent to solve multi-task lasso, for example, while finding the search direction using a quadratic approximation of the objective. In such cases, we can either solve the specified problem directly, or we can reformulate it using the quadratic regularizer; the former is quite simple.

\subsection{lInf regularizer}
Here, we solve $\min_w \norm{Aw - b}^{2} + \gl \norm{w}_\infty$. Optimality condition involves taking the subdifferential of $\norm{w}_\infty$. A closed form solution can be derived by using proof by cases: eg: see a paper on muti-task learning by Han Liu.

\section{Forward stepwise regression for sparsity}
Trying to solve Aw = y. Take F = active set of features used in approximating y. Start with $F = \nullSet$, w = 0. At step t, find residue $r = y - Aw_t$, find feature i most correlated with r; set $w_{i}$ to this projection; continue. \exclaim{Observe how the residue changes at each step!}

This is a greedy algorithm, so suboptimal. But note that this is not the same as simply finding the optimal w'' without any sparsity constraint, and then dropping some small elements.

\subsection{Forward stagewise regression}
A non greedy modification to forward stepwise. Find feature set B with maximum correlation with residue; for each such i: keep increasing $w_{B}$ until you find another feature(s) with equal correlation with the residue; then add these feature(s) to B; repeat.

At any time, B is the set of features which form the \exclaim{least angle} with the residue. The coefficients increase such that $Aw_B$ increases exactly along the projection of the residue on the hyperspace spanned by features in B.

Also, in case at some time, a feature correlated with other some feature in B gets added to B, the coefficient of B.

\subsection{Least Angle Regression}
A modification of the Forward stagewise regression, so that $w_{B}$ is increased at one shot, rather than gradually.

\subsection{Lasso solving modification}
Losso just specifies an objective, which may be achieved in many ways, including using forward regression.

Can turn LAR to Lasso solver: Whenever $w_{i}$ for some i in B hits 0, drop it out of B.

\subsubsection{Reason for the fix}
Compare conditions you get on lasso solution by setting gradient to 0 with conditions for LAR at any time: Correlation of residue with B with features in B is equal and correlation with other features is lower. They are identical as long as $sgn(w_j) = $ sign of correlation of residue with feature j.

\section{0 norm regularizer: Compressed sensing}
\subsection{Problem scenario}
X is short and fat, Xw = y has many solutions for w, want to find w with $\norm{w}_0\leq s$, $Xw \approx y$. If X were tall and thin, the solution of a similar problem is easy: solve $\min \norm{Xw - y}^{2}$ and pick the s most important components of w.

Consider the equivalent problem, where we assume columns of X are normalized.

\subsubsection{Finding support: Combinatorial hardness}
The difficulty comes from finding the support (non-zero coordinates) of w. Once support of w is found, it is easy to find the optimal linear combination of these components to get close to y: just solve $\min \norm{X'w' = y}$, where $m \times s$ X' is derived from X by dropping some columns.

There are many possible ways to form w with $\norm{w}_0\leq s$.

\subsection{Finding support: Target optimization problems}
Maybe want to limit w to certain number of non-zeros. Solve $\min f(w) + l \norm{w}_{0}$, where $f(w) = \norm{Xw - y}_2^{2}$.

This is same as $\min f(w): \norm{w}_{0} \leq s$. Feasible set is not convex: consider epigraph of $\norm{w}_0 \leq 1$ for example.

A stricter version of the problem is: $\min \norm{w}_0 : Xw = y$, as this does not allow $Xw \approx y$.

\subsection{Solution using 1 norm minimization}
Just solve the linear program $\min \norm{w}_1 : Xw = y$.

Arrival at sparse solution is guaranteed when some restricted isometry and incoherence properties hold for X. 

\subsubsection{Restricted isometry constant for s}
$1-\gd_{s} \leq \frac{\norm{Xw}^{2}}{\norm{w}^{2}} \leq 1+\gd_s, \forall w: \norm{w}_0 \leq s$.

\subsubsection{Incoherence/ almost orthogonality}
$\abs{\dprod{Xw, Xw'}} \leq T_{s, s'}$ for w, w' such that $supp(w) \inters supp(s') = \emptyset, \norm{w}_0 \leq s, \norm{w'}_0 \leq s'$.

\section{1 norm regularizer: Lasso}
$\min f(w) + l \norm{w}_{1} = \min \norm{Aw-b}_{2}^{2} + l \norm{w}_{1}$. Allegedly aka least absolute shrinkage and selection operator.

Same as minimizing f(w) subject to $\norm{w}_{1} < c$. As you increase c, you get less and less sparse solutions. When $c\geq \norm{\hat{w}}_{1}$ where $\hat{w}$ is the unregularized least squares solution, you get least squares solution.

Want to get optimality condition $\gradient (f(w) + l \norm{w}_{1}) = 0$, but $\norm{w}_1$ not differentiable at all points; so take $l \norm{w}_{1} = l \sum_i sgn(w_i) w_i$. Assume that $sgn(w_i')$ around the solution $w'$ is known.

Get conditions useful in Lasso solving modification of Least angles regression: Let $B = {o: w_i' \neq 0}$, the basis/ support set. $\forall j\in B: a_{j}^{T}(y-Aw) = \gl sgn(w_j)$, $\forall j\notin B: a_{j}^{T}(y-Aw) = \gl *sgn(w_j) \leq |\gl|$. If A's columns have norm 1, get geometric meaning for $\gl$: an upper bound for correlation of the residue with any feature!

\subsection{Importance}
Often yields sparse solutions: from geometry.

$\norm{w}_q$ for $q \in (0, 1]$ would also yield sparse solutions: see geometric interpretation for justification. These correspond to imposing the constraint $\norm{w}_q \leq c$, which corresponds to the feasible region being non-convex. 1 norm is the least q which yields besides leading to a sparse solution, is also convex.

For importance of finding sparse models, see statistics survey.

\subsection{The geometry}
$\norm{w}_{1} < c$ is hyper-rhombus, f(w) is a paraboloid, whose each level set is an ellipsoid. In general, w for min f(w) will lie outside the hyper-rhombus. So, the optimal w: the point where the edge of the hyper-rhombus meets the t-level set: f(w) = t, for the smallest t. Usually this happens where 2 edges meet, or where many $w_{i}$ are 0: visualize in 2D; hence sparsity.

Compare with ridge regression.

\part{Convex optimization}
\chapter{The problem}
\section{Importance, efficient solvability}
Superset of LP. Many problems in nature are convex optimization problems. Many non-convex problems have convex equivalents : see section on modelling/ specifying optimization problems.

(Nesterov, Nemorinsky) For any convex optimization problem, there exists self concordant barrier functions; so interior point methods and barrier methods can be made applicable: This is a non constructive proof. So, there exists a polynomial time algorithm for every convex optimization problem, but you will find it with certainty only if you can find these self concordant barrier functions.

\section{Standard form}
$\min f_0(x): f(x) \leq 0, Ax = b$.

All $f_i(x)$ are convex.

\section{Convexity of feasible region X}
Optimization fn is convex, constraint sets are convex sets. So, X is convex.

\subsection{Geometry}
$\gradient f_0(x^{*})$, if $\neq 0$, defines supporting hyperplane for X at $x^{*}$. Imagine contours of $f_0$, with minimum outside X, colliding with X at some point $x^{*}$.

\section{Identifying convex opt problems}
\subsection{Check Convexity of fesible regions}
Maybe compare with known convex sets (see vector spaces survey).

\subsubsection{Any equality constraints should be linear.}
Eg: $x: h_i(x) = b$ not a convex set when $h_i(x)$ is a quadratic fn. These could be replaced by 2 inequality constraints in the standard form; both of which would need to represent convex sets.

\section{Properties}
\subsection{Local optimum is the global optimum}
By contradiction: Take radius R local optimum x; suppose there were y more than R away from $x$ with $f_0(y) < f_0(x)$; then, can conjure a point z, a convex combination of y, $x$ which is less than R away from $x$ with $f_0(z) < f_0(x)$.

\subsection{Lagrangian dual functional}
Can easily get the Lagrangian dual functional: $g(l, m) = \inf_x L(x, l, m)$ by setting $\gradient_x L() = 0$ and eliminating $x$ from L(x, l, m).

\subsection{Certificate of optimality with strong duality}
Aka KKT certificate. If feasible $(x', l', m')$ satisfy KKT conditions, they are optimal. THis is a good way of solving convex optimization problem.

Pf: From complimentary slackness: $f_0(x') = L(x', l', m')$; from the optimality condition and convexity of $f$, $f_0$: \\
$g(l', m') = \inf_x L(x, l', m') = L(x', l', m') = f_0(x')$. So, $x'$ is optimal.

\subsection{Bound norm of solution}
All sublevelsets of $f_0$ are convex. So, find a ball $\set{x: f_0(x) \leq B > p^{*}}$ which includes $0$; then we can say that $\norm{x} \leq 2B$.

Eg: This technique was used in bounding the deviation of the solution of a l1 regularized logistic regression problem from the actual parameters defining an ising model by pradeep etal.

\section{Dual problem}
Strong Duality usually holds. 'Constraint qualifications' can tell you whether strong duality holds.

\subsection{Strict feasibility Constraint qualification}
(Slater) If there exists  some strictly (primal) feasible $x$, strong duality holds. \exclaim{This is very general: some strictly feasible $x$ should exist!} Generalizable to convex programs specificed using conic inequalities.

Actually, can relax constraint qualification to: $\exists x: $x$ \in relint(D)$ wrt affine plain $Ax - b = 0$.

\exclaim{Applies to convex conic inequality constraints too!} Also, this is a way to get strong duality without using the global optimality criteria perspective.

\subsubsection{From supporting hyperplanes view}
Consider the supporting hyperplanes view of the dual problem (see dual problem section). For convex optimization problems, convexity of $f$, $f_0$ ensures that  $G = \set{(u, t)| f_0(x) \leq t, f(x) \leq u}$ is convex. So, supporting hyperplane at $(0, p^{*})$ for both G and \\$G'=\set{(u, t)| f_0(x) \leq t, f(x) \leq 0}$ are the same. 

When the primal is strictly feasible, the supporting hyperplane at $(0, p^{*})$ is non vertical; so intercept with $f_0$ axis is well defined; and the dual problem is feasible. Thence, slater's condition.

\chapter{Unconstrained problems: algorithms}
\section{Problem, algorithm framework}
\subsection{Problem, Assumptions}
Problem: $min_x f(x)$, f convex.

\subsection{Algorithm framework}
Produce sequence $x^{(k)} \in dom(f), k\in Z_+$: $f(x^{k}) \to p^{*}$. Input to algorithm: starting point $x^{(0)}$.

Alternate notation: $x^{+}$ for current iterate.

\subsubsection{As Iteratively solving gradient eqns}
Can be interpreted as iterative methods\\
 for solving optimality condition $\gradient f(x) = 0$: a set of non-linear equations.

\subsection{Common assumptions}
\subsubsection{Assumptions about f}
f is twice continuously differentiable, so dom(f) is open.

$p^{*}$ is attained.

\paragraph*{Strong convexity}
If f is strongly convex, $f(x) - p^{*} \leq (2m)^{-1}\norm{f(x)}_2^{2}$; so RHS can be used as a stopping criterion.

Also guarantees that sublevelsets are bounded: $f(y)\geq f(x) + \gradient f(x)^{T}(y-x) + \frac{m}{2}\norm{x-y}_2^{2}$; this ensures faster convergence.

\subsubsection{Initial point: Assumptions}
$x^{(0)} \in dom(f)$.

Sublevel set $S = \set{x: f(x) \leq f(x^{(0)})}$ is closed. This is hard to verify usually, except when all sublevel sets are closed $\equiv$ epigraph of f is closed. This is needed, because some methods try to draw secants in the epigraph of f(x).

\section{Descent methods}
\subsection{Algorithm}
$x^{(k+1)} = x^{(k)}+ t^{(k)}\change x^{(k)}$ with $f(x^{(k+1)}) < f(x^{(k)})$; do this repeatedly until stopping criterion is met, in each iteration: finding step or search direction $\change x$, the step size/ length t.

This is guaranteed to eventually come arbitrarily close to the minimum.

\subsubsection{Descent direction}
$\change x$ is a descent direction if $\gradient f(x)^{T}\change x < 0$.

\subsection{Find search direction}
\label{descent:Search Direction}
See later sections.

\subsubsection{Search direction from optimality conditions of approximation}
Aka Newton equations in some cases.

Maybe model f(x) with $\hat{f}(x)$, to find a search direction: find the optimality conditions for minimizing $\hat{f}(x)$ or f(x) and find $\change x$ which does this exactly or approximately.

For examples, see section on 2nd order approximation descent.

\subsection{Line search for t}
\subsubsection{Restriction to a slice}
$g(t) = f(x+ t \change x)$ is f restricted to a slice along $\change x$. This is convex as f is convex.

\paragraph*{Visualization}
Plot $g(t)$ vs t, try to find $t$ which minimizes this.

\subsubsection{Exact line search}
$t = \argmin_{t>0} g(t)$. This is usually expensive.

\subsubsection{Backtracking}
Parameters: $a \in (0, 1/2), b \in (0, 1)$; b is the shrinkage parameter.

Start with t=1 (or small enough $t$ to guarantee that $x + t\change $x$ \in dom(f)$), repeat $t \dfn bt$ until stopping criterion is met:
$f(x + t \change x) < f(x) + at\gradient f(x)^{T} \change x$. With shrinkage of t, the RHS reduces. This is aka Armijo's rule.

Can be rewritten as $f(x + t \change x)- f(x) < ah(t)$, where h(t) is the change in f for t, according to a linear approximation of $f$. This form is useful when f() is composed of differentiable and non-differentiable parts.

\paragraph*{Guaranteed reduction in objective}
As $\change x$ is a descent direction, \\
$\gradient f(x)^{T} \change x < 0$.

So, when $f(x + t \change x) < f(x) + at\gradient f(x)^{T} \change x$, we have $f(x + t \change x) < f(x)$, so it is a step size which definitely reduces f().

Such a $t$ will always exist: you keep decreasing $t$ until that happens.

\paragraph*{As secant in epigraph of g(t)}
Take $g(t):R \to R$: one dim fn. Consider $\gradient_t g(t)$ at t=0. 
$\gradient_t g(0) = \gradient_x f(x) \change $x$ \leq 0$. $g(0) + at\gradient_t g(0) = f(x) + at\gradient f(x)^{T} \change x$, when a=1, is the tangent to g(t) at t=0, and therefore to f(x). $f(x) + at\gradient f(x)^{T} \change x$, for $a<1$ is such a secant: you are making the slope less negative.

x changes along the search direction only, but you change $t \in [0, t_{0}]$, to get $t$ which is close (from below) to the intersection of a secant in the epigraph of g(t) with g(t).

As $f(x) + at\gradient f(x)^{T} \change x$ is a secant, for the stopping condition should be met for some $t$ if $\change x$ is indeed a descent direction. This ensures that $t$ is such that $f(x + t \change x) - f(x) \approx  at\gradient f(x)^{T} \change x$, the improvement achieved by using a secant.

\paragraph*{Variants}
Rather than ensure that $t$ is such that \\
$f(x + t \change x) - f(x) \approx  ah(t)$, the improvement achieved by using a secant; one can use second order approximations instead to define h, rather than a linear approximation: this perspective is different from the 'secant view'.

\subsubsection{Arbitrary choice}
Using backtracking rule or doing exact search during line search guarantee reduction in the objective (and therefore convergence), but they can be expensive. An alternative can be to just use 1 as the step length.

The cost of doing so is that convergence is no longer guaranteed: it is possible for example that, taking the step 1 repeatedly, one cycles between the same pair of points between which lies the optimal point. However, in practice, convergence is usually observed.

This technique is used, for example, in \\
'iterateively reweighted least squares', where each step involves solving a least squares problems (which may correspond to the local 2nd order approximation).

\section{Steepest descent}
Aka Gradient descent.

\subsection{As 1st order approximation minimization}
As $f(x + v) = f(x) + \dprod{\gradient f(x), v}$, minimize $\dprod{\gradient f(x), v}$ - Also see geometric view section.

\subsection{Descent direction}
For a given $\norm{.}$, $\change x = \argmax_{v: \norm{v} = 1} \dprod{-\gradient f(x), v}$.

$\dprod{\gradient f(x), v}= -\dprod{\gradient f(x), -v} = -\norm{\gradient f(x)}_*$. \exclaim{So this is a descent direction!}

\subsection{Stopping criterion}
Use $\norm{\gradient x} \leq \eps$ as stopping criterion.

\subsection{Convergence}
$f(x^{(k)}) - p^{*} \leq c^{k}(f(x^{(0)}) - p^{*})$. \why

\subsection{Geometric view}
\subsubsection{2-dim functional example}
Consider contours/ level sets of $f:R^{2} \to R$. The level set $\set{x:f(x) = t}$ surrounds $\set{x:f(x) = t-1}$; etc.. At $x: f(x) = t$, the $\gradient f(x)$ is perpendicular to level set $\set{x:f(x) = t}$, pointing away from $\set{x:f(x) = t-1}$; and so is the gradient descent direction; but it points towards $\set{x:f(x) = t-1}$.

\subsubsection{Goodness for circular level set case}
If the level set contours are circular, then $-\gradient f(x)$ points straight towards the minimum level set. But, consider an ellipsoid. Then, $-\gradient f(x)$ is passes through a few level sets $\set{x:f(x) = t' < t}$, but it misses many smaller level sets. Eg: imagine level sets which are shaped like concentric rounded triangles.

\subsubsection{Zig-zagness of path towards optimum}
In this case, despite finding the best step size, the sequence of points produced is not a direct line towards $x^{*}$, but forms a zig-zag path.

\subsubsection{Implications of choice of norm}
If you choose a good $\norm{.}$, the shape of whose unit ball approximates the shape of the level sets/ contours, $\change x$ will be such that it points towards the minimal contour of $f$, while still having a large enough inner product with $-\gradient f(x)$ to guarantee that it is a descent direction. So, you can get to $x^{*}$ more directly.

But, if you pick a bad norm, Eg: \exclaim{$\norm{}_2$ which results in a reduction to gradient descent}, the path to $x^{*}$ is longer.

\section{2nd order approximation descent}
Aka Newton method (reason described below), but not Gauss-Newton method, which is a further approximation and is specific to least squares problem.

\subsection{General search direction}
Take $f_0(x+d) \approx f_0(x) + x^{T}\gradient f(x) + x^{T}Hx$ for some $H \succeq 0$. If $H$ is the 2nd order derivative, this is the Newton method described later. Often $H$ is an easily computed approximation of $\gradient^{2}f_0(x)$.

\subsection{Search direction from Hessian}
$\change x = -\gradient^{2}f(x)^{-1} \gradient f(x) $. Minimized 2nd order approximation: $\hat{f}(x + v) = f(x) + \dprod{\gradient f(x), v} + 2^{-1} v^{T}\gradient^{2}f(x)v$. So, $f(x)$ approximated with a quadratic curve!

This is Aka Newton's method, as it corresponds to solving for the root of the optimality condition $\gradient f(x) = 0$.

\subsubsection{Solving Newton equations fast}
Solving the linear system of equations: $\change x = -\gradient^{2}f(x)^{-1} \gradient f(x) $ can be expensive in general: $O(n^{3})$. But, in practice, one can often exploit structure  in the $\gradient^{2}f(x)^{-1}$ matrix to find the (possibly approximate) search direction fast - this is very important. Can often use iterative methods to solve this.

\subsubsection{Correctly computing gradient and Hessian}
See comments from the gradient descent case.

\subsection{Geometric view}
Consider the 2-dimensional example described in the 'geometric view of steepest descent' section.

\subsubsection{Local approximation by ellipses}
Note: picking $\norm{x} = (\dprod{x^{T} Px})^{1/2}$ for $P \succeq 0$ yields ellipsoid unit balls. This can be thought of taking: $\hat{f}(x + v) = f(x) + \dprod{\gradient f(x), v} + 2^{-1} v^{T}Pv$ and using $\inf_v \hat{f}(x + v)$ as the search direction. 

\subsubsection{2nd order approximation ellipses}
Choosing $\norm{x} = (\dprod{x^{T} \gradient^{2}f(x)x})^{1/2}$, the local Hessian norm, results in reduction to 2nd order approximation descent. To see this, consider the 'constraints in the objective' form of $\change x = \argmax_{v: \norm{v} = 1} \dprod{-\gradient f(x), v}$.


\subsection{Affine invariance}
The newton method is supposed to be invariant to affine transformation of the input: can confirm by computing $\gradient$ and $\gradient^2$ for $f(Ax' + b)$ and $f(x)$, where $Ax' + b = x$. 

\subsection{Stopping criterion: Affine invariant }
Take $\gl(x) = (\gradient f(x)^{T} \gradient^{2}f(x)^{-1}\gradient f(x))^{1/2} = (-\gradient f(x)^{T}\change x)$, use $\gl(x) \leq \eps$.

Estimates proximity to $x^{*}$: $f(x) - \inf_y \hat{f}(y) = 2^{-1}\gl(x)^{2}$: from simple substitution.

Equals length of $\change x$ in the local Hessian norm : see steepest descent connection.

Also measures the $\change x$ directional derivative, $\gradient f(x)^{T} \change x = -\gl(x)^{2}$. So, it measures how close $\gradient f(x)$ is to being 0, as it is at $x^{*}$.

This is also affine invariant, unlike $\norm{x}$. Pf: Consider x = Ax' +b. Consider $D_{x'}f(Ax'+b) = D_{Ax'+b}f(Ax'+b)A = D_{x}f(x)A$; thence get $\gradient_{x'} f(Ax'+b)$, and thence $\gradient_{x'}^2 f(Ax'+b) = D_{x'} \gradient_{x'} f(Ax'+b)$.

\subsection{Speed: comparison with 1st order methods}
Computing the search direction makes 2nd order methods slow in general; but if some structure in $\gradient^{2}f(x)$ is exploited to make this faster, it becomes very fast.

Each iteration of 1st order methods are usually much faster, but many more iterations are required.

\subsection{Convergence: classical bounds}
\subsubsection{Assumptions}
f strongly convex with constant m.

$\gradient^{2} f$ is Lipschitz continuous. This measures how well f(y) can be approximated by the quadratic functional $f(x) + \gradient f(x) (y-x) + 2^{-1}(y-x)^{T} \gradient^{2}f(x) (y-x)$.

$\exists \gh \in [0, m^{2}/L], \gamma > 0$.

\subsubsection{Linear decrease phase}
Aka Damped Newton Phase. $\norm{\gradient f(x)} \geq \gh \implies f(x^{(k+1)}) - f(x^{(k)}) \leq -\gamma$. Linear decrease in objective value.

This phase ends after atmost $\frac{f(x^{(0)} - p^{*})}{\gamma}$ iterations.

Most iterations require backtracking search.

\subsubsection{Quadratically convergent phase}
$\norm{\gradient f(x)^{(k)}} \leq \gh \implies \frac{L}{2m^{2}}\norm{\gradient f(x^{(k+1)})} \leq (\frac{L}{2m^{2}}\norm{\gradient f(x^{(k)}})^{2}$. Quadratic decrease in gradient size.

So, for $l>k$: $\frac{L}{2m^{2}}\norm{\gradient f(x^{(l)})} \leq 2^{-2^{(l-k)}}$.

All iterations use step size $t=1$! \why 

\exclaim{Observing these on a (maybe log) residue vs iteration and step size vs iteration plots is a good way of verifying correct implementation of gradients etc..!}

\subsubsection{Overall bounds, defects}
To achieve $f(x) - p^{*} \leq \eps$: $\frac{f(x^{(0)} - p^{*})}{\gamma} + \log \log (\eps_0/ \eps)$ iterations needed, where $\eps_0 = f(m, L)$ too.

Provides qualitative insight into behaviour of 2nd order approx descent.

But, constants m, L are usually unknown: so cannot say beforehand when convergence will happen.

Bounds are not affine invariant, even though the 2nd order approx descent method is.

\subsection{Convergence for self concordant functions}
(Nesterov, Nemorinsky) Get better bounds, which don't suffer from the defects of classical analysis.

\section{Alternating minimization}
Consider f(x, y); repeat these steps: \\
$x \assign \argmin_x f_0(x, y), y \assign \argmin_y f_0(x, y)$. Guarantees that the objective is reduced in each iteration.

When minimization is done one coordinate at a time, it is called coordinate descent; if it done one coordinate-set at a time, it is called block-coordinate descent.

\section{Diagnosing error in code}
\subsection{Incorrect gradient computation: symptoms}
Often algebraic mistakes happen while computing the gradient.

To detect such a case, compare with the numerically computed gradient: See numerical analysis survey.

Or, observe that the step size found in the direction of the gradient is very small.



\chapter{Equality constrained problems}
\section{Problem, assumptions}
$\min f(x): Ax = b$. So, optimality conditions: $\gradient f(x^{*}) + A^{T}v^{*} = 0$.

\subsection{Common Assumptions}
f is twice differentiable, A has full row rank: can always get an equivalent problem which satisfies this.

Optimum $p^*$ is attained.

\section{Solution strategies}
\subsection{Reduction to unconstrained optimization}
$\min f(x): Ax = b \equiv \min f(Hv + x')$, where $Ax' = b$, H spans null(A).

\subsection{Search direction from optimality conditions of approximation}
See \ref{descent:Search Direction}.

For examples, see section on 2nd order approximation descent.

\subsection{Local approximation by ellipsoid}
Minimize $f(x + \change x) \approx \hat{f}(x + \change x) = f(x) + \gradient f(x)^{T}v + 0.5 \change x^{T} P\change x$ subject to $A(x + \change x)= b$, for $P \succ 0$. From optimality conditions for minimum of this approximation, get search direction: $\mat{P & A^{T}\\ A & 0} \mat{\change $x$ \\ w} = \mat{-\change f(x) \\ 0}$.

\subsection{2nd order approximation descent}
Aka Newton method. Minimize $f(x + \change x) \approx \hat{f}(x + \change x) = f(x) + \gradient f(x)^{T}v + 0.5 \change x^{T} \gradient^{2}f(x) \change x$ subject to $A(x + \change x)= b$. 

\subsubsection{Search direction}
From optimality conditions for minimum of this approximation, get search direction: $\mat{\gradient^{2}f(x) & A^{T}\\ A & 0} \mat{\change $x$ \\ w} = \mat{-\change f(x) \\ 0}$.

\paragraph*{Solving linear equation fast}
Solving this linear equation can be slow in general: $O(n^{3})$; but as in the unconstrained case, if you can exploit some special structure in the LHS matrix - maybe to find approximate search direction, search direction can be found fast : see unconstrained case for details.

\subsubsection{Line search maintains feasibility}
$\change $x$ \in null(A)$, so $t\change x$ found during line search remains feasible. \exclaim{Feasibility always maintained!}

\subsubsection{Affine invariant stopping criterion}
Use $\gl(x) = (\change x^{T} \gradient^{2}f(x) \change x)^{1/2} = (-\gradient f(x)^{T}\change x)^{1/2}$. Use last term to compare with stopping criterion for unconstrained minimization case. In general, it is not same as $\change x^{T} \gradient^{2}f(x) \change x$.

Justification for use of $\gl(x)^{2}/2$ as stopping criterion: Just as in the unconstrained case, $f(x) - p^{*} \approx f(x) - \inf_{Ax = b}\hat{f} = 2^{-1}\gl(x)^{2}$; and $-\gl(x)^{2}$ is also the directional derivative along $\change x$.

\subsubsection{Analysis: Use Newton method on equiv unconstrained problem}
Take $\min f(x): Ax = b \equiv \min f(Hv + x')$ form. $x^{(t)} = Hv^{(t)}+x'$. So, analysis of convergence of unconstrained problem applies here too!

\subsubsection{Using infeasible start}
\paragraph*{Primal, dual variable update view}
Take residue $r(x, m) = (\gradient f(x) + A^{T}m, Ax-b)$, solve the vector optimization problem: $\min_{x, m} r(x, m)$: so, minimizing the objective while maintaining constraint, but also getting close to feasibility. Try to get $r(y + \change y) \approx r(y) + Dr(y)\change y = 0$. So, use search direction from solving: \\
$\mat{\gradient^{2}f(x) & A^{T}\\ A & 0} \mat{\change $x$ \\ w} = -\mat{\change f(x) \\ Ax - b}$, with $w = $m$ + \change m$, the new guess for the dual variable.

Compare with search direction in feasible start case, note the change in last element of RHS.

\exclaim{Not a descent alg: $f(x^{+}) \leq f(x)$ possible!}

\paragraph*{Line search}
Backtracking line search is conducted on $\norm{r(y)}$. Stopping condition for line search: $\norm{r(y + \change y)} \leq (1 - \ga t)\norm{r(y)}$, for a certain $\ga$. So, stopping criterion becomes stricter as $t$ increases.

\paragraph*{Stopping criterion}
Ax = b and $\norm{r(y)} \leq \eps$.

\paragraph*{Switch to feasible start alg}
As soon as you get $x^{(k)}: Ax^{(k)} = b$, switch to feasible start version of the algorithm: often ensures faster descent.

\chapter{Inequality constrained problems}
\section{Barrier methods}
\subsection{Make inequality constraints implicit}
Consider constraint $f_i(x) \leq 0$. Can make this constraint implicit by including barrier functional $-\gf(x)$ in the objective $f_0'(x)$: now, the constraint is enforced by thus constraining the $dom(f_0(x))$.

\subsubsection{Motivation using indicator fn}
The best log barrier to use is actually an indicatior function $\gf(x) = I_{f_i(x)\leq 0}$ which is 0 if $f_i(x)\leq 0$, $\infty$ otherwise.

\subsubsection{Logarithmic barriers}
As $t \to \infty$, get $-t^{-1}\sum_i \log -f_i(x) \approx I_{f_i(x)\leq 0}$. So, $\gf(x) = \sum_i \log f_i(x)$. This is a good barrier functional, as it is convex, twice continuously differentiable.



\subsection{Optimize both distance to log barrier and f0}
Take problem in standard form. Solve the vector optimization problem: \\
$\min (f_0(x), -\gf(x)) : Ax = b$, where $\gf(x) = \sum \log(-f_i(x))$ is the log barrier, which is convex from composition rules.

\subsubsection{Tradeoff: minimizing f0 and repulsion from barrier}
Scalarize this to get the objective $\min g(t) = \min $t$ f_0(x) - \gf(x)$: aka the centering problem. This is an equality constrained convex optimization problem. Bigger $t$ tends to favor minimizing $f_0(x)$, while allowing $x$ to get closer to the barrier $f(x) = 0$.

For $t$ large enough, letting $x$ get closer to the barrier does not affect the minimization of $f_0(x)$ : maybe minimum is acheived at a point where some constraints are inactive.

\subsubsection{Barrier algorithm}
Start with some t. Solve the centering problem specified by t. Grow $t$ by $\gm \approx 20$. Repeat until stopping criterion is satisfied.

\subsubsection{Interpretation using Lagrangian}
Take the optimality condition : \\
$\gradient f_0(x^{*}) +t^{-1}\sum_i (-f_i(x^{*}))^{-1}\gradient f_i(x^{*}) + t^{-1}A^{T}m^{*}$.

This can be seen as the minimum of a Lagrangian-like function: $L(x, l, m) = f_0(x) + \sum_i l_i f_i(x) +  m^{T}Ax$, with $l \geq 0$. The optimal values are: \\
$l_i^{*} =  t^{-1}(-f_i(x^{*}))^{-1} \geq 0, A^{T}m^{*} = 0$.

So, $p^{*} \geq g(l^{*}(t), m^{*}(t)) = L(x^{*}(t),l^{*}(t), m^{*}(t)) = f_0(x^{*}(t)) - m/t$. m/t is the duality gap, goes to 0 as $t \to \infty$. So, $x^{*}(t) \to x^{*}$ as $t \to \infty$. \exclaim{Can use m/t as stopping criterion!}

\subsubsection{Primal, dual points on the central path}
$(x^{*}(t))$ are primal points on the central path. $(l^{*}(t), m^{*}(t))$ are dual points on the central path. 

\chapter{Ideas for faster solution}
To reduce the time taken to solve the convex optimization problem, you reduce : a] the time taken per iteration b] the number of iterations by using a clever initialization point.

\section{Solving using the dual function}
Take primal $\min f_{0}(x): \set{f_{i}(x) \leq 0}, \set{h_{i}(x) = 0} $; Get Lagrangian: $L(x, l, m)$; get $g(x) = \inf_x L(x, l, m)$; Solve $\max_{l, m} g(l, m)$; derive $x^{*}$ from $l^{*}, m^{*}$.

Make extra inferences using KKT conditions.

\section{Warm start}
Can use the solution of a closely related optimization problem to solve the current problem. This idea is used in barrier method!

Sometimes this gives a better solution as using a bad initialization point would have returned a relatively worse solution due to the number of iterations exceeeding the limit specified in the maxiter parameter passed to the solver.


\part{Classes solved with convex programming}
\chapter{Quasiconvex optimization problem}
$f_0$ is quasi-convex, $f_i(x)$ are convex. Epigraph form: $\min t: f_0(x) \leq t; f(x) \leq 0, Ax = b$.

This can have locally optimal points which are not globally optimal: there can be plateaus: from properties of quasiconvex fn.

\section{Generality of constraints}
Can replace all quasi-convex sublevel sets, which are convex sets, with sublevel sets of convex functions. Eg: Consider the case of the linear fractional programs.

\section{Solution using bisection}
Take the epigraph form, fix t. Can solve this using bisection (see another section), with each optimization problem being solved using convex programming.

\chapter{Second order cone program (SOCP)}
Minimize a linear functional : $f^{T}x$ subject to Fx = g and second order cone constraints.

Eg: used in robust linear programming.

\section{Second order cone constraints}
$\norm{A_i x - b_i}_2 -(c_i^{T} x + d_i)\leq 0: A_i \in R^{n_i \times n}$. This is obviously a convex constraint as the LHS is a sum of a convex function and a linear function. But squaring both sides is not a good way of showing convexity of the feasible set.

$(A_i $x$ - b_i, c_i^{T} $x$ + d_i)$ distinguishes part of a second order cone in $R^{n_i + 1}$.

\section{Generality}
Generalizes LP:  Take $n_i = 0$.

When $c_i = 0$, becomes QCQP. Does it generalize all QCQP? \chk

\chapter{Conic inequalities convex program}
Consider proper cones $K_i$. Then, let objective $f_0$ remain convex, have equality constraints $Ax = b$, but specify constraints using generalized convex functions: $f_i(x) \preceq_{K_i} 0$. This is still a convex program as suvblevelsets of the above continue to be convex.

Strong duality holds if Slater's constraint qualification holds.

\section{Conic form problem}
Extends LP using conic inequalities: $Fx + g \preceq_K 0$.

\section{Semidefinite programming (SDP)}
Minimize linear fn $c^{T}x$ subject to linear matrix inequalities (LMI) involving symmetric matrices (see matrix algebra survey). Can collapse n LMI's into a single LMI $A_0 + \sum x_iA_i \preceq 0$ by increasing the matrix size n times.

LMI's define convex sets. Can also be viewed as a convex optimization problem specified using conic inequalities.

\subsection{Generality}
SOCP, which includes LP, can be reduced to SDP. Can replace second order cone constraint with $\mat{(c_i^{T}x + d_i) & A_ix + b_i\\
(A_ix + b_i)^{T} & (c_i^{T}x + d_i)} \succeq 0$. \why

So, can flexibly specify SDP with semidefinite cone, quadratic cone, linear inequalities' constraints; so called SQLP.

\subsection{Recognizing SDP's}
First, can try manipulating objective to be a linear functional, perhaps by taking the epigraph form. Then, $A \preceq 0$ with $A_{i,j} = a_{ij}^{T}x + b_{ij}$ form of LMI's are common. Also, Schur's complement is often useful in forming the constraint.

\subsection{Examples}
ew minimization, matrix norm minimization.


\subsection{Dual SDP}
Lagrangian $L(x, Z) = c^{T}x + \dprod{A_0 + \sum x_iA_i, Z} = c^{T}x + \dprod{A_0, Z} + \sum x_i\dprod{A_i, Z}$ with $Z \succeq 0$.

$g(Z) = \inf_x L(x, Z) = \dprod{A_0, Z}$ if $\sum x_i\dprod{A_i, Z} + c_ix_i = 0 \forall x$, else $g(Z) = -\infty$. So, dual problem: $\max \dprod{A_0, Z}: Z \succeq 0, \dprod{A_i, Z} + c_i = 0 \forall i$. (Note: used self-duality of $S_{++}^{n}$ in writing $Z \succeq 0$.) \exclaim{Also an SDP!}

Dual of dual is the primal!: Use $L(Z, m, L) = \dprod{G, Z} + \sum_i m_i  \dprod{F_i, Z} + \dprod{L, -Z} + c^{T}m$.

\chapter{Geometric programming}
$\min_x f_{0}(x): \set{f_{i}(x) \leq 0}, \set{h_{i}(x) = 0} $, with all $f_i$ posynomials and $h_i$ monomials (see vector functionals survey).

Conversion to convex form: use $y_i = \log x_i$ and restate problem.

Eg: used in finding perron-frobenius ew: $|\ew_{max}(A)|$.

\part{Non convex optimization}
\chapter{Discrete optimization problems}
\section{Difficulty}
The difficulty here arises from the fact that a huge (often exponentially large in the number of variables due to combinatorial explosion) number of assignments to the discrete variables should be considered in order to find the optimum.

\section{General Strategies}
Use exhaustive search.

\subsection{Relaxation to allow continuous values}
Constraints in an Integer program can be relaxed to allow variables to take on real values.

\section{Graph based problems}
Max flow, min cut problem. See graph theory survey.

\subsection{Resource allocation}
Often modelled with graphs. Edges indicate resource constraints or conflicts.

\subsubsection{Maximum weight matching}
Find the heaviest set of disjoint edges.

For bipartite graphs: if $\exists$ a unique matching, loopy belief propogation will find it.

\chapter{Continuous variables: general strategies}
The main difficulty here arises from the presence of a large number of local optima.

Or use local optimization attempts with random choices of initial points.

Or use a relaxation. 

\section{Convexification/ smoothing}
Turn it into a convex optimization problem: eg change objective fn to $e^{x}$, or maybe strong duality holds: so can solve dual problem: see section on modelling/ formulating problems too.

\subsection{Dual of the dual}
Dual of the dual is sometimes a convex relaxation to the original problem, besides being a lower bound to it.

\subsection{Local approximation}
Or approximate $f_0$ (maybe locally) by a convex function or atleast a smoother function. Eg: Trust region methods.

\subsection{Smoothing}
Smoothing reduces irregularity of the function output - ie it reduces the depth of local minima.

Gaussian smoothing is frequently used: $g(x_0) = \int_{x \in [-\infty, \infty] } e^{-\gl (x-x_0)^{2}} f(x) dx$.

\section{As sampling}
Global optimization can be seen as sampling from the feasible set, using anything monotonic with $f_0(w) = E(w)$ as a measure of energy/ improbability - albeit with the intention of finding the minimum. 

In exploring the feasible set with special attention towards finding an optimum, one would want most updates to improve the objective, while the remaining updates help get out of local minima.

\subsection{Stochastic gradient descent}
Often the objective function can be decomposed as follows: $E(w) = \sum_{i=1:n} E_i(w)$ - for example, in terms of various data-points in the case of maximum likelihood esitation.

Here, for each iteration, one chooses a data-point $E_i(w)$ at random or in a sequence, and uses $-\gradient E_i(w)$ as a descent direction. 

\subsubsection{Comparison with stochastic gradient descent}
Unlike gradient descent, which, using $\gradient E(w)$ as a direction, can consistently reduce E(w) if the step size is appropriately chosen, stochastic gradient descent does not make such a guarantee. The advantage of stochastic gradient descent is in not being stuck at local optima, and in the greater speed with which $\gradient E_i(w)$ can be evaluated.

\subsection{Damping jitters}
Sampling techniques may sometimes result in excessive oscillations in the value of $E(w)$ after each variable update: $w_i = w_{i-1} + \change w_{i-1}$, where $\change w_{i-1} = \ga t_{i-1}$ using step size $\ga$ and direction $t_{i-1}$.

One may use a momentum parameter to dampen the abrupt changes in direction: $\change w_{i-1} =  \change w_{i-2} + \ga t_{i-1}$.

This is beneficial because in exploring the feasible set with special attention towards finding an optimum, one would want most updates to improve the objective, while the remaining updates help get out of local minima.

\subsection{Using distribution sampling techniques}
See section on sampling from a distribution in randomized algorithms survey. Note that in sampling for optimization, algorithms may pay attention towards finding optima.

\chapter{Local optimization}
\section{Result}
The result is often highly sensitive to the initial value of $x$. Also, one cannot guarantee that one will reach the minimum closest to the initial point: For example, when gradient descent is used, the angle of the gradient may lead one to a point which then leads a different well.

\section{Techniques}
Can use any convex optimization technique, like gradient descent or alternating minimization.

\part{Discrete and Combinatorial optimization}
\chapter{Integer programming (IP)}
LP problem where variables can only take integer valued solutions. It is NP hard to find a solution: you have combinatorial hardness.

Approximate with LP; solve it; round the solutions. \tbc

\section{Randomized rounding}
Round $x$ to $\floor{x}$ with prob $x - \floor{x}$.

\chapter{Optimal substructure problems}
Aka Dynamic programming.

\section{Applicability: Decision tree view}
The problem can be cast as one of taking a sequence of decisions, and one wants to find the optimal sequence of decisions. So, essentially, one tries to find the optimal path through a decision tree. The number of decisions one needs to take is bounded by $N$.

Problems exhibit the 'optimal substructure' property, and also often the 'overlapping subproblems' property.

\subsection{Optimal substructure}
Optimal solutions of simpler subproblems can be compared in some way to find the overall optimal solution.

A problem corresponds to a decision tree $D_l$ at level $l$. Each subproblem corresponds to finding optimal path $p_i$ through a different decision subtree $D_i$ one would arrive at by fixing the first decision to be $e_i$. One constructs the optimal decision path $p= \min_i f(p_i + e_i)$.

Eg: In case of shortest path problem: $d(s, e) = \min_{v \in \nbd(s)}[d(s, v) + d(v, e)]$.

\subsubsection{Remembering subproblems used}
$p$ is a sequence of decisions $d_{l, i}$ - each corresponding to making decision $i$ at level $l$ of the decision tree. One needs to remember the decision taken at level $l$ - the optimal subpath augmented. Eg: in the example above, in order to reconstruct the shortest path from s to e, one needs to remember which $v \in \nbd(s)$ was used. 

\subsection{Overlapping subproblems}
The subproblems solved are repeated. This corresponds to the case where decision sub-paths to various leaves are actually identical. So it is profitable to remember solutions to subproblems.

\section{Top down vs Bottom up}
\subsection{Top down solution}
A top down solution can be easily expressed in terms of a recursive function $f(D)$ which acts on a certain decision tree and returns a] the optimal decision path $p$ and b] its cost.

In doing this, if the 'overlapping subproblem' property holds, the algorithm memoizes : ie remembers optimal solutions to these subproblems whenever they are solved.

\subsection{Bottom up}
This solution is only applicable when the 'overlapping subproblem' property holds.

The algorithm solves decision trees of the smallest depth, records their results and builds solutions to progressively larger decision trees. So, one goes from level $N$ and works one's way up to level $1$.

\subsubsection{Tabular view}
Suppose that any node in the decision tree has at most $N$ children. This process can be viewed by means of a $N \times M$ table or a list of $N$ lists.

First, one constructs a list or column corresponding to the consequences of $M$ different decisions at level $N$.

Then, one constructs a list corresponding to the consequences of $M$ decisions at level $N-1$, and also a list of 'backpointers' specifying the ideal decision at level $N$ if one were to fix decision $d$ at level $N-1$.

One does this unductively until one covers all decisions up to level $1$.

\subsection{Time complexity}
From description of the bottom up solution, it is clear that time/ space required is $M*N$ - unlike $M^N$ in case all paths in the entire decision tree are to be considered (true in case 'overlapping subproblems' property does not hold).

\section{Examples}
Shortest path algorithm can be formulated as dynamic program - see graph theory survey.

FFT: See functional analysis ref.

Determining the most likely state sequence in the case of a HMM.

\chapter{Branch and bound}
Systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded en masse, by using upper and lower estimated bounds of the quantity being optimized.

\chapter{With belief propogation}
Rewrite as a problem of finding the mode of a distribution: $\max_x Pr(x): Pr(x) \propto 1_{f(x) \leq 0, h(x) = 0} e^{f_0(x)}$: the exponentiation is to ensure non-negativity.

This is useful when $f_0, $f$, h$ are decomposable into functionals over cliques: then can take advantage of factorization.

Used in combinatorial optimization.

% \bibliographystyle{plain}
% \bibliography{optimization}

\end{document}
