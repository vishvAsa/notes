<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Vishvas&#39;s notes  | 1 Quadratic</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.72.0" />
    
    
      <META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
    

    <link href="/storage/emulated/0/notesData/notes/computing/optimization/4_convex_programming/1_quadratic/" rel="alternate" type="application/rss+xml" title="Vishvas&#39;s notes" />
    <link href="/storage/emulated/0/notesData/notes/computing/optimization/4_convex_programming/1_quadratic/" rel="feed" type="application/rss+xml" title="Vishvas&#39;s notes" />

    <meta property="og:title" content="1 Quadratic" />
<meta property="og:description" content="For ideas about its importance - both in solving general convex optimization problems and in other domains, look at the &lsquo;Least squares&rsquo; chapter.
Linear Constrained QP Aka Quadratic programming. Minimize convex quadratic functional over a polyhedron (see vector spaces survey). Generalizes linear programming, special case of QCQP.
Visualization If optimal value occurs when a constraint is active: Think of contours of the objective hitting an edge of the polyhedron. Slightly harder than in the LP case, where the minimum always occured in a vertex." />
<meta property="og:type" content="article" />
<meta property="og:url" content="file:///storage/emulated/0/notesData/notes/computing/optimization/4_convex_programming/1_quadratic/" />

<meta itemprop="name" content="1 Quadratic">
<meta itemprop="description" content="For ideas about its importance - both in solving general convex optimization problems and in other domains, look at the &lsquo;Least squares&rsquo; chapter.
Linear Constrained QP Aka Quadratic programming. Minimize convex quadratic functional over a polyhedron (see vector spaces survey). Generalizes linear programming, special case of QCQP.
Visualization If optimal value occurs when a constraint is active: Think of contours of the objective hitting an edge of the polyhedron. Slightly harder than in the LP case, where the minimum always occured in a vertex.">

<meta itemprop="wordCount" content="1756">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="1 Quadratic"/>
<meta name="twitter:description" content="For ideas about its importance - both in solving general convex optimization problems and in other domains, look at the &lsquo;Least squares&rsquo; chapter.
Linear Constrained QP Aka Quadratic programming. Minimize convex quadratic functional over a polyhedron (see vector spaces survey). Generalizes linear programming, special case of QCQP.
Visualization If optimal value occurs when a constraint is active: Think of contours of the objective hitting an edge of the polyhedron. Slightly harder than in the LP case, where the minimum always occured in a vertex."/>

      
    

    <script src="/storage/emulated/0/notesData/notes/webpack_dist/dir_tree-bundle.js"></script>
    <script type="text/javascript">
    
    let baseURL = "file:\/\/\/storage\/emulated\/0\/notesData\/notes";
    let basePath = "/" + baseURL.split("/").slice(3).join("/");
    let siteParams = JSON.parse("{\u0022contactlink\u0022:\u0022https:\/\/github.com\/vvasuki\/notes\/issues\/new\u0022,\u0022disqusshortcode\u0022:\u0022vvasuki-site\u0022,\u0022env\u0022:\u0022production\u0022,\u0022githubeditmepathbase\u0022:\u0022https:\/\/github.com\/vvasuki\/notes\/edit\/master\/content\/\u0022,\u0022mainSections\u0022:[\u0022math\u0022],\u0022mainsections\u0022:[\u0022math\u0022]}");
    let pageDefaultsList = JSON.parse("[{\u0022scope\u0022:{\u0022pathPrefix\u0022:\u0022\u0022},\u0022values\u0022:{\u0022comments\u0022:true,\u0022layout\u0022:\u0022page\u0022,\u0022search\u0022:true,\u0022sidebar\u0022:\u0022home_sidebar\u0022}}]");
    let sidebarsData = JSON.parse("{\u0022home_sidebar\u0022:{\u0022contents\u0022:[{\u0022url\u0022:\u0022recdir:\/\/\u0022},{\u0022contents\u0022:[{\u0022title\u0022:\u0022‡§ú‡•ç‡§Ø‡•å‡§§‡§ø‡§∑‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/jyotiSham\/\u0022},{\u0022title\u0022:\u0022‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/sanskrit\/\u0022},{\u0022title\u0022:\u0022‡§Æ‡•Ä‡§Æ‡§æ‡§Ç‡§∏‡§æ\u0022,\u0022url\u0022:\u0022..\/mImAMsA\/\u0022},{\u0022title\u0022:\u0022Notes Home\u0022,\u0022url\u0022:\u0022..\/notes\/\u0022},{\u0022title\u0022:\u0022‡§ï‡§æ‡§µ‡•ç‡§Ø‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/kAvya\/\u0022},{\u0022title\u0022:\u0022‡§∏‡§Ç‡§∏‡•ç‡§ï‡§æ‡§∞‡§æ‡§É\u0022,\u0022url\u0022:\u0022\/..\/saMskAra\/\u0022},{\u0022title\u0022:\u0022Vishvas\u0027s home page\u0022,\u0022url\u0022:\u0022\/..\/\u0022}],\u0022title\u0022:\u0022‡§∏‡§ô‡•ç‡§ó‡•ç‡§∞‡§π‡§æ‡§®‡•ç‡§§‡§∞‡§Æ‡•ç\u0022},{\u0022title\u0022:\u0022\\u003ci class=\\\u0022fas fa-hand-holding-heart\\\u0022\\u003e\\u003c\/i\\u003eDonate Via Vishvas\u0022,\u0022url\u0022:\u0022https:\/\/vvasuki.github.io\/interests\/dharma-via-vishvas\/\u0022}],\u0022title\u0022:\u0022Parts\u0022}}");
    let autocompletePageUrl = "\/storage\/emulated\/0\/notesData\/notes\/data\/pages.tsv";
    
    var pageRelUrlTree = {};
</script>

    <script>
    
    let pageVars = {};
    pageVars.pageUrlMinusBasePath = "\/storage\/emulated\/0\/notesData\/notes\/computing\/optimization\/4_convex_programming\/1_quadratic\/".replace(basePath, "/");
    pageVars.pageParams = {};
    pageVars.pageSource = "computing\/optimization\/4_convex_programming\/1_quadratic.md";
    console.log(pageVars.pageSource);
    var pageDefaults;
    for (let possiblePageDefaults of pageDefaultsList) {
      if (pageVars.pageSource.startsWith(possiblePageDefaults.scope.pathPrefix)) {
        pageDefaults = possiblePageDefaults.values
      }
    }
    
    </script>
    <script src="/storage/emulated/0/notesData/notes/webpack_dist/main-bundle.js"></script>
    <script src="/storage/emulated/0/notesData/notes/webpack_dist/transliteration-bundle.js"></script>
    <script src="/storage/emulated/0/notesData/notes/non_webpack_js/disqus.js"></script>
    <script src="/storage/emulated/0/notesData/notes/webpack_dist/ui_lib-bundle.js"></script>
    <link rel="stylesheet" href="/storage/emulated/0/notesData/notes/css/fonts.css">
    <link rel="stylesheet" href="/storage/emulated/0/notesData/notes/css/@fortawesome/fontawesome-free/css/solid.min.css">
    

    <link rel="stylesheet" href="/storage/emulated/0/notesData/notes/css/@fortawesome/fontawesome-free/css/fontawesome.min.css">

    
    <link rel="alternate" hreflang="sa-Deva" href="#ZgotmplZ" />
    <link rel="alternate" hreflang="sa-Deva" href="#ZgotmplZ?transliteration_target=devanagari" />
    <link rel="alternate" hreflang="sa-Knda" href="#ZgotmplZ?transliteration_target=kannada" />
    <link rel="alternate" hreflang="sa-Mlym" href="#ZgotmplZ?transliteration_target=malayalam" />
    <link rel="alternate" hreflang="sa-Telu" href="#ZgotmplZ?transliteration_target=telugu" />
    <link rel="alternate" hreflang="sa-Taml-t-sa-Taml-m0-superscript" href="#ZgotmplZ?transliteration_target=tamil_superscripted" />
    <link rel="alternate" hreflang="sa-Taml" href="#ZgotmplZ?transliteration_target=tamil" />
    <link rel="alternate" hreflang="sa-Gran" href="#ZgotmplZ?transliteration_target=grantha" />
    <link rel="alternate" hreflang="sa-Gujr" href="#ZgotmplZ?transliteration_target=gujarati" />
    <link rel="alternate" hreflang="sa-Orya" href="#ZgotmplZ?transliteration_target=oriya" />
    <link rel="alternate" hreflang="sa-Beng-t-sa-Beng-m0-assamese" href="#ZgotmplZ?transliteration_target=assamese" />
    <link rel="alternate" hreflang="sa-Beng" href="#ZgotmplZ?transliteration_target=bengali" />
    <link rel="alternate" hreflang="sa-Guru" href="#ZgotmplZ?transliteration_target=gurmukhi" />
    <link rel="alternate" hreflang="sa-Cyrl" href="#ZgotmplZ?transliteration_target=cyrillic" />
    <link rel="alternate" hreflang="sa-Sinh" href="#ZgotmplZ?transliteration_target=sinhala" />
    <link rel="alternate" hreflang="sa-Shar" href="#ZgotmplZ?transliteration_target=sharada" />
    <link rel="alternate" hreflang="sa-Brah" href="#ZgotmplZ?transliteration_target=brahmi" />
    <link rel="alternate" hreflang="sa-Modi" href="#ZgotmplZ?transliteration_target=modi" />
    <link rel="alternate" hreflang="sa-Tirh" href="#ZgotmplZ?transliteration_target=tirhuta_maithili" />
    <link rel="alternate" hreflang="sa-Latn-t-sa-Zyyy-m0-iast" href="#ZgotmplZ?transliteration_target=iast" />
  </head>

  <body class="ma0 bg-near-white">
    


    
<header class="p-1 bg-yellow">
    <nav role="navigation">
      <div id="div_top_bar" class="row">
        <div class="col-md-3 justify-content-start">
          <a  href="/storage/emulated/0/notesData/notes/" class="btn col border">
            <i class="fas fa-gopuram ">Vishvas&#39;s notes <br/> 1 Quadratic</i>
          </a>
        </div>
        <div id="div_top_bar_right" class="col-md-auto d-flex justify-content-center">
          <form action="/storage/emulated/0/notesData/notes/search">
            <input id="titleSearchInputBox" placeholder="‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ø‡§ï‡§æ‡§®‡•ç‡§µ‡§ø‡§∑‡•ç‡§Ø‡§§‡§æ‡§Æ‡•ç" name="s"><i class="btn fas fa-search "></i>
          </form>
          <a href="/storage/emulated/0/notesData/notes/custom_site_search/"  class="btn btn-default">Full search</a>
          <select name="transliterationDropdown" size="1" onchange="module_transliteration.updateTransliteration()">
            <option value="devanagari" selected="">‡§∏</option>
            <option value="iast">ƒÅ</option>
            <option value="kannada">‡≤Ö</option>
            <option value="malayalam">‡¥Ö</option>
            <option value="telugu">‡∞ï</option>
            <option value="tamil_superscripted">‡Æï¬≤</option>
            <option value="tamil_extended">‡Æï</option>
            <option value="grantha">ëåÖ</option>
            <option value="gujarati">‡™Ö</option>
            <option value="oriya">‡¨Ö</option>
            <option value="assamese">‡¶Ö‡¶∏</option>
            <option value="bengali">‡¶Ö</option>
            <option value="gurmukhi">‡®Ö</option>
            <option value="cyrillic">–ø—É</option>
            <option value="sinhala">‡∂Ö</option>
            <option value="sharada">ëÜëëáÄëÜ∞</option>
            <option value="brahmi">ëÄÖ</option>
            <option value="modi">ëò¶ëòªëòöëò≤</option>
            <option value="tirhuta_maithili">ëíÅ</option>
          </select>
        </div>
        <div class="row col  d-flex justify-content-center">
          <div><a  name="previousPage" href="" class="btn btn-secondary">###<i class="fas fa-caret-left"></i></a></div>
          <div ><a name="nextPage" href="" class="btn btn-secondary"><i class="fas fa-caret-right"></i> ### </a></div>
        </div>
        <ul id="top-bar-right-custom" class="list-group list-group-horizontal">
        </ul>
      </div>
    </nav>
</header>

    
    <div class="row" name="contentRow">
      
      <aside class="col-md-3 card border " id="sidebar">
        <div id="sidebarTitle" class="card-title bg-light-gray border d-flex justify-content-between" >
          <a name="sidebarToggleLink" data-toggle="collapse" href="#sidebar_body" role="button" aria-expanded="true" aria-controls="sidebar_body" onclick="module_main.default.sidebarToggleHandler()">
            Menu <i class="fas fa-caret-down"></i></a>
        </div>
        <nav class="card-body p-0 collapse show" id="sidebar_body">
          <ul id="displayed_sidebar" class="list pl2 p-2 bg-yellow">
        </ul>
        </nav>
      </aside>
      <main class="col p-3" role="main">
        
<header class='border d-flex justify-content-between'>
    <h1 id="1 Quadratic">1 Quadratic</h1>
    
    <a id="editLink" class="btn btn-primary"  href="https://github.com/vvasuki/notes/edit/master/content/computing/optimization/4_convex_programming/1_quadratic.md"><i class="fas fa-edit"></i></a>
    
</header>
<article>
  <aside id="toc_card" class="card border ">
    <div id="toc_header" class="card-title border d-flex justify-content-between">
        <a data-toggle="collapse" href="#toc_body" role="button" aria-expanded="true" aria-controls="toc_body">
          What's in this page? <i class="fas fa-caret-down"></i> </a>
    </div>
    <div id="toc_body" class="card-body collapse p-0">
      
      <ul id="toc_ul" class="list p-0">
      </ul>
    </div>
  </aside>
  <div id="post_content">
  <p>For ideas about its importance - both in solving general convex optimization problems and in other domains, look at the &lsquo;Least squares&rsquo; chapter.</p>
<h2 id="linear-constrained-qp">Linear Constrained QP</h2>
<p>Aka Quadratic programming. Minimize convex quadratic functional over a polyhedron (see vector spaces survey). Generalizes linear programming, special case of QCQP.</p>
<h3 id="visualization">Visualization</h3>
<p>If optimal value occurs when a constraint is active: Think of contours of the objective hitting an edge of the polyhedron. Slightly harder than in the LP case, where the minimum always occured in a vertex.</p>
<h3 id="dual-problem">Dual problem</h3>
<p>This is also a quadratic program; strong duality always holds! \chk</p>
<h2 id="quadratically-constrained-qp-qcqp">Quadratically constrained QP (QCQP)</h2>
<p>Minimize convex quadratic functional, subject to convex quadratic constraints: so all equality constraints are specified by linear inequalities. Feasible set is an intersection of hyperellipes and a polyhedron.</p>
<h2 id="least-squared-error-problem">Least squared error problem</h2>
<h3 id="problem">Problem</h3>
<p>Aka least squares.</p>
<h4 id="unregularized-problem">Unregularized problem</h4>
<p>\(\min \norm{Aw - b}^{2}\), \(A \in R^{m \times n}, m&gt;n, rank(A) = m\).</p>
<h5 id="for-the-weighted-least-squares">For the weighted least squares</h5>
<p>More weight to one observation or correlated observations:\ Use Weighted inner product to find projection.\<br>
\(WAx = Wb; \min \norm{WAx - Wb} = \min \norm{Ax-b}_{M}\) for \(M=W^{*}W\).</p>
<h5 id="solution">Solution</h5>
<p>See numerical analysis survey for geometry and solution.</p>
<h4 id="the-regularized-problem">The regularized problem</h4>
<p>Aka weight decay.</p>
<p>Want to balance love of sparsity or smallness with desire for a good fit. Pick a regularizer which is small for good w, and large for bad w.</p>
<p>Take \(\min f(w) + l\norm{w}<em>n\). As n decreases, feasible set decreases: consider progression of 1 balls from \(\norm{.}</em>\infty\) to \(\norm{}_0\).</p>
<h3 id="application">Application</h3>
<h4 id="convex-optimization">Convex Optimization</h4>
<p>A common technique of solving convex optimization problems is to  approximate the objective with a quadratic function and then solve it.</p>
<h4 id="other-domains">Other domains</h4>
<p>See regression problem in statistics survey for motivation. Same as finding maximum likelihood solution while fitting linear model with Gaussian noise.</p>
<p>Interpolation of functions linear in parameters is also a specific case: see numerical analysis survey.</p>
<h4 id="regularized-problems">Regularized problems</h4>
<p>If solving a regression problem, this amounts to the least squares solution, when a prior probability distribution is imposed on w to favor smaller w values.</p>
<p>\(w\) could be the coefficients of polynomial used to fit data in \(A\), \(b\). To avoid overfitting, we want \(w\) to be small.</p>
<h5 id="penalties-and-priors">Penalties and priors</h5>
<p>Different regularizers (usually norms) correspond to different priors on w. Shape of the prior distribution looks like the unit spheres corresponding to the norms used: sphere, diamond etc..</p>
<p>\(\norm{}<em>{q}\) penalty for \(q&gt; 2\) usually not worth the effort. Sometimes, \(\norm{}</em>{1.x}\) penalty used as compromize between lasso and quadratic regularizers.</p>
<h3 id="standard-formulations">Standard formulations</h3>
<h4 id="normalizing-columns-of-a">Normalizing columns of A</h4>
<p>By solving \(A&rsquo;w&rsquo; = b\), can solve \(ADD^{-1}w \approx b\) using diagonal matrix D.</p>
<h3 id="quadratic-regularizer">Quadratic regularizer</h3>
<p>Aka ridge regression. \<br>
\(\min f(w) + lw^{T}I&rsquo;w= \min \norm{Aw-b}_{2}^{2}+ lw^{T}I&rsquo;w\) instead, to control size of w too.</p>
<p>This objective is the lagrangian fn corresponding to the problem of finding \(\min f(w) = \min \norm{Aw-b}_{2}^{2}\) subject to \(w^{T}I&rsquo;w &lt; c\).</p>
<h4 id="the-solution-form">The solution form</h4>
<p>\((A^{T}A + lI)\hat{w} = A^{T}b\): by setting \(\gradient (f(w) + l\norm{w}<em>{2}^{2})= 0\). Take \(A = U\SW V^{*}\), get \(X\hat{w} = U \SW (\SW^{2} + lI)^{-1} \SW U^{T}y = UDU^{T}y\), where \(D</em>{i,i} = \sw_{i}&rsquo; = \frac{\sw_{i}^{2}}{\sw_{i}^{2} + l}\).</p>
<p>So, \(X\hat{w} = \sum \sw_{i}&rsquo; u_{i}^{T} u_{i}y\); observe action of \(l \in [0,\infty]\) in \(\sw_{i}'\): the shrinkage parameter; or the prior distribution on w.</p>
<h4 id="the-geometry">The geometry</h4>
<p>Consider hypersphere \(\norm{w}_{2}^{2} &lt; c\) and the paraboloid \(f(w)\). The objective is to find w within the hypersphere, having the minimum \(f(w)\) value. Visualize by taking contour map of \(f(w)\) : successively larger ellipsoids. optimal \(w\) is where the minimal contour touches the hypersphere, when the center of \(f(w)\) lies outside the hypersphere.</p>
<p>Compare with geometry of \(\min f(w)\) problem: see linear alg survey.</p>
<h4 id="effect">Effect</h4>
<p>Shrinks components of \(w\) towards each other in magnitude. Also takes care of correlated features.</p>
<p>Does not penalize small \(w_i\), so does not lead to sparsity.</p>
<h3 id="l2-and-linf-regularizers">l2 and linf regularizers</h3>
<h4 id="l2-regularizer">l2 regularizer</h4>
<h5 id="problem-1">Problem</h5>
<p>Solve \(\min_w \norm{Aw - b}^{2} + \gl \norm{w}_2\), with \(\gl &gt; 0\). An alternate formulation is \(\min_w w^{T}Hw + v^{T}w + \gl \norm{w}_2\), with \(H \succeq 0\).</p>
<h5 id="solution-1">Solution</h5>
<p>The obtimality condition involves taking the subdifferential of \(\norm{w}_2\). For diagonal \(H\), this has a closed form solution. The solution technique involves some nifty variable substitution.</p>
<h5 id="relation-with-quadratic-regularizer">Relation with Quadratic regularizer</h5>
<p>Rewriting as \(\min_w \norm{Aw - b}^{2} : \norm{w}_2 &lt; C(\gl)\); we see that this is equivalent to using the quadratic regularizer and solving: \(\min_w \norm{Aw - b}^{2} : \norm{w}_2^{2} &lt; C(\gl)^{2}\). So, the tradeoff curve, is for practical purposes, identical.</p>
<p>The latter formulation has a simpler solution as subgradient calculations are not involved in specifying the optimality conditions for analytically finding a solution.</p>
<p>But, sometimes we are given the problem where an \(l_2\) regularizer is used. Such problems occur in doing block coordinate descent to solve multi-task lasso, for example, while finding the search direction using a quadratic approximation of the objective. In such cases, we can either solve the specified problem directly, or we can reformulate it using the quadratic regularizer; the former is quite simple.</p>
<h4 id="linf-regularizer">lInf regularizer</h4>
<p>Here, we solve \(\min_w \norm{Aw - b}^{2} + \gl \norm{w}<em>\infty\). Optimality condition involves taking the subdifferential of \(\norm{w}</em>\infty\). A closed form solution can be derived by using proof by cases: eg: see a paper on muti-task learning by Han Liu.</p>
<h3 id="forward-stepwise-regression-for-sparsity">Forward stepwise regression for sparsity</h3>
<p>Trying to solve Aw = y. Take F = active set of features used in approximating y. Start with \(F = \nullSet\), w = 0. At step t, find residue \(r = y - Aw_t\), find feature i most correlated with r; set \(w_{i}\) to this projection; continue. \exclaim{Observe how the residue changes at each step!}</p>
<p>This is a greedy algorithm, so suboptimal. But note that this is not the same as simply finding the optimal w&rsquo;&rsquo; without any sparsity constraint, and then dropping some small elements.</p>
<h4 id="forward-stagewise-regression">Forward stagewise regression</h4>
<p>A non greedy modification to forward stepwise. Find feature set B with maximum correlation with residue; for each such i: keep increasing \(w_{B}\) until you find another feature(s) with equal correlation with the residue; then add these feature(s) to B; repeat.</p>
<p>At any time, B is the set of features which form the \exclaim{least angle} with the residue. The coefficients increase such that \(Aw_B\) increases exactly along the projection of the residue on the hyperspace spanned by features in B.</p>
<p>Also, in case at some time, a feature correlated with other some feature in B gets added to B, the coefficient of B.</p>
<h4 id="least-angle-regression">Least Angle Regression</h4>
<p>A modification of the Forward stagewise regression, so that \(w_{B}\) is increased at one shot, rather than gradually.</p>
<h4 id="lasso-solving-modification">Lasso solving modification</h4>
<p>Losso just specifies an objective, which may be achieved in many ways, including using forward regression.</p>
<p>Can turn LAR to Lasso solver: Whenever \(w_{i}\) for some i in B hits 0, drop it out of B.</p>
<h5 id="reason-for-the-fix">Reason for the fix</h5>
<p>Compare conditions you get on lasso solution by setting gradient to 0 with conditions for LAR at any time: Correlation of residue with B with features in B is equal and correlation with other features is lower. They are identical as long as \(sgn(w_j) = \) sign of correlation of residue with feature j.</p>
<h3 id="0-norm-regularizer-compressed-sensing">0 norm regularizer: Compressed sensing</h3>
<h4 id="problem-scenario">Problem scenario</h4>
<p>X is short and fat, Xw = y has many solutions for w, want to find w with \(\norm{w}_0\leq s\), \(Xw \approx y\). If X were tall and thin, the solution of a similar problem is easy: solve \(\min \norm{Xw - y}^{2}\) and pick the s most important components of w.</p>
<p>Consider the equivalent problem, where we assume columns of X are normalized.</p>
<h5 id="finding-support-combinatorial-hardness">Finding support: Combinatorial hardness</h5>
<p>The difficulty comes from finding the support (non-zero coordinates) of w. Once support of w is found, it is easy to find the optimal linear combination of these components to get close to y: just solve \(\min \norm{X&rsquo;w&rsquo; = y}\), where \(m \times s\) X&rsquo; is derived from X by dropping some columns.</p>
<p>There are many possible ways to form w with \(\norm{w}_0\leq s\).</p>
<h4 id="finding-support-target-optimization-problems">Finding support: Target optimization problems</h4>
<p>Maybe want to limit w to certain number of non-zeros. Solve \(\min f(w) + l \norm{w}_{0}\), where \(f(w) = \norm{Xw - y}_2^{2}\).</p>
<p>This is same as \(\min f(w): \norm{w}_{0} \leq s\). Feasible set is not convex: consider epigraph of \(\norm{w}_0 \leq 1\) for example.</p>
<p>A stricter version of the problem is: \(\min \norm{w}_0 : Xw = y\), as this does not allow \(Xw \approx y\).</p>
<h4 id="solution-using-1-norm-minimization">Solution using 1 norm minimization</h4>
<p>Just solve the linear program \(\min \norm{w}_1 : Xw = y\).</p>
<p>Arrival at sparse solution is guaranteed when some restricted isometry and incoherence properties hold for X.</p>
<h5 id="restricted-isometry-constant-for-s">Restricted isometry constant for s</h5>
<p>\(1-\gd_{s} \leq \frac{\norm{Xw}^{2}}{\norm{w}^{2}} \leq 1+\gd_s, \forall w: \norm{w}_0 \leq s\).</p>
<h5 id="incoherence-almost-orthogonality">Incoherence/ almost orthogonality</h5>
<p>\(\abs{\dprod{Xw, Xw&rsquo;}} \leq T_{s, s&rsquo;}\) for w, w&rsquo; such that \(supp(w) \inters supp(s&rsquo;) = \emptyset, \norm{w}_0 \leq s, \norm{w&rsquo;}_0 \leq s&rsquo;\).</p>
<h3 id="1-norm-regularizer-lasso">1 norm regularizer: Lasso</h3>
<p>\(\min f(w) + l \norm{w}<em>{1} = \min \norm{Aw-b}</em>{2}^{2} + l \norm{w}_{1}\). Allegedly aka least absolute shrinkage and selection operator.</p>
<p>Same as minimizing f(w) subject to \(\norm{w}<em>{1} &lt; c\). As you increase c, you get less and less sparse solutions. When \(c\geq \norm{\hat{w}}</em>{1}\) where \(\hat{w}\) is the unregularized least squares solution, you get least squares solution.</p>
<p>Want to get optimality condition \(\gradient (f(w) + l \norm{w}_{1}) = 0\), but \(\norm{w}<em>1\) not differentiable at all points; so take \(l \norm{w}</em>{1} = l \sum_i sgn(w_i) w_i\). Assume that \(sgn(w_i&rsquo;)\) around the solution \(w&rsquo;\) is known.</p>
<p>Get conditions useful in Lasso solving modification of Least angles regression: Let \(B = {o: w_i&rsquo; \neq 0}\), the basis/ support set. \(\forall j\in B: a_{j}^{T}(y-Aw) = \gl sgn(w_j)\), \(\forall j\notin B: a_{j}^{T}(y-Aw) = \gl *sgn(w_j) \leq |\gl|\). If A&rsquo;s columns have norm 1, get geometric meaning for \(\gl\): an upper bound for correlation of the residue with any feature!</p>
<h4 id="importance">Importance</h4>
<p>Often yields sparse solutions: from geometry.</p>
<p>\(\norm{w}_q\) for \(q \in (0, 1]\) would also yield sparse solutions: see geometric interpretation for justification. These correspond to imposing the constraint \(\norm{w}_q \leq c\), which corresponds to the feasible region being non-convex. 1 norm is the least q which yields besides leading to a sparse solution, is also convex.</p>
<p>For importance of finding sparse models, see statistics survey.</p>
<h4 id="the-geometry-1">The geometry</h4>
<p>\(\norm{w}<em>{1} &lt; c\) is hyper-rhombus, f(w) is a paraboloid, whose each level set is an ellipsoid. In general, w for min f(w) will lie outside the hyper-rhombus. So, the optimal w: the point where the edge of the hyper-rhombus meets the t-level set: f(w) = t, for the smallest t. Usually this happens where 2 edges meet, or where many \(w</em>{i}\) are 0: visualize in 2D; hence sparsity.</p>
<p>Compare with ridge regression.</p>

  </div>
</article>







<aside class="card border" id="section-tree-item-">
    <div class="card-title bg-light-gray border d-flex justify-content-between">
        <a href="#ZgotmplZ">1 Quadratic </a>
        <a data-toggle="collapse" href="#section-tree-item-body-" role="button" aria-expanded="false" aria-controls="section-tree-item-body-" >‚Ä¶<i class="fas fa-caret-down" class="collapsed"></i> </a>
    </div>
    <nav id="section-tree-item-body-" class="card-body p-0 collapse">
        
        <li>draft: false</li>
        
        <li>iscjklanguage: false</li>
        
        <li>title: 1 Quadratic</li>
        
    </nav>
</aside>


      </main>
    </div>
    <footer class="bg-yellow  p-1" role="contentinfo">
  <div id="disqus_thread"></div>
  <div id="div_footer_bar" class="container-fluid d-flex justify-content-between">
    <a class="btn btn-secondary" href="https://github.com/vvasuki/notes/issues/new" >
      ‡§™‡•ç‡§∞‡§§‡§ø‡§∏‡•ç‡§™‡§®‡•ç‡§¶‡§É
    </a>
    <a class="btn btn-secondary" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" /></a>
    <div class="btn small border">
      Built on 2020 Nov 22 10:54:55 UTC. (<a href="http://google.com/search?q=10%3a54%3a55%20UTC to IST">IST</a>)
    </div>
  <ul id="footer-bar-right-custom" class="list-group list-group-horizontal">
  </ul>
  </div>
</footer>

  </body>
  <script type='text/javascript'>
    
    
    
    module_main.default.onDocumentReadyTasks();
  </script>
</html>
