<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Vishvas&#39;s notes  | Stochastic control processes</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.72.0" />
    
    
      <META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
    

    <link href="/storage/emulated/0/notesData/notes/computing/ai/Stochastic_control_processes/" rel="alternate" type="application/rss+xml" title="Vishvas&#39;s notes" />
    <link href="/storage/emulated/0/notesData/notes/computing/ai/Stochastic_control_processes/" rel="feed" type="application/rss+xml" title="Vishvas&#39;s notes" />

    <meta property="og:title" content="Stochastic control processes" />
<meta property="og:description" content="Reinforcement learning setting Interaction with the environment The agent can be in a set of states \(S\), and can perform a set of actions \(A\).
The effect of an action is given by transition probability distributions associated with each (sate, action) pair \(P_{s, a}:S \to [0, 1]\), and by rewards which may sometimes be provided by the environment.
Policy learning Agent wants to find the optimum policy \(\pi&rsquo;:S \to A\) by trial and error" />
<meta property="og:type" content="article" />
<meta property="og:url" content="file:///storage/emulated/0/notesData/notes/computing/ai/Stochastic_control_processes/" />

<meta itemprop="name" content="Stochastic control processes">
<meta itemprop="description" content="Reinforcement learning setting Interaction with the environment The agent can be in a set of states \(S\), and can perform a set of actions \(A\).
The effect of an action is given by transition probability distributions associated with each (sate, action) pair \(P_{s, a}:S \to [0, 1]\), and by rewards which may sometimes be provided by the environment.
Policy learning Agent wants to find the optimum policy \(\pi&rsquo;:S \to A\) by trial and error">

<meta itemprop="wordCount" content="1084">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Stochastic control processes"/>
<meta name="twitter:description" content="Reinforcement learning setting Interaction with the environment The agent can be in a set of states \(S\), and can perform a set of actions \(A\).
The effect of an action is given by transition probability distributions associated with each (sate, action) pair \(P_{s, a}:S \to [0, 1]\), and by rewards which may sometimes be provided by the environment.
Policy learning Agent wants to find the optimum policy \(\pi&rsquo;:S \to A\) by trial and error"/>

      
    

    <script src="/storage/emulated/0/notesData/notes/webpack_dist/dir_tree-bundle.js"></script>
    <script type="text/javascript">
    
    let baseURL = "file:\/\/\/storage\/emulated\/0\/notesData\/notes";
    let basePath = "/" + baseURL.split("/").slice(3).join("/");
    let siteParams = JSON.parse("{\u0022contactlink\u0022:\u0022https:\/\/github.com\/vvasuki\/notes\/issues\/new\u0022,\u0022disqusshortcode\u0022:\u0022vvasuki-site\u0022,\u0022env\u0022:\u0022production\u0022,\u0022githubeditmepathbase\u0022:\u0022https:\/\/github.com\/vvasuki\/notes\/edit\/master\/content\/\u0022,\u0022mainSections\u0022:[\u0022math\u0022],\u0022mainsections\u0022:[\u0022math\u0022]}");
    let pageDefaultsList = JSON.parse("[{\u0022scope\u0022:{\u0022pathPrefix\u0022:\u0022\u0022},\u0022values\u0022:{\u0022comments\u0022:true,\u0022layout\u0022:\u0022page\u0022,\u0022search\u0022:true,\u0022sidebar\u0022:\u0022home_sidebar\u0022}}]");
    let sidebarsData = JSON.parse("{\u0022home_sidebar\u0022:{\u0022contents\u0022:[{\u0022url\u0022:\u0022recdir:\/\/\u0022},{\u0022contents\u0022:[{\u0022title\u0022:\u0022‡§ú‡•ç‡§Ø‡•å‡§§‡§ø‡§∑‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/jyotiSham\/\u0022},{\u0022title\u0022:\u0022‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/sanskrit\/\u0022},{\u0022title\u0022:\u0022‡§Æ‡•Ä‡§Æ‡§æ‡§Ç‡§∏‡§æ\u0022,\u0022url\u0022:\u0022..\/mImAMsA\/\u0022},{\u0022title\u0022:\u0022Notes Home\u0022,\u0022url\u0022:\u0022..\/notes\/\u0022},{\u0022title\u0022:\u0022‡§ï‡§æ‡§µ‡•ç‡§Ø‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/kAvya\/\u0022},{\u0022title\u0022:\u0022‡§∏‡§Ç‡§∏‡•ç‡§ï‡§æ‡§∞‡§æ‡§É\u0022,\u0022url\u0022:\u0022\/..\/saMskAra\/\u0022},{\u0022title\u0022:\u0022Vishvas\u0027s home page\u0022,\u0022url\u0022:\u0022\/..\/\u0022}],\u0022title\u0022:\u0022‡§∏‡§ô‡•ç‡§ó‡•ç‡§∞‡§π‡§æ‡§®‡•ç‡§§‡§∞‡§Æ‡•ç\u0022},{\u0022title\u0022:\u0022\\u003ci class=\\\u0022fas fa-hand-holding-heart\\\u0022\\u003e\\u003c\/i\\u003eDonate Via Vishvas\u0022,\u0022url\u0022:\u0022https:\/\/vvasuki.github.io\/interests\/dharma-via-vishvas\/\u0022}],\u0022title\u0022:\u0022Parts\u0022}}");
    let autocompletePageUrl = "\/storage\/emulated\/0\/notesData\/notes\/data\/pages.tsv";
    
    var pageRelUrlTree = {};
</script>

    <script>
    
    let pageVars = {};
    pageVars.pageUrlMinusBasePath = "\/storage\/emulated\/0\/notesData\/notes\/computing\/ai\/Stochastic_control_processes\/".replace(basePath, "/");
    pageVars.pageParams = {};
    pageVars.pageSource = "computing\/ai\/Stochastic_control_processes.md";
    console.log(pageVars.pageSource);
    var pageDefaults;
    for (let possiblePageDefaults of pageDefaultsList) {
      if (pageVars.pageSource.startsWith(possiblePageDefaults.scope.pathPrefix)) {
        pageDefaults = possiblePageDefaults.values
      }
    }
    
    </script>
    <script src="/storage/emulated/0/notesData/notes/webpack_dist/main-bundle.js"></script>
    <script src="/storage/emulated/0/notesData/notes/webpack_dist/transliteration-bundle.js"></script>
    <script src="/storage/emulated/0/notesData/notes/non_webpack_js/disqus.js"></script>
    <script src="/storage/emulated/0/notesData/notes/webpack_dist/ui_lib-bundle.js"></script>
    <link rel="stylesheet" href="/storage/emulated/0/notesData/notes/css/fonts.css">
    <link rel="stylesheet" href="/storage/emulated/0/notesData/notes/css/@fortawesome/fontawesome-free/css/solid.min.css">
    

    <link rel="stylesheet" href="/storage/emulated/0/notesData/notes/css/@fortawesome/fontawesome-free/css/fontawesome.min.css">

    
    <link rel="alternate" hreflang="sa-Deva" href="#ZgotmplZ" />
    <link rel="alternate" hreflang="sa-Deva" href="#ZgotmplZ?transliteration_target=devanagari" />
    <link rel="alternate" hreflang="sa-Knda" href="#ZgotmplZ?transliteration_target=kannada" />
    <link rel="alternate" hreflang="sa-Mlym" href="#ZgotmplZ?transliteration_target=malayalam" />
    <link rel="alternate" hreflang="sa-Telu" href="#ZgotmplZ?transliteration_target=telugu" />
    <link rel="alternate" hreflang="sa-Taml-t-sa-Taml-m0-superscript" href="#ZgotmplZ?transliteration_target=tamil_superscripted" />
    <link rel="alternate" hreflang="sa-Taml" href="#ZgotmplZ?transliteration_target=tamil" />
    <link rel="alternate" hreflang="sa-Gran" href="#ZgotmplZ?transliteration_target=grantha" />
    <link rel="alternate" hreflang="sa-Gujr" href="#ZgotmplZ?transliteration_target=gujarati" />
    <link rel="alternate" hreflang="sa-Orya" href="#ZgotmplZ?transliteration_target=oriya" />
    <link rel="alternate" hreflang="sa-Beng-t-sa-Beng-m0-assamese" href="#ZgotmplZ?transliteration_target=assamese" />
    <link rel="alternate" hreflang="sa-Beng" href="#ZgotmplZ?transliteration_target=bengali" />
    <link rel="alternate" hreflang="sa-Guru" href="#ZgotmplZ?transliteration_target=gurmukhi" />
    <link rel="alternate" hreflang="sa-Cyrl" href="#ZgotmplZ?transliteration_target=cyrillic" />
    <link rel="alternate" hreflang="sa-Sinh" href="#ZgotmplZ?transliteration_target=sinhala" />
    <link rel="alternate" hreflang="sa-Shar" href="#ZgotmplZ?transliteration_target=sharada" />
    <link rel="alternate" hreflang="sa-Brah" href="#ZgotmplZ?transliteration_target=brahmi" />
    <link rel="alternate" hreflang="sa-Modi" href="#ZgotmplZ?transliteration_target=modi" />
    <link rel="alternate" hreflang="sa-Tirh" href="#ZgotmplZ?transliteration_target=tirhuta_maithili" />
    <link rel="alternate" hreflang="sa-Latn-t-sa-Zyyy-m0-iast" href="#ZgotmplZ?transliteration_target=iast" />
  </head>

  <body class="ma0 bg-near-white">
    


    
<header class="p-1 bg-yellow">
    <nav role="navigation">
      <div id="div_top_bar" class="row">
        <div class="col-md-3 justify-content-start">
          <a  href="/storage/emulated/0/notesData/notes/" class="btn col border">
            <i class="fas fa-gopuram ">Vishvas&#39;s notes <br/> Stochastic control processes</i>
          </a>
        </div>
        <div id="div_top_bar_right" class="col-md-auto d-flex justify-content-center">
          <form action="/storage/emulated/0/notesData/notes/search">
            <input id="titleSearchInputBox" placeholder="‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ø‡§ï‡§æ‡§®‡•ç‡§µ‡§ø‡§∑‡•ç‡§Ø‡§§‡§æ‡§Æ‡•ç" name="s"><i class="btn fas fa-search "></i>
          </form>
          <a href="/storage/emulated/0/notesData/notes/custom_site_search/"  class="btn btn-default">Full search</a>
          <select name="transliterationDropdown" size="1" onchange="module_transliteration.updateTransliteration()">
            <option value="devanagari" selected="">‡§∏</option>
            <option value="iast">ƒÅ</option>
            <option value="kannada">‡≤Ö</option>
            <option value="malayalam">‡¥Ö</option>
            <option value="telugu">‡∞ï</option>
            <option value="tamil_superscripted">‡Æï¬≤</option>
            <option value="tamil_extended">‡Æï</option>
            <option value="grantha">ëåÖ</option>
            <option value="gujarati">‡™Ö</option>
            <option value="oriya">‡¨Ö</option>
            <option value="assamese">‡¶Ö‡¶∏</option>
            <option value="bengali">‡¶Ö</option>
            <option value="gurmukhi">‡®Ö</option>
            <option value="cyrillic">–ø—É</option>
            <option value="sinhala">‡∂Ö</option>
            <option value="sharada">ëÜëëáÄëÜ∞</option>
            <option value="brahmi">ëÄÖ</option>
            <option value="modi">ëò¶ëòªëòöëò≤</option>
            <option value="tirhuta_maithili">ëíÅ</option>
          </select>
        </div>
        <div class="row col  d-flex justify-content-center">
          <div><a  name="previousPage" href="" class="btn btn-secondary">###<i class="fas fa-caret-left"></i></a></div>
          <div ><a name="nextPage" href="" class="btn btn-secondary"><i class="fas fa-caret-right"></i> ### </a></div>
        </div>
        <ul id="top-bar-right-custom" class="list-group list-group-horizontal">
        </ul>
      </div>
    </nav>
</header>

    
    <div class="row" name="contentRow">
      
      <aside class="col-md-3 card border " id="sidebar">
        <div id="sidebarTitle" class="card-title bg-light-gray border d-flex justify-content-between" >
          <a name="sidebarToggleLink" data-toggle="collapse" href="#sidebar_body" role="button" aria-expanded="true" aria-controls="sidebar_body" onclick="module_main.default.sidebarToggleHandler()">
            Menu <i class="fas fa-caret-down"></i></a>
        </div>
        <nav class="card-body p-0 collapse show" id="sidebar_body">
          <ul id="displayed_sidebar" class="list pl2 p-2 bg-yellow">
        </ul>
        </nav>
      </aside>
      <main class="col p-3" role="main">
        
<header class='border d-flex justify-content-between'>
    <h1 id="Stochastic control processes">Stochastic control processes</h1>
    
    <a id="editLink" class="btn btn-primary"  href="https://github.com/vvasuki/notes/edit/master/content/computing/ai/Stochastic_control_processes.md"><i class="fas fa-edit"></i></a>
    
</header>
<article>
  <aside id="toc_card" class="card border ">
    <div id="toc_header" class="card-title border d-flex justify-content-between">
        <a data-toggle="collapse" href="#toc_body" role="button" aria-expanded="true" aria-controls="toc_body">
          What's in this page? <i class="fas fa-caret-down"></i> </a>
    </div>
    <div id="toc_body" class="card-body collapse p-0">
      
      <ul id="toc_ul" class="list p-0">
      </ul>
    </div>
  </aside>
  <div id="post_content">
  <h2 id="reinforcement-learning-setting">Reinforcement learning setting</h2>
<h3 id="interaction-with-the-environment">Interaction with the environment</h3>
<p>The agent can be in a set of states \(S\), and can perform a set of actions \(A\).</p>
<p>The effect of an action is given by transition probability distributions associated with each (sate, action) pair \(P_{s, a}:S \to [0, 1]\), and by rewards which may sometimes be provided by the environment.</p>
<h3 id="policy-learning">Policy learning</h3>
<p>Agent wants to find the optimum policy \(\pi&rsquo;:S \to A\) by trial and error</p>
<h4 id="sub-problems">Sub-problems</h4>
<p>Looking at occasional reward, agent must assign credit to past actions: credit assignment problem.</p>
<p>Exploration vs exploitation trade-off: can&rsquo;t find high reward states if you try to greedily known best state.</p>
<h3 id="reward-learning">Reward learning</h3>
<p>Aka Inverse reinforcement learning problem.</p>
<p>Sometimes, given the description of the environment (states \(S\), actions \(A\), transition distributions \(P_{s, a}\)) and an agent&rsquo;s policy \(\pi\), one wants to learn the rewards which motivated the agent to arrive at the policy.</p>
<h4 id="non-triviality-criteria">Non triviality criteria</h4>
<p>A trivial solution would be a reward function which assigns equal reward to all \((a, s_1, s_2)\) triplets - this would make all policies equally desirable. So, one would seek a sort of maximum entropy solution which favors the observed policy as strongly as possible over other policies.</p>
<h4 id="motivation">Motivation</h4>
<p>In economics and biology, the precise mechanism that motivates an animal is interesting. Eg: Are waiters who tend to seat customers outside, do so in order to attract other customers? \tbc</p>
<h3 id="reinforcement-learning-in-animals">Reinforcement learning in Animals</h3>
<p>Stimulus \(\equiv\) belief state; response \(\equiv\) action. An inspiration for devoloping reinforcement learning algs.</p>
<h4 id="conditioning">Conditioning</h4>
<p>Eg: Pavlov&rsquo;s dog; behaviorist Skinner training dog to jump against wall in 20 minutes. 1st order vs higher order conditioning. Cats escaping a box to get to fish.</p>
<p>Acquisition due to reinforcement during various trials; Extinction due to removal/ change in R(q): Visualize with a salivation level vs number of trials graph. Spaced trials better for acquisition than massed trials: more time for animal to make correlations.</p>
<p>Habituation: Plateau in response level.</p>
<p>Extinction burst: When you stop reward, animal temporarily tries much harder.</p>
<p>Avoidance: Animal avoids certain states/ stimuli after -ve reward, thereby looses chance to sample/ explore the state further.</p>
<p>Reinforcement schedules: Fixed reward: performance ratio, fixed interval (not good: animal learns interval) etc..</p>
<p>Conditioning due to different reinforcers (food, water etc..).</p>
<h3 id="learning-by-simulation">Learning by simulation</h3>
<p>Suppose you wanted to create an intelligent automaton - eg: pilot program for a drone plane. It is very hard to think of all possible cases - a pilot&rsquo;s knowledge is often intuitive and trained by the environment. On the other hand, it could be simpler to specify and thus simulate the environment. Thus, one can train the automaton in the simulated environment.</p>
<h2 id="markov-decision-process-mdp">Markov decision process (MDP)</h2>
<h3 id="abstract-problem">Abstract problem</h3>
<p>We consider the reinforcement learning problem, but we consider an immediate reward function \(R: A \times S \times S \to \Re\). So the reward is fixed for each \((a, s_1, s_2)\) - this can be interpreted as expected reward (which may be determined by an agent from past experience).</p>
<h4 id="limited-dependence-on-history">Limited dependence on history</h4>
<p>State transition shows the Markovian property: dependence only on the previous state and action.</p>
<h4 id="representation-notation">Representation, notation</h4>
<p>For a Diagrammatic representation and summary of alternative terms/ symbols used, see Wiki.</p>
<p>Essentially, one considers the state transition graph of a Markov chain whose states are given by \(S \union A\), where any path between two state-nodes always needs to pass through an action-node (and similarly for paths between action nodes).</p>
<p>The extension is that edges \((a, s) \in A \times S\) may be labelled also with a reward value, apart from the transition probability. Edges \((s, a) \in S \times A\) are labelled with transition probabilities according to the policy being depicted.</p>
<h3 id="policy">Policy</h3>
<p>A policy is a map \(p: S \to A\).</p>
<h4 id="long-term-reward">Long term reward</h4>
<p>One can compare policies by somehow aggregating rewards one would expect over \(k\) (aka horizon) time-steps. This is better than a simplistic evaluation of a policy which only considers the immediate expected reward. The horizon considered could either be finite or infinite.</p>
<p>It is logical for the weights corresponding to distant times to be smaller.</p>
<h4 id="weighted-sum-state-values">Weighted sum: state values</h4>
<p>One common way of aggregating reward over \(k\) time-steps while assigning lower weights to future rewards is to take a weighted sum of rewards accrued during this period. Often geometrically decreasing weights - powers \(g^k\) of the discounting factor \(g \in (0, 1)\) are used.</p>
<p>This expected reward for applying policy \(p\) over horizon \(k\) starting from state \(s_0\), aka value of the state \(s_0\), is \(V_p(s_0) = \sum_{t \in \set{0..k}} g^t E_P[R(p(s_t), s_t, s_{t+1})]\). This is proportional to the geometrically weighted moving average, and hence equivalent to using it where comparing policies is concerned.</p>
<h4 id="best-policy">Best policy</h4>
<p>One ideally wants to find a policy which maximizes the value of the current state.</p>
<p>For a finite state MDP with an infinite horizon where rewards are aggregated using geometrically decreasing weights, one only needs to solve the following linear program to find the policy vector \(p\):</p>
<p>$$\argmax_{p(s)} \sum_{s&rsquo;} P(p(s), s, s&rsquo;)(R(p(s), s, s&rsquo;) + gV(s&rsquo;))$$ subject to $$V(s) = \sum_{s&rsquo;} P(p(s), s, s&rsquo;)(R(p(s), s, s&rsquo;) + gV(s&rsquo;))$$ (which is aka the Bellman equation).</p>
<h4 id="solution-techniques">Solution techniques</h4>
<p>Besides linear programming, one can use various iterative dynamic programming techniques.</p>
<p>The policy iteration starts with an arbitrary vectors \(p\). It then does the following repeatedly until convergence: a] compute corresponding value vector \(V\) by repeatedly computing \(V_p(s_0) = \sum_{t \in \set{0..k}} g^t E_P[R(p(s_t), s_t, s_{t+1})]\) until convergence, b] improve \(p\) using current estimate of \(V\). As each step improves the solution, and because there is only one minimum, this is guaranteed to converge.</p>
<h3 id="dealing-with-large-state-spaces">Dealing with large state spaces</h3>
<p>Dealing with large state-spaces. One can deal with large state spaces by collapsing similar state together. Eg: In the case where the state space corresponds  to the 3-dimensional coordinates of an aircraft relative to a target, one can use an alternative state space defined instead by distance to the target.</p>
<p>One can do Q-learning: There is no need to specify explicitly the transition probability P or list the states. This is advantageous when the state space is huge.</p>
<h2 id="partially-observable-markov-decision-process-pomdp">Partially observable Markov decision process (POMDP)</h2>
<h3 id="problem-setting">Problem setting</h3>
<p>As in case of MDP&rsquo;s, we have sets of states \(S\), actions \(A\) and transition probabilities \(P\).</p>
<h4 id="observations">Observations</h4>
<p>But, the states are not necessarily fully observable. So, we have a set of observations \(O\), and observation probabilities \(P_O\).</p>
<h4 id="simplified-rewards">Simplified rewards</h4>
<p>The reward function is simpler than in MDP (possibly at the expense of a bigger state space): \(R: A \times S \to \Re\).</p>
<h3 id="belief-states">Belief states</h3>
<p>\tbc</p>

  </div>
</article>







<aside class="card border" id="section-tree-item-">
    <div class="card-title bg-light-gray border d-flex justify-content-between">
        <a href="#ZgotmplZ">Stochastic control processes </a>
        <a data-toggle="collapse" href="#section-tree-item-body-" role="button" aria-expanded="false" aria-controls="section-tree-item-body-" >‚Ä¶<i class="fas fa-caret-down" class="collapsed"></i> </a>
    </div>
    <nav id="section-tree-item-body-" class="card-body p-0 collapse">
        
        <li>draft: false</li>
        
        <li>iscjklanguage: false</li>
        
        <li>title: Stochastic control processes</li>
        
    </nav>
</aside>


      </main>
    </div>
    <footer class="bg-yellow  p-1" role="contentinfo">
  <div id="disqus_thread"></div>
  <div id="div_footer_bar" class="container-fluid d-flex justify-content-between">
    <a class="btn btn-secondary" href="https://github.com/vvasuki/notes/issues/new" >
      ‡§™‡•ç‡§∞‡§§‡§ø‡§∏‡•ç‡§™‡§®‡•ç‡§¶‡§É
    </a>
    <a class="btn btn-secondary" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" /></a>
    <div class="btn small border">
      Built on 2020 Nov 22 10:54:55 UTC. (<a href="http://google.com/search?q=10%3a54%3a55%20UTC to IST">IST</a>)
    </div>
  <ul id="footer-bar-right-custom" class="list-group list-group-horizontal">
  </ul>
  </div>
</footer>

  </body>
  <script type='text/javascript'>
    
    
    
    module_main.default.onDocumentReadyTasks();
  </script>
</html>
