\documentclass{beamer}
\usepackage{graphicx, color}

\input{../../../macros}
\input{../../../presentationMacros}

\usetheme{Warsaw}

\title{Presenting: `Cryptographic primitives based on hard learning problems: Blum, Furst, Kearns, Lipton'}
\author{vishvAs vAsuki}
\date{\today}

\begin{document}

\AtBeginSection[]
{
   \begin{frame}
       \frametitle{Outline}
       \tableofcontents[currentsection]
   \end{frame}
}

\AtBeginSubsection[]
{
   \begin{frame}
       \frametitle{Outline}
       \tableofcontents[currentsection,currentsubsection]
   \end{frame}
}

\AtBeginSubsubsection[]
{
   \begin{frame}
       \frametitle{Outline}
       \tableofcontents[currentsection,currentsubsection, currentsubsubsection]
   \end{frame}
}

\frame{\titlepage}

\section{Outline}

\begin{frame}
\frametitle{What to look out for?}
\begin{itemize}
\pitem A \red{new definition} for hardness of learning.
\pitem A \red{pseudorandom bit generator} using hard to learn class of functions.
\end{itemize}
\end{frame}


\section{Introduction to Learning Theory}
\begin{frame}
\frametitle{The binary classification problem}
\begin{itemize}
 \pitem Teach the computer difference between male face and female face by giving examples.\\
\includegraphics[scale=0.25]{images/classifiedFaces.jpg}
 \pitem Ask computer: Is \includegraphics[scale=0.125]{images/maleFace.jpg} a female face?
 \pitem Computer wins if it succeeds with good probability.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The binary classification problem formalized}
\begin{itemize}
 \pitem Known set of n features. Eg: Hairstyle, facial hair, moustache present.
\includegraphics[scale=0.15]{images/classifiedFaces.jpg}
 \pitem Unknown classification function (concept) $c: \set{0,1}^{n} \to \set{M,F}$. Eg: \includegraphics[scale=0.15]{images/concepts.jpg}
 \pitem c belongs to a known set of functions, $C_{n}$. Eg: Polynomial sized DNF over n variables, Halfspaces etc.. 
\pitem See m(n) examples: $\set{(s_{1}, c(s_{1})), (s_{2}, c(s_{2})) .. } = (S, c(S))$.
 \pitem Now, classify test set: $\set{s'_{1}, s'_{2}, ..}$. \includegraphics[scale=0.065]{images/maleFace.jpg}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Some notation}
\begin{itemize}
\pitem $C_{n}$: a set of classifiction functions $\set{0, 1}^{n} \to \set{0, 1}$. Their ensemble $C = \set{C_{n}}$.
\pitem $D_{n}$: A distribution over inputs: $\set{0,1}^{n}$. Their ensemble: $D = \set{D_{i}}$.\\
\includegraphics[scale=0.15]{images/classifiedFaces.jpg}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{L learns a set of functions C wrt D}
\begin{itemize}
\pitem Fix some $c \in C_{n}$. \includegraphics[scale=0.15]{images/concepts.jpg}
\pitem Pick $S \distr D_{n}^{m(n)}$. \includegraphics[scale=0.15]{images/classifiedFaces.jpg}
\pitem Alg L, upon studying (S, c(S)) in time $poly(n, \frac{1}{\eps})$, classifies new examples with $\leq \eps$ error rate.
\pitem $$Pr_{S \distr D_{n}^{m(n)}, x \distr D_{n}}(L(S, c(S), x) = c(x)) \geq 1 - \eps$$ in time $poly(n, \frac{1}{\eps})$.
\includegraphics[scale=0.065]{images/maleFace.jpg}
\pitem L learns C if you can say this $\forall c \in C_{n}, D_{n}, \forall n$.
\pitem Else hard to learn.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{About computational learning theory}
\begin{itemize}
\pitem Can you learn it in polynomial time?
\pitem Is it hard to learn it in polynomial time?
\end{itemize}
\end{frame}

\section{Hardness of learning as a cryptographic assumption}
\subsection{What do we really mean?}
\begin{frame}
\frametitle{Some notation}
\begin{itemize}
\pitem $C_{n}$: a set of classifiction functions $\set{0, 1}^{n} \to \set{0, 1}$.
\pitem $P_{n}$: A distribution over $C_{n}$. Their ensemble: $P = \set{P_{n}}$. \includegraphics[scale=0.15]{images/concepts.jpg}
\pitem $D_{n}$: A distribution over inputs: $\set{0,1}^{n}$. Their ensemble: $D = \set{D_{i}}$.\\
\includegraphics[scale=0.15]{images/classifiedFaces.jpg}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Hardness of learning as a cryptographic assumption}
\begin{itemize}
\pitem Want 'C is hard to learn' to be a cryptographic assumption.
\pitem Any alg L should learn C only with negligible probability.
\pitem Take $(P_{n}, D_{n})$. Pick classifier c using $P_{n}$. Pick many examples using $D_{n}$. Your alg cannot match c(x) with non negligible probability.
\includegraphics[scale=0.065]{images/maleFace.jpg}
\end{itemize}
\end{frame}

\subsection{A problem in hardness definition}
\begin{frame}
\frametitle{Problem in hardness definition}
\begin{itemize}
\pitem Even if you have efficient alg L to learn all but a tiny scattered subset of C, it is no good. C is 'hard to learn'.\\
\includegraphics[scale=0.25]{images/hardToLearnDefnProb.jpg}
\pitem If 'Learning C is hard' were a cryptographic assumption, any proof of security built on this assumption would be \textbf{worthless}. L is strong enough to break this assumption, by cryptographic standards.
\end{itemize}
\end{frame}

\subsection{A new definition for hardness of learning}
\begin{frame}
\frametitle{Learning a concept class C wrt $(P, D)$}
\begin{itemize}
\pitem So, weaken hardness of learning defn or strengthen learnability defn.
\pitem Pick $D_{n} \in D, P_{n} \in P, c \in P_{n}$.\\ \includegraphics[scale=0.15]{images/concepts.jpg}.......................
\includegraphics[scale=0.15]{images/classifiedFaces.jpg}
\pitem Alg L, upon studying S, c(S) in time $poly(n, \frac{1}{\eps})$, classifies new examples with $\leq \eps$ error rate.
\pitem $$Pr_{S \distr D_{n}^{m(n)}, x \distr D_{n}, \red{c \in P_{n}}}(L(S, c(S), x) = c(x)) \geq 1 - \eps$$.
\includegraphics[scale=0.065]{images/maleFace.jpg}
\pitem L learns C if you can say this $\forall P_{n}, D_{n}$.
\pitem Else hard to learn.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The new defintion, pictorially}
'Hard to learn': \includegraphics[scale=0.25]{images/hardToLearnDefnNew.jpg}\\
'Learnable': \includegraphics[scale=0.25]{images/hardToLearnDefnProb.jpg}
\end{frame}

\section{Pseudorandom generator from Hard to learn set of functions}
\begin{frame}
\frametitle{Pseudorandom bit generator (PRBG)}
\begin{itemize}
\pitem $G_{n}$ takes n bits input, makes $g(n)>n$ bits output.
\pitem Any polynomial time alg T does not behave noticably differently on $y \distr U(\set{0, 1}^{g(n)})$ and $g(x)| x \distr U(\set{0, 1}^{n})$.
\pitem $G = \set{G_{n}}$.
\pitem Now, construct a PRBG.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Construct PRBG from hard to learn $P_{n}$ over $C_{n}$}
\includegraphics[scale=0.3]{images/prbg.jpg}
\begin{itemize}
\pitem Proof by contradiction: If you could break this PRBG, $C_{n}$ not hard to learn wrt $(P_{n}, U(\set{0, 1}^{n})$.
\item 1110011100111100001011001... \textit{'Can I predict the next bit?'}
\end{itemize}
\end{frame}

% \begin{frame}
% \frametitle{}
% \begin{itemize}
% \pitem 
% \end{itemize}
% \end{frame}

\section{Conclusion}
\begin{frame}
\frametitle{Some other results in the paper: Skip}
\begin{itemize}
\pitem Things you can make from hard to learn set of functions:
One way functions, A private key cryptosystem.
\pitem A pseudo random generator based on hardness of learning parity functions in the presence of noise.
\pitem They take more pains to relate the circuit size and depth required to evaluate functions in hard to learn $C_{n}$ with the circuit depth and size of the primitives generated.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The take home message}
\includegraphics[scale=.25]{images/sun.jpg}
\begin{itemize}
\pitem Can use hardness of learning, properly defined, as a cryptographic assumption.
\pitem Can generically make pseudorandom bit generator from hard to learn but easy to evaluate classes of functions.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Bye!}
\includegraphics[scale=.5]{images/sun.jpg}
\includegraphics[scale=0.25]{images/classifiedFaces.jpg}
\includegraphics[scale=0.065]{images/maleFace.jpg}
\end{frame}

% \bibliographystyle{plain}
% \bibliography{../}


\end{document}
