\documentclass[oneside, article]{memoir}

\input{../packages}
\input{../packagesMemoir}
\input{../macros}

%opening
\title{Information and Coding Theory: Quick reference}
\author{vishvAs vAsuki}

\begin{document}
\maketitle

\part{Introduction}
\chapter{Notation}
Hamming distance d(x,y).

\chapter{Themes}
Designing efficient and reliable data transmission, for compression and error correction. Suitability of codes for particular purposes.

For Cryptography, see cryptography ref.

\chapter{Information}
\section{Self Information of an event}
Aka surprisal. Measure of information content associated with event e: rarer the event, more the info, and in case of independence $\perp(e, f): h(e, f) = h(e)+h(f)$. In the latter case, Pr(e,f) = Pr(e)Pr(f); thence get derivation: $h(e) = h(X=x) = \log (\frac{1}{Pr(e)})$.

\subsection{As code-length for recording event}
\subsubsection{Coding problem}
Suppose that we wanted to record information that an event occurred, but we wanted to use as few bits in expectation as possible. We want to satisfy this: the more common the event, fewer the bits one would need to transmit the event's occurrence.

\subsubsection{Coding algorithm}
We observe that there can be at most $1/p$ events with probability $p$. So, assigning $\ceil{\log (\frac{1}{Pr(e)})}$ bits to communicate the occurrence of an event ensures that we have a way of encoding all possible events, while using fewer bits to encode commoner events.

This is a code with the least expected code-length, as shown in the entropy section.

\subsection{Unit}
Inspired by the code-length interpretation of surprisal. Depending on whether $\log_{2}$ or $\ln$ is used in definition: bits or nats.

\section{Entropy of an RV X}
\subsection{Definition}
\subsubsection{Desired properties}
Uncertainty associated with an RV: Should not change if probability rearranged for different values of $X$: symmetry; should increase with number of values $X$ can take; if $X \perp Y$, uncertainty of $(X, Y)$ should be sum of uncertainties.

\subsubsection{As expected surprisal}
$H(X) = E[h(X)] = E_{X}[-log(Pr(X=x))] \\
= - \sum Pr(X=x_{i}) \log (Pr(X=x_{i}))$; is the only measure which satisfies this \why.

\subsubsection{Extension to 0 values}
Extend definition for $Pr(X=x_{i}) = 0$: \\
$lt_{Pr(X=x_{i}) \to 0} Pr(X=x_{i}) \log (Pr(X=x_{i})) = 0$, so set $Pr(X=x_{i}) \log (Pr(X=x_{i})) = 0$: so expansibility property: No change in entropy due to adding 0 probability events $X = x_{i}$.

\subsection{Expected Information/ code-length}
Entropy of $X$ is the average amount of information/ surprisal communicated by the corresponding random process.

It is the least expected number of bits required to transmit the value of the random process. \proof: Non negativity of Information divergence.

\subsubsection{Cross entropy}
Even though $X$ may have distribution $D$, an alternative code appropriate for random variable corresponding to distribution $E$ can potentially be used to encode events $X=x$. But, the expected code length is higher if this is done. This inspires a way of measuring divergence between distributions - Information (KL) Divergence/ Code-length divergence $KL(E||D)$. This is described in probability theory survey.

\subsection{As cross entropy relative to U}
$H(X) = \log |ran(X)|$ if $X \distr U$. $KL(X||U) = \log |ran(X)| - H(X)$; but $KL(X,U) \geq 0$, so $U$ has max entropy, reduction in entropy is $KL(X,U)$.

Non uniform distribution has less entropy than uniform distribution. Can use this to reduce the number of bits needed to transmit information.

\subsection{Concavity in case of discrete distribution p}
$H(p) = \sum_i p_i \log(1/p_i)$: concave in $p_i$ as $\gradient^{2} H(p) \succeq 0$. Consider RV X $\distr$ bernoulli(p): entropy cup shaped, with max at p=0.5.

\subsection{Asymptotic equipartition property (AEP)}
Take binary distribution with entropy H, iid sample $\set{X_{i}}$, get sequence $(X_{i})$. Then, sequences will either have probability $2^{-nH}$, or $\approx 0$. So, need only nH bits, rather than n bits. Pf: Set $Y_{i} = \log \frac{1}{Pr(X_{i})}$; By law of large numbers $n^{-1}\sum Y_{i} \to H$; so $-Pr((X_{i}) = (x_{i})) \to nH$.

\section{Joint and cross entropy}
\subsection{Joint entropy}
$H(X,Y) = E_{x,y}[-log(Pr(X=x, Y=y))]$.

Additivity, as requried: If $X \perp Y: H(X,Y) = H(X) + H(Y)$; subadditivity: $H(X,Y) \leq H(X) + H(Y)$.

\subsection{Cross entropy}
$H_{C}(X,Y) = E_{x}[-log(Pr(Y=y))]$: avg bits required to transmit X using protocol designed for $Y$. Compare with information divergence: that is the number of extra bits required to transmit X using a protocol designed for $Y$.

\section{Conditional entropy of X given Y}
$H(X|Y) = E_{y}[H(X|Y=y)] = E_{y}[E_{x}[-log(Pr(X=x|Y=y))]] = H(X,Y) - H(Y)$: Aka equivocation; Avg uncertainty in $X$, after seeing $Y$.

\section{Mutual information of X wrt Y}
$I(X;Y) = E_{x,y}\log[\frac{Pr(X=x,Y=y)}{Pr(X=x)Pr(Y=y)}] = H(X) - H(X|Y) = H(X) + H(Y) - H(X,Y)$ - visualize with a venn diagram!: reduction in uncertainty about X due to knowledge of $Y$. It is symmetric.

This is the expected value of the information gain / code-length divergence: $E_x[H(Y) - H(Y|X=x)]$; and is therefore loosely called information gain when considered in the context of classification problems in machine learning.

\subsection{As deviation from independent distribution}
$I(X;Y) = K(Pr(X=x,Y=y)||Pr(X=x)Pr(Y=y))$; so $I(X;Y) \neq 0$ iff $X \perp Y$. So, it is non negative.

\subsection{Conditional Mutual information wrt Z}
$I(X;Y|Z) = E_{z}[I(X;Y|Z=z)]$.

\section{Other information metrics}
Hamming weight of x: wt(x). Hamming distance: $d(x,y) = wt(x \xor y)$.



\section{Communication complexity}
\subsection{The problem}
A talks to B; A knows a; B knows b; want to find f(a, b) with min communication and even $\infty$ local computation. a, b are n bit numbers.

Easy solution is to send a and b. But these may be large. So want to use some protocol depending on f.

\subsection{Applications}
VLSI, scenarios where communication is very costly.

\subsection{The communication protocol tree}
$A \leftrightarrow B$ communication can be represented as this: A and B take turns sending messages, the message sent at step i is $m_{i} = f_{i}(a, b)$. Maybe distriubution M over (a, b) specified and want to minimize expected communication, maybe want min worst case communication.

So, can look at all possible communication sequences using a protocol tree.

\subsection{Deterministic vs randomized protocols}
Bits transmitted by deterministic protocol, for worst possible (a, b) $\dfn D(f)$. If distribution M specified: $D_{M}(f)$: avg bits used.

Randomized protocols may use public randomness or private random bits. Bits used by them for worst (a, b) $\dfn R(f)$. Randomized protocols much more powerful than deterministic ones: See equality testing example.

Having public random bits is not much more powerful: you can replace public random bit using protocol with private random bit using protocol with only $+ \log n$ bits penalty.

\subsection{Computing f for k input pairs}
Want to do better than $k D(f)$ from trivial algorithm. Deterministic protocol: $\Omega(k \sqrt{D(f)})$. Randomized protocol: $\tilde{\Omega}(R(f)\sqrt{k})$.

\subsection{Examples}
\subsubsection{Checking equality}
$f(a, b): b \iseq a$. Any det protocol needs n bits. So use fingerprinting (see Randomized algs ref).

A uses rand r, sends fingerprint (F(a, r), r) to B.

To show that F is good: Make $\hat{F}(a) = ((F(a, r_{1}), r_{1}), .. (F(a, r_{s}), r_{s}))$; pick rand element and send. For all $a \neq b$, show Hamming dist $\del(\hat{F}(a), \hat{F}(b))$ large.

\part{Coding}
\chapter{Fingerprinting}
This codes can also be used as error detection codes.

\section{Chinese reminder code}
Codes which use a mod p, with rand p. $\hat{F}(a)$ elements will use diff fields; so not preferred.

\subsection{Checking equality}
A picks rand prime p between 1 and $k = n^{3}$; Sends (a mod p, p) to B; B says '=' if $a \equiv b \mod p$.

$Pr_{p}(a \equiv b \mod p|a \neq b) \leq n^{-1}$: num(p with $a \equiv b mod p$ when $b \neq a$) or, $num(p | (a-b)) \leq n^{-1}$ as $a-b \in [0, 2^{n}-1]$; so $Pr(p|a-b) < \frac{n}{\Pi(n^{3})} = \leq \frac{n\ln n}{n^{3}} \leq \frac{1}{n}$ Using Prime number theorem.

\section{Univar polynomial code}
(Reed Solomon) Codes which make univar polynomial $p_{a}$ over $\mathbb{F}_{p}$, ($deg \leq n$), from a, prime p, with a's bits representing coefficients.

\subsection{Checking equality}
Fix p. A picks rand r from $F_{p}$, sends $(p_{a}(r), r)$ to B, B accepts if $p_{b}(r) = p_{a}(r)$. $Pr((p_{b}-p_{a})(r) = 0) \leq \frac{n}{p}$: max n roots.

\section{Multivar polynomial code}
(Reed Muller). \tbc

\chapter{Source coding}
Compression. See the example about checking equality.

\chapter{Channel Codes}
\section{Code design}
In most cases, this is an art, rather than a science. Not many things are proved; instead one runs long simulations to show goodness of a code.

\section{Modelling a channel}
Transmitted x is transmuted to y; want to model this process.

\subsection{Channel capacity}
Aka Shannon limit or capacity. The tightest upper bound on the amount of information that can be reliably transmitted over a communications channel.

\subsection{Binary symmetric channel}
Pr($x_i \neq y_i$) = p.

\subsection{Erasure channel}
$Pr(y_i  = x_i) = p$, with 1-p probability, $y_i = ?$ (erased). This can model packet loss.

\section{Model message distribution}
Usually assume uniform distribution over messages.

\section{Tolerating errors}
Design codes and protocols for error detection and correction.

\subsection{ARQ}
If error detected, ask for retransmission.

\subsection{Forward error correction}
Receiver never sends any message back to transmitter. Error correcting code (ECC) attached with data used to fix errors.

\subsection{Decoding}
Set of codes $C \set F_{2}^{n}$. x is received. Select $c \in C$ closest to x.

\paragraph*{List decoding}
Output a list of codes within a certain distance of the mangled code.

\subsection{Joint source-channel coding}
Encoding of a redundant information source for transmission over a noisy channel, and the corresponding decoding. \tbc

\section{Properties}
\subsection{Minimum Hamming Distance d}
Aka distance of the code, Hamming metric. Closely related to the error correcting ability of the code.

More efficient encoding and decoding. \why

\subsection{Code rate}
Code rate k/n. High rate code if this is high.

\section{Types}
\subsection{Block vs Convolutional codes}
Block codes: k-bit info to n-bit code. Block length n.

Convolutional code: k bit info to n bit code.

\subsection{Bound on code size of block codes}
(Gilbert-Varshamov). Take code with length n, distance d, size (not dimension) of the code $A_{q}(n, d)$. Then $A_{q}(n, d) \geq \frac{q^{n}}{\sum_{j=0}^{d-1} \binom{n}{j}(q-1)^{j}}$ \why.

The best rate vs distance tradeoff.

\subsection{w error correcting code}
A code which can correct w erroneous bits.

\paragraph*{w error correcting linear code}
Given n*m generator matrix G and m bit y, find n bit x such that $d(xG, y) \leq w$, if it exists. This is possible only if corresponding $d \geq 2w+1$.

\subsection{Cyclic code}
Right shifting a code $c \in C$ also yields a code in C.

\chapter{[n, k, d] Linear code C}
A type of block code. Block Length n, message length k, min Hamming distance d: encode k bit msg in n bit message.

d is the min Hamming wt of any non zero code vector \why.

\section{A linear subspace of valid codes}
A linear subspace C with dimension k of vector space $F_{q}^{n}$ over finite field $F_{q}$. The channel takes you away from this subspace, find the vector closest to the received message in the subspace.

All vector and scalar ops are in $F_{q}$. For Binary linear codes, use the field $F_2$.

\subsection{Basis codes}
Can be represented as span of basis codes. Basis codes form rows of k*n generating matrix G. Standard form of G: G is of the augmented matrix $[I_{k}:A]$, with k*(n-k) A. To encode x, find xG.

\subsection{Random [n, k] code}
k vectors chosen randomly from $\set{0, 1}^{n}$. Or, full rank G is chosen randomly. Achieves whp Gilbert Varshmov bound on rate vs distance tradeoff.

\section{Decoding}
Check/ parity check matrix H: n*(n-k), with left kernel C; $H = [-A^{T}:I_{n-k}]$ in std form. GH=0. To check y, verify: $yH=0$.

For corrupted y, there is an error vector e with $wt(e) \leq \frac{d-1}{2}$ such that $y\xor e = xG$ for some x. To decode, look at its syndrome: yH = (x + e)H = eH. Then solve for e or look it up in a table. Then find x.

\subsection{[p, q] regular code}
Make a bipartite graph: bits in variable x on one hand, and nodes corresponding to parity checks in H on the other. If this is a [p, q] regular graph, you have a [p, q] regular code.

\subsection{Decoding}
Avg case hardness unknown. Worst case decoding is NP hard. Even finding d is belived hard.

\subsection{As inference over factor graph}
\paragraph*{Make a factor graph}
Make nodes for the transmitted codeword bits x, and for the corresponding received/ corrupted codeword bits y. Make factors corresponding to the parity checks for y: eg: if H contains a check which says $\oplus_{i \in S} x_i$, make a factor $f_S$, such that any x where this is not satisfied has probability 0. Relationship between $x_i$ and $y_i$ can be modelled using a symmetric error: maybe $y_i$ is corrupted with probability p.

\paragraph*{The inference problem}
y is observed, x is unobserved - to be inferred. Can use loopy belief propogation for doing this.

\subparagraph*{Guarantees for [p, q] regular codes}
As the block size n increases, can be sure that loopy belief propogation properly decodes: shown using the 'density evolution' argument. Loopy belief propogation gets into trouble because of cycles; but if you consider the computation tree corresponding to a node, maybe convergence achieved well before a cyclical message is received!

\end{document}
