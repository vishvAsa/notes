\documentclass[oneside, article]{memoir}
\input{../packages}
\input{../packagesMemoir}
\input{../macros}


%opening
\title{Algebra of linear maps over vector spaces: Quick reference}
\author{vishvAs vAsuki}

\begin{document}
\maketitle

\tableofcontents

\part{Introduction}
\chapter{Themes}
\section{References}
Based on \cite{strang}, \cite{trefBau}, \cite{hornJohnson} \cite{hornJohnsonTopics}, \cite{matrixCookbook}.

\subsection{Linear algebra over field F}
Vector space over field F with all linear transformations.

\section{Themes}
Linear transformations and matrices; their properties. Systems of linear equations. Using decompotions to understand the operation of linear maps better.

\subsection{Related surveys}
For info on vectors, vector spaces, various functionals (including norms), non-linear functions: see survey of vector spaces, their functions and functionals.

For numerical analysis, conditioning, stability, differnce equations, differential equations: see Numerical analysis ref.


\section{Characterization of research effort}
See algorithms ref; Both strategies mentioned there are useful.

Experiment with Matlab.

\subsection{Matrix algorithm design}
The decompoisitional approach to matrix computations is extremely useful. Think of a computation in terms of triangular, diagonal, orthogonal etc.. decompositions. Facilitates error analysis.

\subsection{Working with algebra}
Invlolves much algebra in addition. Reasoning about matrix operations: Understand what is going on by the use of algebra; eg: write scaling rows of $A$ as DA where D is diagonal. Write special matrices algebraically. This highly clarifies things, makes them unambiguous. This is an important skill.

Use induction a lot.

Also see algebra/ mathematical structures ref.

\chapter{Notation}
Vectors are small letters. Matrices are capital letters.

\section{Matrix notation}
\subsection{Dimensions}
$A = [a_{ij}]$ is mn matrix: $\in C^{m \times n}$. $q = \min \set{m,n}$. Square A: $m \times m$. $\hat{A}$ is rectangular.

\subsection{Matrices related to A}
$(v_{1}, v_{2} \dots)$ is a column vector v.

$a_{i}$: ith element of vector a or ith column of A. $a_{i}^{*}$: by context: ith row of $A$ or transpose of $a_{i}$. $A_{k+1:m,k:f}$: a submatrix; other unambiguous matlab notations $A_{i,:}$ for ith row etc.. $a_{i,j}$ an element of A. $A_{i,j}$: by context: an element of $A$ or a submatrix of A. 

Conjugate matrix $\conj{A}$. Adjoint (Hermitian conjugate) of A: $A^{*}=\conj{A^{T}}$. $\tilde{A}$: $A$ as stored on computer; or as calculated by alg.

Dilation of matrix A: add rows and cols to A.

\subsection{Special matrices}
Permutation matrix, P. Lower triangular matrix L. Upper triangular matrix U. Diagonal matrix $D = diag(d_{i}) = diag(d)$. Orthogonal (or Orthonormal) matrix Q, $\hat{Q}$. Identity I.

\subsection{Special vectors}
ith col of I: $e_{i}$ (Canonical unit vector). e or 1: col vector of 1's. $P_{\perp q}$: projector to space $\perp q$.

\section{Abused notation}
$y = O(\eps) \implies  \exists c = f(m,n), \forall x \lim_{\eps_{M} \to 0} y \leq c\eps_{M}$. Extra Defn: If $y = \frac{a(x)}{b(x)}$, at $b(x) = 0$, $y = O(\eps)$ means $a(x) = O(\eps b(x))$.


\part{Linear operators}
\chapter{Linear operators: Matrix representation}
\section{Linear Transformation}
\subsection{Definition and Linearity}
\subsubsection{Linearity}
$A(ax+by) = aA(x)+bA(y)$. $A$ called operator, from viewing vector as functions.

\subsubsection{Mapping between vector spaces}
For any field $F$, can consider linear transformations $A: F^{n} \to F^{m}$.

\subsection{Applications, examples}
Vector spaces can model many real world things (see vector spaces ref), even functions. In all of these, linear transformations have deep meanings.

Over function spaces: Differentiation, integration, multiplication by fixed polynomial in $P_n$.

Geometric operations: $Ax$. \\
Rotation, projection, reflection, stretching, shearing.

\subsection{As a matrix}
\subsubsection{Use action on basis vectors}
Take the linear operation $A$, take standard basis vectors $\set{e_i}$ of dom(A). Take an input $x$, which, as a combination of basis vectors, is $x = \sum_i x_i e_i$. Now, by linearity, $A(x) = \sum_i x_i A(e_i)$.

\exclaim{So, $A(e_i)$ is the basis of the range space range(A)!}

\subsubsection{Matrix}
Now, write a matrix $A$, a 2-dim array of numbers such that the ith column, $a_i = A_{:,i} = A(e_i)$. This will be a $m \times n$ array. Reason for doing this: Ax now be defined to equal A(x).

\subsubsection{Matrix * vector multiplication}
Define $Ax = \sum_i a_i x_i$. \exclaim{Voila - linear operation represented by matrix vector multiplication!}

For other views of $Ax$, see a later section.

\paragraph*{Vector dot product}
\exclaim{This also defines row vector * column vector multiplication!} This is also the standard inner product.

\subsubsection{Row view: inner product with rows}
Take the standard inner product. Then, Ax is the vector formed by $(\dprod{A_{i,:}, x})$.

\subsection{Changing basis vectors}
Representation of the same geometric point can change with the choice of bases.

Take the point $x = \sum_i x_i e_i$, written according to the old standard basis $\set{e_i}$. Express $\set{e_i}$ in terms of the new basis $\set{e'_i}$:, $e_i = \sum_i a_i e'_i$; write it as the vector $u_i$. Thence, construct the matrix U. Then, $Ux = y$ is the representation of the point in terms of the new basis $\set{e'_i}$.

\subsection{Representation wrt i/p and o/p bases}
Similarly, representation of a linear transformation can also change with choice of bases.

When the right input and output basis is chosen, every matrix is actually diagonal : mere scaling: see SVD.

\section{Forward operation: Ax}
\subsection{Spaces related to range(A)}
\subsubsection{Range space}
As $A$ is linear, $range(A) = \set{Ax : \forall x \in dom(A)}$ is the vector (sub)space $\linspan{a_i}$ by definition of matrix-vector multiplication. So, aka the column space.

By linearity, $A$ maps $x = x_{r}+x_{n} \in C^{n}, x_n \in N(A), x_r \perp N(A) $ to $Ax_r \in C^{m}$.

\subsubsection{Left null space}
Also, \textbf{orthogonal complement} of $range(A)^\perp =N(A^{T})$, the Left Null space; both in $C^{m}$.

\subsection{Subspaces of the domain}
\subsubsection{Null space (Kernel)}
N(A): $\set{x: Ax = 0}$. By linearity of $A$, this is a vector space. N(A) dimension = degrees of freedom = free vars' number.

Left null space is $N(A^{T})$.

\paragraph{Find null-space}
Ax=0; Reduce $A$ to U; identify pivot and free vars; find values of pivot vars in terms of f free vars; rewrite as combination of f vectors (basis of null space). If $N(A)=C^{0}$, every b is unique combo of range(A).

Similarly, Find \textbf{left null-space} $N(A^{T})$ basis.

\subsubsection{Row space}
This is the space spanned by rows of $A$, so it is $range(A^{*})$. $S \perp N(A)$ wrt standard inner product, $range(A) + N(A) = F^{n}$.

Every x: $Ax \neq 0$ has a component in the row space.

\subsubsection{SVD view}
See elsewhere.

\subsection{Rank of A}
\subsubsection{Row and column ranks}
The number of linearly independent rows in $A$ is row rank. Similarly column rank is defined. So, column rank = dim(range(A)).

\subsubsection{Row rank = column rank}
Do triangular row elimination to get PA = LU. Then, row rank = number of non-zero rows/ pivots in A.

But, every column $a_i$ of $A$, corresponding to a 0 pivot, is a linear combination of the non-0 pivot columns: construct the matrix A' with only such columns, and solve $A'x = a_i$ using triangular row elimination. So, column rank = number of non-zero pivots.

\subsubsection{SVD view}
Take SVD: $A = U \SW V^{*}$. Rank r of $A$ corresponds to number of non 0 $\sw_i$; Can take reduced SVD: $A = U_r \SW_r V_r^{*}$. $A$ actually acts between r-dim subspaces.

\subsubsection{Invertability}
Both row space and range(A) must have dimension r. Every $x_{r}$ in $range(A^{T})$ mapped to unique $Ax_{r}$. Invertability, I. Every $Ax_{r}$ in range(A) mapped to unique $x_{r}$: Else, if $Ax_{r'} - Ax_{r}= 0$, $x_{r'} - x_{r}$ is both in row space and N(A). Also, $A \in C^{m \times n}$ ($m \geq n$) full column rank iff no $x_{r}$ mapped to same $Ax_{r}$. So, $A$ invertible (both as a function and group theoretically) iff r=m=n. So unique decomposition into basis vectors.

Else, use pseudoinverse to get left inverse if $m>n$, right inverse if $n>m$. To invert Ax=b for $n>m$, identify n-m linearly dependent columns in $A$, set corresponding $x_{i} = 0$, drop those columns to get A', solve for x' is $A'x' = b$.

\subsection{Identity operation}
$Ix = x$.

\section{Matrix multiplication: AB}
\subsection{Composition of transformations}
AB is defined to conform to transformation ABx. So, ABCx is associative.

\subsubsection{As sum of rank 1 matrices}
$u\otimes v = uv^{T}$ makes Rank 1 matrix. $AB = a_{1}b_{1}^{*} + .. + a_{p}b_{p}^{*}$, where $b_{i}^{*} = B_{i,:}$. This ensures that $ABx = \sum_i (B_{i,:}x) a_i$, as intended! \exclaim{Remarkable!}

Similarly, for diagonal matrix D: $ADB = \sum_{i} a_{i}d_{i,i}b_{i}^{*}$. So, for symmetric $S = U \SW U^{T}$, $A^{T}SB = A^{T}U\SW U^{T}B = \sum \sw_i (U^{T}a_i)^{T}(U^{T}b_i) = \sum \sw_i a_i^{T}b_i$.

\subsection{Elementwise description}
From sum of rank 1 matrices form: $(AB)_{i,j} = A_{i,:}B_{:,j}$.

\subsubsection{Columns of the result}
A acts on B's rows; B acts on A's columns. Every col of C=AB is a linear combination of A's cols according to some col of B: $c_i = Ab_i$.

\subsubsection{Form of ABC}
Consider expansion of quadratic functional (see vector spaces ref). Similarly, in D = ABC has $D_{i,j} = A_{i,:}(BC_{:,j})$.

So, $D_{i,j}$ is a linear combination of $B_{i,j}$.

\subsection{Computation}
Needs $O(n^{3})$ naively. Else needs $O(n^{2.7})$ by Strassen alg. If sparse need $(n^{2}\nu)$.

\subsection{Rank}
rank(AB) $\leq$ min (rank(A), rank(B)): Take A=LU factorization, as AB = LUB, rank(AB) $\leq$ rank(B).

\subsection{Inverse and transpose}
$(AB)^{-1}=B^{-1}A^{-1}$ (Take ABx to x.). $(AB)^{*}=B^{*}A^{*}$ from defn.

So, as $I = (AA^{-1})^{*} = (A^{-1})^{*}A^{*}$, $(A^{-1})^{*} = (A^{*})^{-1}$ (aka $A^{-*}$). Also, $(AA^{*})^{*} = AA^{*}$ and $A^{*}A$ Hermitian.

\subsection{Block matrices}
Block multiplication works.

\section{Other matrix products}
\subsection{Entrywise product}
Aka Hadamard product, Schur product. $A.A$.

\subsection{Kronecker product}
Aka outer product. $A$ $m\times n$, B $p \times q$; $C = $A$ \kron B$ is $mp \times nq$ block matrix with $C_{i,j} = A_{i,j}B$.

From defn, $\exists A, B: A \kron B \neq B \kron $A$ ; (A \kron B)^{T} = B^{T} \kron C^{T}; A \kron (B+C) = $A$ \kron B + A \kron C; aA \kron bB = ab A\kron B$. $(A \kron B)(C \kron D) = AC \kron BD$ by block multiplicity. So, $(A \kron B)^{-1} = A^{-1} \kron B^{-1}$. Also, using $QA = LDU: rank(A \kron B) = rank(A)rank(B)$.

$\ew$ vector: $\ew(A \kron B) = \ew(A)\kron \ew(B)$: take eigenpairs of $A$ and B: \\$(\ew_{i}, v_{i}), (\mu_{j}, u_{j})$; 
$(A \kron B)(v_{i} \kron u_{j}) = Av_{i} \kron Bu_{j} = \ew_{i}\mu_{j}(v_{i} \kron u_{j})$.

\section{Inverse operation: Solve Ax = b for x}
\subsection{Solvability Assumption}
$b \in range(A)$.

\subsection{Left and right inverses}
Right inverse: $I=AA^{-1}$. Similarly, left inverse is defined.

\subsubsection{Existance conditions}
The left inverse exists exactly when you can solve Ax = b for all $b\in range(A)$. Right inverse exists when you can solve $x^{T}A = b^{T}  \forall b \in range(A^{T})$.

\subsubsection{Equivalence if both exist}
If both left and right inverses exist, they're the same: $B_{l}^{-1}BB_{r}^{-1} = B_{l}^{-1} = B_{r}^{-1}$.

\subsection{Solutions}
\subsubsection{Full column rank}
If $A$ has full column rank, you have a unique solution, left inverse exists, $x = A_l^{-1}b$. Proof: Triangularization by row elimination goes through.

\subsubsection{Rank Defective matrix}
If $A$ is column-rank deficient, eg: short and fat, you have an overdetermined set of linear equations: many 'equally good' solutions exist, but you may want one with certain properties (like sparsity). See numerical analysis ref for details.

\subsubsection{Finding solutions}
See numerical analysis reference for various techniques.

\subsubsection{Finding inverses}
Find right inverse: Gauss-Jordan: Make augmented matrix A:I; use row operations to reduce it to $I:A^{-1}$. So, $\exists A^{-1}$ (right) iff r=m. Similar trick for left inverse.

Or make cofactor matrix C, use $A^{-1} = \frac{C^{T}}{det(A)}$.

\subsection{Block matrices}
\subsubsection{Block Gaussian elimination}
Take $X = \mat{A & B \\ C & D }$, solve $X \mat{x \\ y} = \mat{u\\ v}$ for $\mat{x \\ y}$, thence derive $X^{-1}$. Solving for x in the top equation and substituting it in the bottom, you will have reduced the problem to:

$\mat{I & A^{-1}B \\ 0 & S = D-CA^{-1}B } \mat{x \\ y} = \mat{A^{-1} & 0 \\ -CA^{-1} & I } \mat{u\\ v}$.

\subsubsection{Block LU}
Thence, get block LU, Aka Leibnitz factoriazation: \\
$X = \mat{A & 0 \\ C & I } * \mat{I & A^{-1}B \\ 0 & S = D-CA^{-1}B }$.

\subsubsection{Schur complement}
S is called the Schur complement of $A$ in X.

\subsubsection{Inverse of X}
Do block back substitution to get $X^{-1}$.

\subsection{Pseudoinverse for long thin A}
Projector + inverse. For rectangular, non column rank deficient matrices: $m\geq n$. Takes $b = A(x_{r}+x_{n})$ to $x_{r}$ in row space, cannot revert $Ax_{n}=0$ ($x_{n} \in N(A)$).

$A^{+} = (A^{*}A)^{-1}A^{*} = \hat{R}^{-1}\hat{Q}^{*}$ (as $A=\hat{R}\hat{Q}$) $= V\hat{\SW}^{-1}\hat{U}^{*}$ (from SVD).

\section{Restriction to a subspace S}
Let Q be the orthogonal matrix formed by an orthonormal basis of S.  Then, projection of $a_i$ in S is $QQ^{T}a_i$. So, $Ax = \sum_i x_i a_i$, when restricted to S, becomes $Ax = \sum_i x_i QQ^{T}a_i$. So, $QQ^{T}A$ is the operator $A$ restricted to the subspace S.

\section{Submatrices}
Principle submatrices: $A_{1:k, 1:k} \forall k$. For each principle submatrix, schur complement is defined: see determinant section.

\chapter{Approximating complete and incomplete matrices}
\section{Approximating a matrix}
\subsection{The problem}
Take set of matrices S. Want $\argmin_{B \in S} \norm{A - B}$ is minimized wrt some $\norm{}$ and some set S.

\subsubsection{The error metric}
Also, often $\norm{.}$ is an operator norm, as this ensures that $\norm{(A-B)x}$ is low wrt the corresponding vector norm. Other times, as in the case of matrix completion problems, it may be desirable for $\norm{}$ to be the Frobenius norm.

\subsubsection{Low rank (k) factorization problem}
In many problems, S is the set of rank k matrices, where k is small.

Often, we prefer $A \approx B = XY^{T}$ rather than computing B, where rank(X), rank(Y) $\leq k$: $A$ may be sparse, but the best B may be dense, so we may run out of memory while storing, and out of time while computing Bx. You can always get $B = XY^{T}$ from B just by taking SVD of B.

\paragraph*{Restriction to subspace view}
Similarly, may want restriction of $A$ to a low rank subspace: $B = QQ^{T}A$, Q is low rank, orthogonal; but it should be the best subspace which minimizes the error. $A$ good Q tries to approximate the range space of A.

\subsubsection{Sparse approximation}
Maybe want $\min_B \norm{B-A}_2: \norm{B}_0 \leq k$. This can be solved just by setting B = $A$, and then dropping the k smallest $B_{i, j}$ from B.

\paragraph*{1 norm regularized form}
Even the optimization problem $\min_B \norm{B-A}_2^{2} + l\norm{B(:)}_1 $ leads to sparse solutions.  This form is useful in some sparse matrix completion ideas. Solution: just take B=A, and then drom $B_{i,j} \leq l/2$: \exclaim{thresholding!}

Pf: $\min_B \sum_{i, j}(B_{i,j} - A_{i, j})^{2} + l\sum_{i, j} sgn(B_{i, j})B_{i, j} $. Let B' be the solution to this. If $A_{i, j} \geq 0$, so is $B_{i, j}$; If $A_{i, j} \leq 0$, so is $B_{i, j}$: if they oppose in sign, setting $B_{i, j} = 0$ definitely lowers the objective. So, get equivalent problem: $\min_B \sum_{i, j}(B_{i,j} - A_{i, j})^{2} + l\sum_{i, j} sgn(A_{i, j})B_{i, j}$ subject to $sgn(A) = sgn(B)$. The new objective is differentiable. Its optimality condition: $B_{i, j} = A_{i, j} - l/2$; but the feasible set only includes $sgn(B) = sgn(A)$. So if $A_{i, j} - l/2 \leq 0$, the feasible B closest to the minimum of $\min_B \sum_{i, j}(B_{i,j} - A_{i, j})^{2} + l\sum_{i, j} sgn(A_{i, j})B_{i, j}$ has the corresponding $B_{i, j} = 0$.

\subsection{Best rank t approximation of A from SVD}
\subsubsection{The approximation}
Let $A_{t} = \sum_{j=1}^{t} \SW_{j}u_{j}v_{j}^{*}$ be the approximation. $A_{t}$ is the best rank t approx to $A$ (wrt $\norm{.}_{2}$, $\norm{.}_{F}$), captures max energy of $A$ possible: $\norm{A-A_{t}} = \inf_{B} \norm{A-B}$.

\subsubsection{Approximation error}
Then, approximation error is $A-A_{t} = \sum_{i=t+1}^{r}\sw_{i}u_{i}v_{i}^{*}$. As $\norm{X} = \sw_1(X)$: $\norm{A-A_{t}}_{2} = \sw_{t+1}$, $\norm{A-A_{t}}_{F} = \sqrt{\sum_{i=t+1}^{r} \sw_{i}^{2}}$.

\subsubsection{Geometric interpretation}
Approximate hyperellipse by line, 2-dim ellipse etc... 

\paragraph*{Approximating domain and range spaces}
Using SVD for example, $A$ can be viewed as a combination of rotation and diagonal matrices. So, getting a low rank approximation of $A$ can be viewed as first getting low rank approximations of the range and domain spaces with orthogonal basis matrices $U_t$ and $V_t$ respectively, and then finding a square $S = U_t^{T}AV_t^{T}$ such that $A \approx U_t S V_t$.

\subsubsection{Proof}
Proof by $\contra$: dim(N(B))= r-t, Let $\forall w \in N(B), \norm{Aw} = \norm{(A-B)w} < \SW_{t+1}\norm{w}$; but $\exists$ t+1 subspace $\set{v: \norm{Av}\geq \SW_{t+1}}$; $dim(\set{w})+\dim(\set{v}) = r+1$: absurd.

So, $\sw_{k} = \min_{S \subset C^{n}, dim(S) = n-k+1} \max_{x \in S} \norm{Ax}_{2} \\
= \max_{S \subset C^{n}, dim(S) = k} \min_{x \in S} \norm{Ax}_{2}$.

\subsubsection{Randomized Approach}
Take the $A \approx B = QQ^{T}A$ view. $A$ good Q must span $U_k$. Can use something akin to the power method of finding ev. Take random m*k matrix W. $Y = (AA^{T})^{q}AW = U\SW^{q+1}VW$, and get $Y = QR$ to get Q. q = 4 or 5 is sufficient to get good approximation of $U_k$. (Tropp etal) if you aim to get k+p approximation, you get low expected error.
  
\subsection{With few observations in A only}
Same as the missing value estimation problem.

\section{Missing value estimation}
\subsection{The problem}
May be you have the matrix $A$, and you have observed only a few entries $O$. If there were no other conditions, there would be infinite solutions. But, maybe you also know that $A$ has some special structure: low rank, or block structure or smoothness etc..

Can also view as getting an approximation $B \in S$ for $A$, where S is the set of matrices having the specified structure.

\subsection{Smoothness constraint}
Sovle $\min \sum_{i,j} d(B_{i, j}, B_{i-1,j}) + d(B_{i, j}, B_{i,j-1})$ such that $B_{i,j} = A_{i,j} \forall (i,j) \in O$. If we use l2 or l1 metrics for d(), this can be solved with convex optimization.

\subsection{Low rank constraint}
$rank(B) \leq k$: this is not a convex constraint.

\subsubsection{Singular value projection (SVP)}
(Jain, Meka, Dhillon) Set $B^{(0)}_{i,j} = A_{i,j} \forall (i,j) \in O$, set remaining values in $B^{(0)}$ arbitrarily. Then, in iteration i, do SVD of $B^{(i)}$ to get rank k approximation $B^{(i+1)}$, set $B^{(i+1)}_{i,j} = A_{i,j} \forall (i,j) \in O$.

\paragraph*{Projection viewpoint}
Take $S_1 = \set{B:B_{i,j} = A_{i,j} \forall (i,j) \in O},\\
 S_2 = \set{B:rank(B)\leq k}$. You start with $B_0 \in S_1$, project it to the closest $C \in S_2$, project C to the closest $B_1 \in S_1$ etc..


\subsection{Applications}
The netflix problem.

\chapter{Important decompositions}
\section{Importance of decompositions}
Very important in restating and understanding the behavior of a linear operator. Also, important in solving problems: get decomposition, use it repeatedly. For algebraic manipulation: Factor the matrix: QR, LU, Eigenvalue decomposition, SVD.

\section{EW revealing decompositions}
See section on eigenvalues.

\subsection{Eigenvalue decomposition}
Aka Spectral Decomp. Only if $A$ diagonalizable.

Take $S$: Eigenvectors-as-columns matrix, with independent columns; $\EW$: Eigenvalue diagonal matrix. Then, AS = SL; So, $S^{-1}AS = L$; $A=SLS^{-1}$: a similarity transformation. Also, If AS=SL, S's columns must be eigenvectors.

A diagonalized into $\EW$. $A$ and $\EW$ are similar.

\subsubsection{Non-defectiveness connection}
$\exists S^{-1}\EW S$ iff $A$ is non defective: If $\exists S^{-1}\EW S$: $\EW$ diagonal, non defective, so $A$ non defective; if $A$ non defective: can make non singular S; thence $S^{-1}\EW S$.

\subsubsection{Left ev}
$S^{-1}AS = \EW$, so $S^{-1}A = \EW S^{-1}$. So, the rows of $S^{-1} = L$ are the left ev.

\subsubsection{Change of basis to ev}
$x = SS^{-1}x = SLx = \sum_i \dprod{x, L_{i, :}} s_i$. Thus, any x can be conveniently rewritten in terms of right ew, with magnitudes of components written in terms of left ew.

\subsubsection{Unitary (Orth) diagonalizability}
For $A=A^{*}$, $A=-A^{*}$, Q etc..

A unitarily diagonalizable iff it is normal: $A^{*}A = AA^{*}$: From uniqueness of SVD, $US^{2}U^{*} = VS^{2}V^{*}$; so, $|U| = |V|$; U and V may only differ in sign. So, for some $|\EW| = |S|, $A$ = ULU^{*}$. Aka Spectral theorem.

\subsection{\htext{$A = QUQ^{*}$}{Schur} factorization}
(Schur). $A$ and upper traingular U similar; all ew on U's diagonal. If $A=A^{*}$, $U=U^{*}$, so U is diagonal.

Every $A$ has $QUQ^{*}$: by induction: assume true for m; take any $\ew$ and corresponding eigenspace $V_{\ew} \perp V_{\ew}^{\perp}$; use orth vectors from these spaces as bases; in this basis, operator represented by $A$ has matrix representation $A'=\mat{\ew I_{\ew} & B\\ O & C} = Q^{*}AQ$; can then repeat the process for C.


\section{Singular Value Decomposition (SVD)}

\subsection{Reduced (Thin) SVD}
If $m\times n$ $A$ with $m>n$, rank r = n, unitary $n\times n$ $V = [v_{i}]$, unitary $m\times n: \hat{U}=[u_{i}], n\times n: \hat{\SW} = $ diagonal matrix with ${\sw_{i} \geq 0}$ in descending order, $AV=\hat{U}\hat{\SW}$; then $A=\hat{U}\hat{\SW}V^{*}$. If $r<n$; still get reduced SVD by padding V with $\perp$ vectors, $\hat{U}$ with appropriate $\perp$ vectors, $\SW$ with 0 diagonalled columns.

\subsection{Full SVD}
Pad $\hat{U}$ with m-r $\perp$ vectors, $\hat{\SW}$ with 0's to make U $m \times m$, invertible; V stays same; $A=U\SW V^{*}$. So, SVD for $m<n$: $A^{*}=V\SW U^{*}$. So, $A^{*}U=V\SW$. Also, $Av_{i} = u_{i}\sw_{i}$. So, $range(A)=u_{1} \dots u_{r}$, $range(A^{*})=v_{1} \dots v_{r}$, $N(A)=v_{r+1} \dots v_{m}$ ($Av_{r+1}=0$), $N(A^{*})=u_{r+1} \dots u_{n}$.

\subsection{Geometric view}
Take $Ax = U\SW V^{*}x$: $V^{*}$ rotates the unit ball to unit ball: $v_{i} \to e_{i}$, $\SW$ stretches unit ball along axes to make it hyperellipse, U rotates it. Every $A$ with SVD maps unit ball to hyperellipse (Eqn: $\sum \frac{x_{i}^{2}}{\sw_{i}^{2}} = 1$): Orthonormal $\{v_{i}\}$ (left singular vectors) mapped to orthogonal $\{u_{i}\sw_{i}\}$ (Principle semiaxes, orthonormal right singular vectors $\times$ singular values). So $\sw_{1} = \norm{A}_{2}$, $\sw_{n} = \norm{A^{-1}}_{2}$.

From geometric action of $U\SW V^{*}x$, every $A$ with SVD is a diagonal matrix when domain and range bases are altered (See $Ax=b$ as $AVx'=Ub'$, then $\SW x'=b'$). 'If ye want to understand $A$, take its SVD.'

\subsection{Existance}
Every $A$ has a SVD: by induction; prove 1*1 case; assume (m-1)*(n-1) case  $B=U_{2}\SW_{2}V_{2}^{*}$; take m*n A; $\sw_{1} = \norm{A}_{2} = \sup \|Av\|_{2}$; so $\exists v_{1}$ in the unit ball with $Av_{1} = u_{1}\sw_{1}$; So extend $v_{1}$ and $u_{1}$ to orthonormal $V_{1}$ and $U_{1}$, make $\SW_{1}$ solving $U_{1}^{*}AV_{1}=\SW_{1}$; 1st col is $\sw_{1}$; as $\norm{A}=\norm{\SW} = \sw_{1}$, non-diag elements of 1st row gotto be 0; let rest of $\sw_{1}=B$.

\subsection{Conditional uniqueness up to a sign}
SVD is unique if $\set{\sw_{i}}$ unique ('up to sign'): write Hyperellipse ellipse semiaxes in descending order; but can reverse sign of $u_{i}$, $v_{i}$ or can multiply them with any suitable pair of complex roots of 1.

\subsection{Singular value properties}
See another section.

\subsection{Finding SVD using EVD}
Use eigenvalue decompositions: $AA^{*} = U \SW^{2}U^{*}$, and $A^{*}A = V \SW^{2}V^{*}$. Otherwise, find eigenvalue decomposition of $B = \mat{0 & A\\ A^{T} & 0}$: then ew(A) are composed of zeros and sw(A) repeated with different signs. ev of B is $(\sqrt{2})^{-1}\mat{U_n & V\\ \sqrt{2}U_{m-n} & 0}$.

\subsection{Polar decomposition}
$m \leq n$: take SVD $A = U [\SW\ 0] [V_{1}\ V_{2}]^{*} = U \SW V_{1}^{*},\\
 P^{2} = AA^{*} = U \SW^{2}U^{*}$: +ve semidefinite; take $P = U \SW U^{*}$: Hermitian +ve semidefinite. So, $A = U \SW V_{1}^{*} = PUV_{1}^{*} = PY$, where Y has orthonormal rows.

So, if $m \geq n$: $A = YQ$ for Hermitian +ve semidefinite Q, Y with orth columns: apply thm to $A^{*}$.

\section{PA = LU}
Here unit lower triangular L, upper triangular U.

Can also make: PA = LDU: For , unit upper triangular U, diagonal D.

\subsection{Existance and uniqueness}
\subsubsection{Existance}
See triangularization by row elimination algorithm in numerical analysis ref. That this runs to completion proves existance.

\subsubsection{Uniqueness if P=I}
A = LU unique: Else if $LU = L'U'$, $L'^{-1}L = U'U^{-1}$: absurd. So A=LDU unique.

\subsection{For hermitian positive definite matrices}
As $A \succeq 0$, P = I.

As $A = LDU = A^{*}$, can take $A = RR^{*}$, where R = $LD^{1/2}$ (Cholesky). It is also unique: $r_{j,j} = \sqrt{d_{j,j}} >0$ fixed by definition; it inturn fixes rest of R.

\subsection{Importance}
Very useful in solving linear equations invloving the same matrix A: can store L, U for repeated reuse.


\section{A = QR = LQ'}
Express columns of $A$ as linear combinations of orthogonal $\set{q_i}$. For proof of existance, see triangular orthonormalization algorithm in numerical analysis ref.

Taking the QR factorization of $A^{T}$, you also get $A = LQ^{T}$, where $L$ is lower triangular.

\subsection{Importance}
Often, we need to get an orthogonal basis for range(A).

\subsection{Column Rank deficient A}
If $A$ were rank deficient, multiple columns would be linear combinations of same set of $q_i$'s.  As Q is square, we would have 0 rows .

\subsubsection{Rank revealing QR}
In such cases, we can always assume that the 0 rows appear at the bottom, revealing the rank.

\section{Factorization of Hermitian matrices}
\subsection{Unitary diagonalizability, SVD}
From Schur factorization: Can write $A=Q \EW Q^{*}$. So, has full set of orthogonal eigenvectors. So, can write: $A = \sum_i \ew_i q_i q_i^{*}$.

Also, singular values $s_{i}= |\ew_{i}|$, but can't write $A = U \SW V^{*} = U \SW U^{*}$: there may be sign difference between U and V's columns due to $\ew_i < 0$.

\subsection{Symmetric LDU factorization}
(Cholesky). $A = R^{*}R$. As $A = LDU^{*}=UDL^{*}$, $L=U^{*}$. So, $A = LDL^{*} = LD^{1/2}D^{1/2}L^{*} = R^{*}R$; $d_{j,j} > 0$ as $a_{j,j}>0$; $r_{j,j} = \sqrt{d_{j,j}} >0$ chosen.

By SVD, $\norm{R}^{2} = \norm{A}$.

\subsection{Square root of semidefinite A}
$A = (A^{1/2})^{*}A^{1/2}$. Diagonalize, get $A = QLQ^{*}$, $A^{1/2} = QL^{1/2}Q^{*}$ : the unique +ve semidefinite solution.

\chapter{Special linear operators}
\section{Orthogonal (Unitary) m*n matrix}
Columns orthonormal: $Q^{*}Q=I$; and $m \leq n$.

\subsection{Change of basis operation}
Qx=b: $x=Q^{*}b$: so, x has magnitudes of projections of b on q's: Change of basis op.

Alternative view: $Q^{*}(\sum a_{i}q_{i}) = \sum a_{i}e_{i}$.

\subsubsection{Preservation of angles and 2 norms}
So, $\dprod{Qa, Qb} = b^{*}Q^{*}Qa = \dprod{a,b}$. Also, $\norm{Qx} = \norm{x}$: So, length, angle preserved; analogous to $z \in C$, with $|z| = 1$. If $\norm{Qx} = \norm{x}$, $Q^{*}Q = I$.

\subsection{Square Unitary matrix}
\subsubsection{Orthogonality of rows}
If Q square, even rows orthogonal: $Q^{*}Q=I \implies QQ^{*}=I$: $q_{i}^{*}q_{j} = \change_{i,j}$ (Kronecker $\change$ = 1 iff i=j, else 0.).

\subsubsection{Rotation + reflection}
Because of its being a change of basis operation, by geometry, $Q$ is rotation or reflection or a combination thereof. 

The distinction between orthogonal matrices constructed purely out of rotation matrices (proper rotation), and those involving orthogonal matrices which involve reflections (improper rotation) is important in geometric computations: in applications such as robotics, computational biology etc..

\subsubsection{Determinant}
$det(Q) = \pm1 : det(Q^{*}Q) = det(I) = 1$.

$Q$ is a rotation if $|Q|=1$ or a reflection if $|Q|=-1$: True for m=2; For $m >2$, see that determinant is multiplicative.

\subsubsection{Permutation matrix P}
A permuted I. Permutes rows (PA) or columns (AP). Partial permutation matrix: every row or column has $\leq 1$ (maybe 0) nz value.

\subsubsection{Rotation matrix}
To make a rotation matrix, take new orthogonal basis $(u_i)$: the coordinate system is rotated, $e_i \to u_i$, get matrix U. $(U^{*}x)_i = u_i^{*}x$: x rotated. Note: $U^{*}u_i  = e_i$.

\subsubsection{Reflection matrix}
Take reflection accross some basis vector (not any axis). This is just I with -1 instead of some 1.

\section{Linear Projector P to a subspace S}
\subsection{Using General projection definition}
See definition of the generalized projection operation in vector spaces ref. Here, we consider the case where projection happens to be a linear operator: that is, it corresponds to the minimization of a convex quadratic vector functional, where the feasible set is a vector space, the range space of the projector $P$.

\subsection{Definition for the linear case}
P such that $P^{2}=P$: so, a vector already in S is not affected.

(I-P) projects to N(P): If Pv=0, (I-P)v=v; vice versa. Rank r projectors project to r dimension space.

Oblique projectors project along non orthogonal basis.

\subsection{Orthogonal projector}
Here, $(I-P)x \perp Px$: Eg: projectors which arise from solving the least squares problem. Ortho-projectors $\neq$ orthogonal matrices.

If P=P*, P orth projector: If P=P*, $\dprod{(I-P)x, Px} = 0$. If P orth proj; make orthonormal basis for range(P), N(P); get Q; now $PQ= Q\SW$, with $\sw_{i}$ 1 or 0 suitably: SVD! So, if P orth proj, P=P*.

Ergo, (I-P) also orth proj. Also, $P = \hat{Q}\hat{Q}^{*}$ (As $A = \hat{Q}\hat{R}$): Also from $v = r + \sum (q_{i}q_{i}^{*})v$. All $\hat{Q}\hat{Q}^{*}$ orth proj: satisfy props.

$\norm{P} = 1$. \why

\section{Hermitian matrix}
Aka Self Adjoint Operator. Symmetric matrix: $A=A^{T}$. It generalizes to Hermitian matrix $A=A^{*}$; analogous to $R \subseteq C$. Not all symmetric matrices are Hermitian.

Notation: Symmetric matrices in $R^{n \times n}: S^{n}$; +ve definite among them: $S^{n}_{++}$.

Skew/ anti symmetric matrix: $A= -A^{T}$, generalizes to skew Hermitian.

$\dprod{Ax, y} = y^{*}Ax = \dprod{x, A^{*}y}$.

\subsection{Importance}
Appears often in analysis: Any $B = \frac{B+B^{*}}{2} + \frac{B-B^{*}}{2}$: Hermitian + Skew Hermitian. Also in projector.

Many applications: Eg: Covariance matrix, adjascency matrix, kernel matrix.

\subsection{Self adjointness under M}
For SPD $A$, M, $M^{-1}A$ self adjoint under M as $\dprod{x, M^{-1}Ay}_{M} = y^{*}Ax = \dprod{M^{-1}Ax, y}_{M}$.

\subsection{Eigenvalues (ew)}
All eigenvalues real: \\
$\conj{l}x^{*}x = (lx)^{*}x = (Ax)^{*}x = x^{*}(Ax) = lx^{*}x$, so $\conj{l}=l$.

ev of Distinct ew are orthogonal: $x_{2}^{*}Ax_{1} = l_{1}x_{2}^{*}x_{1} = l_{2}x_{2}^{*}x_{1}, \therefore x_{2}^{*}x_{1}(l_{1}-l_{2}) = 0$.

\subsubsection{sw and norm}
Unitary diagonalizability possible for A: see section on factorization of hermitian matrices. Thence, $|\ew(A)| = \sw(A)$; so $|\ew_max(A)| = \norm{A}$.


\subsection{Factorizations}
For details, see section on factorization of hermitian matrices.

\subsection{Skewed inner prod \htext{$x^{*}Ax$}{..}}
$x^{*}Ay = (y^{*}Ax)^{*}$. So, $x^{*}Ax = \conj{x^{*}Ax}$; so $x^{*}Ax$ real.

\section{+ve definiteness}
\subsection{Definition}
If $\forall 0 \neq x \in C^{n}: x^{*}Ax \in R; x^{*}Ax \geq 0$, $A$ +ve semi-definite, or non-negative definitene.

If $x^{*}Ax > 0$, +ve definite: $A \succ 0$.

Similarly, -ve (semi-)definite defined.

\subsubsection{Importance}
Important because Hessians of convex quadratic functionals are +ve semidefinite. Also, it is importance because of its connections with ew(A).

\subsection{+ve semidefinite cone}
The set of +ve semidefinite matrices, is a proper cone. If restricted to symmetric matrices, get $S_+^{n}$.

\subsubsection{Matrix inequalities}
Hence, inequalities wrt the cone defined. \exclaim{Can write $A \succeq 0$ to say $A$ is +ve def.} This is what is usually assumed by $\succeq$ when dealing with +ve semidefinite matrices - not elementwise inequalities.

\paragraph*{Linear matrix inequality (LMI)}
$A_0 + \sum x_i A_i \preceq 0$. Note that this is equivalent to having $A \preceq 0$ with $A_{i,j} = a_{ij}^{T}x + b_{ij}$ form. Used in defining semidefinite programming.

\subsubsection{Analogy with reals}
Hermitians analogous to R, +ve semidef like $\set{0} \union R^{+}$, +ve def like $R^{+}$ in $\mathbb{C}$.

\subsubsection{Support number of a pencil}
$s(A, B) = \argmin t: tB - $A$ \succeq 0$.

\subsection{Non hermitian examples}
Need not be hermitian always. \\
Then, as $x^{*}Bx = x^{*}B^{*}x$, anti symmetric part in $B = \frac{B+B^{*}}{2} + \frac{B-B^{*}}{2}$ has no effect.

\subsection{Diagonal elements, blocks}
$e_{i}^{*}Ae_{i} = a_{i,i}$. So, $a_{i,i}$ real. $a_{i,i} \geq 0$ if $A$ +ve semidefinite; $a_{i,i}> 0$ if $A$ +ve definite; but converse untrue.

Similarly, for $X \in C^{m\times n}$ invertible: $X^{*}AX$ has same +ve definiteness as A. Taking X composed of $e_{i}$, any principle submatrix $A_{i,i}$ can be writ as $X^{*}AX$; so $A_{i,i}$ has same positive definiteness as A.

\subsubsection{Off-diagonal block signs invertible}
Off-diagonal block signs are invertible without loosing +ve semidefiniteness. Pf: If $\mat{x^{T} & y^{T}}\mat{A & B \\ C & D}\mat{x \\ y} \geq 0 \forall \mat{x \\ y}$, then $\mat{x^{T} & y^{T}}\mat{A & -B \\ -C & D}\mat{x \\ y} \geq 0 \forall \mat{x \\ y}$.



\subsection{Eigenvalues: Real, +ve?}
$\forall i: \ew_i \in R$: take ev $x$, must be able to compare $x^{T}Ax = \gl x^{*}x$ with 0.

If $A \succeq 0$, ew $\ew_i \geq 0$: $\ew_i x^{*}x = x^{*}Ax \geq 0$. Also, if $A \succ 0$, $\ew_i > 0$.

\subsubsection{Determinant}
$det(A) = \prod \gl_i \geq 0$.

\subsection{+ve inner products}
For +ve definite matrices, get +ve inner products: Take eigenvalue decompositions: $A = \sum_i \ew_i q_i q_i^{T}, B = \sum_i l_i p_i p_i^{T}$.

So, the +ve definite cone is self dual.

\subsection{Invertibility}
If $A$ +ve def., $A$ is invertible: $\forall x\neq 0: x^{*}Ax \neq 0$, so $Ax \neq 0$; so $A$ has no nullspace. If $A$ +ve semi-def, can't say this.

\section{Hermitian +ve definiteness}
Also, see properties of not-necessarily symmetric +ve semidefinite matrices.

\subsection{From general matrices}
Any $B^{*}B$ or $BB^{*}$ hermitian, +ve semidefinite: $x^{*}B^{*}Bx = \norm{Bx}$. So, if B invertible, $B^{*}B$ is +ve definite. So, if B is long and thin, $B^{*}B$ is +ve definite, but if B is short and fat: so singular, $B^{*}B$ is +ve semi-definite, also singular.

\subsection{Connection to ew}
If $A = A^{*}$, all eigenvalues $l>0$, then $A$ is +ve definite: $x^{*}Ax \in R$, $x^{*}Ax = x^{*}U\EW U^{*}x = \sum \ew_{i}x_{i}^{2}$.

Magnitudes of ew same as that of sw: as you can easily derive SVD from eigenvalue decomposition. So, singular value properties carry over.

\subsection{Connection to the diagonal}
\subsubsection{Diagonal dominance}
If $A = A^{*}$, diagonal dominance and non-negativity of $A_{i,i}$ also holds, then $A$ is +ve semidefinite. See diagonal dominant matrices section for proof.

\subsubsection{Diagonal heaviness}
The biggest element of $A$ is the biggest diagonal element. For some k : $a_{k,k} \geq a_{i,j} \forall i,j$. Pf: Suppose $a_{i,j} > a_{k,k}$; then consider submatrix $B = \mat{a_{i,i} & a_{i, j}\\ a_{i, j} & a_{j,j}}$; $B \succeq 0$, but due to assumption $|B| \leq 0$, hence $\contra$.

\subsection{Check for +ve (semi) definiteness}
Do Gaussian elimination, see if pivots $\geq 0$. $x^{*}Ax = x^{*}LDL^{*}x$, if pivots good, can say $= \norm{D^{1/2}L^{*}x} \geq 0$.

\subsection{Block matrices: Schur complement connection}
Take $X = \mat{A & B \\ B^{T} & C}$, $S  = C - B^{T}A^{-1}B$. Then, if $A \succ 0$, $X \succeq 0 \equiv S \succeq 0$. Also, $X \succeq 0 \equiv (A \succ 0 \land S \succ 0)$.

\subsubsection{Proof: rewrite as optimization problem}
Take $f(u, v) = u^{T}Au + 2v^{T}B^{T}u + v^{T}Cv = \mat{u^{T} & v^{T}}X \mat{u \\ v}$. Solve $\min_u f(u, v)$. By setting $\gradient_u f(u, v) = 0$, get minimizer $u' = -A^{-1}Bv$, $f(u', v) = v^{T}Sv$.

\subsection{+ve semidefinite cone}
Denoted by $S_{++}^{n}$ and $S_{+}^{n}$.

\subsubsection{Self duality}
If $A, B \succeq 0$, $\dprod{A, B} = \dprod{\sum_i \ew_i q_i q_i^{*}, \sum_j \ew'_i q_j q_j^{*}} \geq 0$. So, dual of $S_{++}^{n}$ is itself.

When you consider the dual of a semidefinite program, this is important.

\section{Speciality of the diagonal}
\subsection{Diagonally dominant matrix}
$|A_{i,j}| \geq \sum_{j \neq i} A_{i,j}$.

\subsubsection{Hermitian-ness and +ve semidefiniteness}
A hermitian diagonally dominant matrix with non-negative diagonal elements is +ve semi-definite. Pf: Take $x^{T}Ax = \sum_{i,j} A_{i,j}x_i x_j \geq \sum |A_{i,j}|(x_i - x_j)^{2}$. The decomposition reminds one of properties of the graph laplacian. Alternate pf: take ev u, taking $Au = \ew u$, show $\ew \geq 0$.

If symmetry condition is dropped, +ve semidefiniteness need not hold.


\section{Other Matrices of note}
\subsection{Interesting matrix types}
Block matrix; Block tridiagonal matrix.

\subsection{Triangular matrix}
Inverse of L, L' is easy to find: $L'_{i,i} = L_{i,i}^{-1}; L'_{i, j} = -L_{i,j}^{-1}$.

ew are on diagonal.

\subsection{Polynomial matrix}
$P = \sum A(n)x^{n}$: Also a matrix of polynomials.

\subsection{Normal matrix}
$A^{*}A= AA^{*}$. By spectral thm, $A = Q \EW Q^{*}$. Exactly the class of orthogonally diagonalizable matrix.

Let $a = (a_{i,i}), \ew = (ew_{i})$: By direct computation, $a = S\ew$, where $S = [|q_{ij}|^{2}] = Q.\bar{Q}$ is stochastic.

\subsection{Rank 1 perturbation of I}
$A=I+uv^{*}$. Easily invertible: $A^{-1} = I + auv^{*}$ for some scalar a.

\subsection{k partial isometry}
$A = U \SW V^{*}$ with $\SW = \mat{I_{k} & 0 \\ 0 & 0}$.

\section{Positive matrix A}
\subsection{Get Doubly stochastic matrix}
This is important in some applications: like making a composite matrix from the social and affiliation networks.

If $A = A'$, This can be done by first dividing by the largest entry, and then adding appropriate entries to the diagonal.

Can do Sinkhorn balancing: iteratively a] do row normalization, b] column normalization.

\section{Stochastic matrices}
\subsection{Definition}
If $A$ is stochastic, A1 = 1, $A\geq 0$.

\subsection{Eigenspectrum and norm: Square A}
\subsubsection{1 is an ev}
If $A$ is stochastic, A1 = 1. So, 1 is an ew, and 1 is the correspondingn ev.

By Gerschgorin thm, $\ew(A) \in [-1, 1]$, so, $\ew_max = 1$.

If $A$ is also real and symmetric, can get SVD from eigenvalue decomposition, and $\sw_max = \norm{A}_2 = 1$.

\subsection{Product of stochastic matrices}
So, if $A$, B stochastic, AB1 = 1, $1^{*}AB = 1^{*}$: AB also stochastic.

\subsection{Doubly Stochastic matrix S}
S is bistochastic if $S \geq 0, S1 = 1, 1^{*}S = 1^{*}$.

(Birkhoff): $\set{S}$ = set of finite convex combos of permutation matrices $P_{i}$. Pf of $\to$: If convex combo of $\set{P_{i}}$, S stochastic. Every $P_{i}$ is extreme point of $\set{S}$. Every non permutation stochastic matrix $A$ is convex combo of stochastic matrices X = $A$ + aB and Y = $A$ - aB; where B and a are found thus: pick nz $a_{ij}$ in row with $\geq 2$ nz entries, then pick nz $a_{kj}$, then pick nz $a_{k,l}$ etc.. till you hit $a_{i',j'}$ seen before; take this sequence T, set $a = \min T$; make $\pm 1, 0$ matrix B by setting entries corresponding to alternate elements in T 1 or -1. $\set{S}$ is compact and convex set with $\set{P_{i}}$ as extreme points.

\subsection{Doubly Substochastic matrix Q}
$equiv$ $Q1 \leq 1, 1^{*}Q \leq 1$.

For permutation matrix P, PQ or QP also substochastic.

Q is dbl substochastic iff B has dbl stochastic dilation S: make deficiency vectors $d_{r}, d_{c}$; get difference matrix $D_{r}  = diag(d_{r}), D_{c}  = diag(d_{c})$; get $S = \mat{Q & D_{r}\\ D_{c}^{T} & Q^{T}}$.

$\set{Q}$ equivalent to set of convex combos of partial permutation matrices: Dilate Q to S, get finite convex combo of $P_{i}$, take the convex combo of principle submatrices.

$Q \in C^{nn}$ is dbl substochastic iff $\exists$ dbl stochastic $S\in C^{nn}$ with $A \geq B$: Take any Q, get finite convex combo of partial permutation matrices; alter each to get permutation matrix; their convex combo is S.

\section{Large matrices}
Often an approximation to $\infty$ size matrices; so have structure or regularity.

\subsection{Sparsity}
Not density. Very few non zero entries per row: $\nu$. Can find Ax in $O(\nu m)$, not $O(m^{2})$ flops. Can find AB in $O(mn \nu)$.

\subsection{Iterative algorithms}
See numerical analysis ref.

\chapter{Random matrices}
\section{Applications}
\subsection{Random projections}
(Johnson Lindenstrauss) See randomized algorithms survey.

\part{Matrix functions and functionals}
\chapter{Matrix vector spaces and associated functionals}
\section{Vector space of matrices over field F}
$M_{m, n}(F), M_{n, n}(F) \equiv M_{n}(F)$. $M_{m, n}(C) = C^{mn} \equiv C^{m \times n}$.

\section{Matrix inner products}
\subsection{Trace inner product}
$\dprod{A,B} = tr(B^{*}A)$ : same as taking vectorizing B and $A$ and using vector $\dprod{.,.}$; also see the elementwise multiplication before addition view. Aka standard inner product.

For symmetric matrices: $\dprod{A, B} = \sum_i \sum_j X_{ii} Y_{ij}$

\section{Matrix norms}
Obeys all properties of vector norms,\\
plus sub-multiplicativity: $\norm{AB} \leq \norm{A} \norm{B}$. Perhaps $\norm{A} = \norm{A^{*}}$ too. Generalized matrix norms need not be submultiplicative.

\subsection{Unitary invariance}
If $\norm{.}$ unitary invariant, by SVD, $\norm{A} = \norm{\SW}$.

\subsubsection{Symmetric gauge fn g}
$g:C^{q} \to R^{+}$ is a vector norm on $C^{q}$ which is also an absolute norm, and is permutation invariant: g(Px) = g(x): a fn on a set rather than a seq.

Every unitarily invariant matrix norm $\equiv$ symmetric gauge fn on $\sw$. Pf: Given $\norm{}: g(x) = \norm{X}: X = diag(x)$ is symm gauge: permutation invariance from unitary invariance of $\norm{}$. $\norm{X} = g(\sw)$ is unitary invariant matrix norm: unitary invariance from invariance of $\SW$; as g is vector norm, get +ve definiteness, non negativity, homogenousness. $\triangle$ ineq: g is absolute, so monotone; $\sw(A + B)$ weakly majorized by $\sw(A) + \sw(B)$, so $\sw(A+B) \leq S[\sw(A) + \sw(B)]$ for doubly stochastic S; so $g(\sw(A+B)) \leq g(S(\sw(A)+\sw(B))) \leq \sum \ga_{i}(g(P_{i}\sw(A)) + g(P_{i}\sw(B))) \leq g(\sw(A)) + g(\sw(B)) = \norm{A} + \norm{B}$, by Birkhoff.

\subsection{Max norm}
$\max |a_{i,j}|$.

\subsection{Matrix p norms Induced by vector norms}
$\norm{A} = \sup_{x} \frac{\norm{Ax}}{\norm{x}}$. Obeys triangle ineq! So, get (p, q) norm, p norm.

$\norm{A}_{p} = \norm{U\SW V^{*}}_{p} = \norm{\SW}_{p}$. 

$\norm{A}_{\infty}$ is max row sum: use suitable $x = |1|^{n}$; thence get $\norm{Ax}_{\infty}$.

$\norm{A}_{1}$ is max col sum.

\subsubsection{Unitary invariance: 2 norm only}
Change of orth basis. $\norm{QA}_{2}=\norm{A}_{2}$ as $\norm{Qx}_{2}=\norm{x}_{2}$.

But, $\norm{QA}_{p} \neq \norm{A}_{p}$ as $\norm{Qx}_{p} \neq \norm{x}_{p}$. By SVD, $\norm{A}_{2} = \norm{A^{*}}_{2}$.

\subsubsection{Comaprison of norms}
$\norm{A}_{\infty} \leq \sqrt{n}\norm{A}_{2}$: Take x with $\norm{x}_{2} = 1$, for which $\norm{Ax}_{2} = \norm{A}_{2}$; then $n\norm{Ax}_{2}^{2} = n\norm{A}_{2}^{2} = \sum_{j}(\sum_{i}nx_{i}A_{j,i})^{2}$; $nx_{i}^{2} \geq 1$, so this exceeds every row sum.

Similarly, $\frac{\norm{A}_{F}}{\sqrt{n}} \leq \norm{A}_{2}$.

$\norm{A}_{2} \leq \sqrt{m}\norm{A}_{\infty}$: For $\norm{x}_{2}=1, {Ax}_{i} \leq $ max row sum of A.

Indicate matrix \textbf{energy}, consider sphere mapped to ellipse.

\subsubsection{Connection with spectral radius}
$\norm{A} \geq |\gl_{max}(A)|$ as $\sup_x \frac{\norm{Ax}}{\norm{x}} \geq |\gl_{max}(A)|$. \exclaim{Wonderful!}

\subsubsection{Find p norm of A}
For $\norm{A}_{2}$ use SVD; aka spectral norm if $A$ square.

Take x with $\norm{x}_{p}$ = 1, maximize $\norm{Ax}_{p}$. Use Triangle inequality: $\norm{Ax}_{1} = \norm{\sum x_{i}a_{i}}\\ \leq \sum \|x_{i}a_{i}\|$, so $\norm{Ax}_{1} = max \norm{x_{i}}$.

Similarly use Cauchy Schwartz ineq. By $\norm{A} \geq \frac{\norm{Ax}}{\norm{x}}$, \\
$\norm{ABx} \leq \norm{A}\norm{Bx} \leq \norm{A}\norm{B} \norm{x}$; so $\norm{AB} \leq \norm{A}\norm{B}$ (in general a loose bound).

\subsection{Matrix (p, q) induced norm}
Aka operator norm. $\max_{\norm{q} = 1} \norm{Ax}_{p}$. Check $\triangle$ ineq.

\subsection{Ky Fan (p,k) norms}
Take $\sw_{i}$ in descending order. $\norm{A}_{p,k} = (\sum_{i=1}^{k}\sw_{i}^{p})^{1/p}$ for $p\geq 1$: p norm to top k $\sw$.

$\triangle$ ineq for (1, k) norm from $\SW$ inequalities. Vector normness for $\norm{A}_{p,k}$: $\norm{x}_{p,k}$ a symmetric gauge fn: $\triangle$ ineq: take $A$, b in descending order to get $a' = (a_{[i]}), b' = (b_{[i]})$; $\sum_{i}^{k} (a_{[i]} + b_{[i]}) \geq \sum_{i}^{k}(a+b)_{[i]}$; so by weak majorization lore, for $p \geq 1$: $\sum_{i}^{k} (a_{[i]} + b_{[i]})^{p} \geq \sum_{i}^{k}(a+b)_{[i]}^{p}$; thence see: $\norm{a' + b'}_{p,k} \leq \norm{a'}_{p,k} + \norm{b'}_{p,k}$ from p-norm properties.

Matrix normness:\\
$\sum_{i=1}^{k}\sw_{i}(AB)^{p} \leq \sum_{i=1}^{k}\sw_{i}(A)^{p}\sw_{i}(B)^{p}\leq \sum_{i=1}^{k}\sw_{i}(A)^{p}\sum_{i=1}^{k}\sw_{i}(B)^{p}$.

$\norm{A}_{1,1} = \norm{A}_{2}$.

\subsection{Schatten p norms}
Apply p norm to singular values. Special case of Ky Fan norm: $\norm{A}_{p,q}  = \norm{A}_{Sp} = (\sum \sw_{i}^{p})^{1/p}$. Vector normness from seeing that this is a symmetric gauge fn.

\subsubsection{Frobenius (Hilbert-Schmidt, Euclidian) norm}
$\norm{A}_{S2} = \norm{A}_{F}$.

$(\sum a_{i,j}^{2})^{\frac{1}{2}} = (\sum \norm{a_{j}}^{2})^{\frac{1}{2}} = (tr A^{*}A)^{\frac{1}{2}} = (tr AA^{*})^{\frac{1}{2}} = (tr \SW^{*}\SW)^{1/2} = A_{F}$. So, based on matrix inner product: $\dprod{A,B} = tr(B^{*}A)$.

So, $\norm{QA}_{F}=\norm{A}_{F}$. By Cauchy Schwartz, $\norm{C}_{F}^{2} = \norm{AB}_{F}^{2} = \\
\sum_{i}\sum_{j} (a_{i}^{*}b_{j})^{2} \leq \sum_{i}\sum_{j} \norm{a_{i}}_{2}^{2}\norm{b_{j}}_{2}^{2} =\norm{A}_{F}\norm{B}_{F}$.

\subsubsection{Trace (Nuclear) norm}
$\norm{A}_{S1} = \norm{A}_{tr} = \sum \sw_{i} = tr((A^{*}A)^{1/2})$. Corresponds to the trace inner product.

In finding $C,D: \min \norm{A - CD}_{tr}$, using trace norm often yields low rank solutions. \chk

\chapter{Other functionals}
\section{Functionals over square matrices}
Also see functionals over +ve definite A.

\subsection{Determinant of square A}
\subsubsection{Definitions}
Det(A) or $|A|$ : The recursive defn. \textbf{Cofactor} of $A_{1,1}$: $C_{1,1}$ = det of submatrix (minor) of A. Defn by properties: 1: det I = 1. 2: If 2 rows are equal, det(A) = 0. 3: det $A$ depends linearly on first row.

\subsubsection{Properties}
So, there is sign change in Det(A) with row exchanges. Also, Det(A) is unchanged with $row_{1} += k row_{2}$. So, 0 row means det $A$ = 0. Also, for L or U, just multiply diagonal. Also, det $A$ = 0 iff $A$ singular.

\paragraph*{Multiplicativity and consequences}
$|A||B|=|AB|$: See that $\frac{|AB|}{|B|}$ has the 3 properties $|A|$ should have.

So, $|A^{-1}| = \frac{1}{|A|}$. So, $|Q|= \pm 1$. So, considering $|PA| = |LU|$ and $|A^{T}P^{T}| = |U^{T}L^{T}|$, $Det(A^{T}) = Det(A)$. So, column operations don't alter det $A$, there is sign change with col exchanges, write det $A$ with column cofactors etc..

\subsubsection{Connection with ew, sw}
Take characteristic polynomial $p_A$. $det(A) = p_{A}(0) = \prod_{i} (l_{i}-l)]_{l=0}$: also by considering $A = QTQ^{*}$. Also, $|det(A)| = \prod \sw_i(A)$.

\paragraph*{Connection with rank and semidefiniteness}
Thence, det(A) = 0 $\equiv$ $A$ is rank deficient. $det(A) < 0 \implies $A$ \nsucceq 0$: implication is one directional.

\subsubsection{Find Det(A)}
Reduce $A$ to U, find det U. For each of the n! diagonals, multiply elements; add after accounting for permutation sign: From linearity property, get row exchanged diagonal matrices, consider relation between determinant sign and row exchanges; Hence note equivalence of definitions. Use eigenvalue decomposition: $|A|=|S\EW S^{-1}|=|S||\EW||S|^{-1} = |\EW|$.

Det(A) = volume spanned by row (or col) vectors: orthogonalize (parallelopiped changed to cube of equal volume): $|A|$ unchanged; now $AA^{T}$ is diagonal matrix; $|AA^{T}| = |A|^{2} = vol^{2}$. \textbf{Perm(A)}: Same as Det(A), ignore sign.

\paragraph*{Block matrices}
Leibnitz: \\
$X = \mat{A & B \\ C & D } = \mat{A & 0 \\ C & I } * \mat{I & A^{-1}B \\ 0 & D-CA^{-1}B }$. Use this if $A$ square, invertible, thence find determinant.

If X symmetric, ie if $B = C^{T}$: $D-CA^{-1}B$ is the Schur complement of A.

\subsection{Trace of A}
$tr(A) = \sum_{i} a_{i,i}$.

A linear map: tr(kA + lB) = k tr(A) + l tr(B): so convex.

\subsubsection{Trace of AB}
Trace is an inner product on matrices. It is same as vectorizing $A$ and B and applying the standard inner product.

$tr(AB) =  \sum_{i} \sw_{i} \sw_{\pi(i)}$ for some permutation $\pi$: from $|A||B|=|AB|$.

(Von Neumann): $tr(AB) \leq \prod_{i} \sw_{i}(A)\sw_{i}(B)$ \why.

\subsubsection{Cyclicity and similarity invariance}
$tr(AB) = tr(BA) = \sum_{i} \sum_{j} a_{i,j}b_{j,i}$; but $tr(ABC) = tr(BCA) \neq tr(ACB)$.

Similarity invariant: $tr(P^{-1}AP) = tr(APP^{-1}) = tr(A)$.

\subsubsection{Trace of outer products}
$tr(ab^{T}) = b^{T}a$ by algebra.

\subsubsection{Connection with ew}
Take characteristic polynomial $p_A$. tr(A) = coefficient of $l^{m-1}$ in $p_{A}$ = $\sum_{i} l_{i}$.

\subsubsection{Gradient to tr}
By considering $tr(f(X + \change X)) - tr(f(X))$, get: $\gradient_X tr(X) = I, \gradient_X tr(XA) = \gradient_X tr(AX) = A^{T}, \gradient_X tr(BXA) = \gradient_X tr(ABX) = B^{T}A^{T}$.

By considering $tr(f(X + \change X)g(X + \change X))- tr(f(X)g(X))$, \\
$\gradient_X tr(AXBX) = B^{T}X^{T}A^{T} + A^{T}X^{T}B^{T}$.

\section{Functionals over +ve definite matrices}
\subsection{Log det divergence}
$f(A) = \log det(A) = \sum_i \log \ew_i(A) = tr(\log(A))$;\\
 often used because it is convex. $\gradient \log det(A) = A^{-1}$ \why.

\section{Singular values (sw)}
See SVD section.

\subsection{Unitary invariance}
$\SW \in R^{mn}$ always, so $\SW = \SW^{*}$. $\SW = U^{*}AV$: so, $\SW$ is unitary invariant: $\sw_{i}(A) = \sw_{i}(Q_{1}AQ_{2})$.

Let $a = (a_{i,i}), \sw = (\sw_{i})$: By direct computation using $U\SW V^{*}$, $a = Z\sw$, where $Z = [u_{ij}v_{ij}] = (U.\bar{V})_{1:q, 1:q}$; take $Q = |Z|$; get $|a| \leq Q \sw$. $\norm{q_{i}^{*}}_{1}^{2} = (\sum_{j} |u_{ij}v_{ij}|)^{2} \leq \sum_{j} |u_{ij}|^{2}\sum_{j} |v_{ij}|^{2} \leq 1$, by induction; also $\norm{q_{i}}_{1} \leq 1$; so Q is substochastic.

\subsection{Effect of row or column deletion}
$A_{r}$: $A$ with r rows or cols deleted; $\sw_{k}(A) \geq \sw_{k}(A_{r}) \geq \sw_{k+r}(A)$. Prove for r=1, get general case thence. Pf where sth col is deleted: For upper bound, use $\sw_{k}(A_{1}) = \max_{S \subset C^{n}, dim(S) = k} \min_{x \in S} \norm{Ax}_{2}$ with extra cond: $x \perp e_{s}$; for lower bound use \\
$\sw_{k}(A_{1}) = \min_{S \subset C^{n}, dim(S) = n-k} \max_{x \in S} \norm{Ax}_{2}$ with extra cond. For row deletion, consider $A^{*}$.

\subsubsection{Sum, product of sw of square A}
\paragraph*{ew sw sum comparison}
By block multiplicity, for any arbit sq orth $x$, Y: $S_{k} = X_{k}^{*}AY_{k}^{*}$: upper left submatrix of $S = X^{*}AY$; So, $\sw_{i}(X_{k}^{*}AY_{k}^{*}) \leq \sw_{i}(X^{*}AY) = \sw_{i}(A)$. So, $|det(X_{k}^{*}AY_{k})| = \prod_{i=1}^{k} \sw_{i}(X_{k}^{*}AY_{k}) \leq \prod_{i=1}^{k} \sw_{i}(A)$.

For square $A$, take $A = QTQ^{*}$, $T = Q^{*}AQ$, $(\ew_{i}) \downarrow$; so take k-principle submatrix, use block multiplicity to get: $T_{k} = Q_{k}^{*}AQ_{k}$\\
So, $|det(T_{k})| = \prod_{i=1}^{k}|\ew_{i}(A)| = |det(Q_{k}^{*}AQ_{k})| \leq \prod_{i=1}^{k} \sw_{i}(A)$: = for k=m.

By majorization lore, $|tr(A)| \leq \sum_{i=1}^{q} |\ew_{i}(A)| \leq \sum_{i=1}^{q} \sw_{i}(A)$. Also, for any $p>0, \sum_{i=1}^{q} |\ew_{i}(A)|^{p} \leq \sum_{i=1}^{q} \sw_{i}(A)^{p}$.

\paragraph*{Sum of sw of matrix products}
$A \in C^{mp}, B \in C^{pn}$.

Take $AB = U\SW V^{*}$; $U_{k}^{*}ABV_k = \SW_{k}$; polar decomposition of $BV_{k} = X_{k}Q; \\
Q^{2} = V_{k}^{*}B^{*}BV_{k};\ det(Q^{2}) \leq \prod_{i=1}^{k} \sw_{i}(B^{*}B) = \prod_{i=1}^{k}\sw_{i}(B)^{2}$.

So, $\prod_{i=1}^{k} \sw_{i}(AB) = |det(U_{k}^{*}ABV_{k})| = |det(U_{k}^{*}AX_{k})det(Q)| \\
\leq \prod_{i=1}^{k}\sw_{i}(A) \sw_{i}(B)$.

By majorization lore, for $p>0, \sum_{i=1}^{q} \sw_{i}(AB)^{p} \leq \sum_{i=1}^{q} \sw_{i}(A)^{p}\sw_{i}(B)^{p}$.

\paragraph*{Sum of sw as trace maximizer of SVD-like decompositions}
.\\$\sum_{i=1}^{k}\sw_{i}(A) 
= \max \set{|tr(X^{*}AY)|: X^{*}X = Y^{*}Y = I, X \in C^{mk}, Y \in C^{nk}}\\
 = \max \set{|tr(AC)| C \in C^{nm}\texttt{ is rank k partial isometry}}$.

Pf: Can get C from Y, X: $C = YX^{*}, \sw_{i}(C^{*}C) = \sw_{i}(XX^{*})= \sw_{i}(X^{*}X) = 1$; Can get Y, X from C: use SVD: $C = U\SW V^{*} = U_{k}V^{*}_{k}$. $|tr(AC)| = |\sum \ew_{i}(AC)| \leq \sum_{i=1}^{q} \sw_{i}(AC) \leq \sum_{i=1}^{q}\sw_{i}(A) \sw_{i}(C) = \sum_{i=1}^{k}\sw_{i}(A)$. Take $A = U\SW V^{*}$, then for rank k isometry $C = V\hat{I}_{k}U^{*}$, $tr(AC) = tr(U\SW\hat{I}_{k}U^{*}) = \sum_{i=1}^{k}\sw_{i}(A)$.

\paragraph*{Triangle inequality}
All $\sw_{i}$ in descending order; then: \\
$\sum_{i=1}^{k}\sw_{i}(A+B) \leq \sum_{i=1}^{k}\sw_{i}(A) + \sum_{i=1}^{k}\sw_{i}(B)$.

Pf: Let C be some rank k partial isometry:\\ $\sum_{i=1}^{k}\sw_{i}(A+B) = \max \set{|tr((A+B)C)|} \leq \max \set{|tr(AC)| + |tr(BC)|} \\
\leq \max \set{|tr(AC)|} + \max \set{|tr(BC)|} = RHS$.

\subsection{Convexity, concavity}
$\sw_1(X)$ is convex, but $\sw_n(X)$ is concave: $ \sw_1(tA + (1-t)B) = \sup_{\norm{x}=1} tAx + (1-t)Bx \leq \sw_1(A)x + \sw_1(B)x$.


\subsection{Properties of A discerned from SVD}
Rank(A) = num +ve singular values: By geometric action, or by algebra U, V unitary, $\SW$ diagonal: $|A|=|U\SW V^{*}|=|U||\SW| |V^{*}| =|\SW|$. Similarly, numerical Rank of $A$ = num +ve not-close-to-0 $\sw$. Good way to find rank.

SVD is most accurate method for finding orthonormal basis for N(A) and range(A).

$A=\sum \sw_{i}u_{i}v_{i}^{*}$: $\sum$ rank 1 matrices.

\chapter{Eigenvalues and relatives}
\section{Eigenvalue (ew)}
\subsection{ew problem}
Aka: ew or eigenwert. For square $A$ only. These are solutions to the eigenvalue problem: $Ax=\ew x$ (Eigen = distinctive). This defines the Eigen pair: $(\ew, x)$, where $\ew$ is ew, x is a (right) ev (eigen vector).

\subsubsection{Left and right eigenpairs}
Also, can define left ev and ew by the relation $xA = \ew x$. ew of $A$ and left ew of $A^{T}$ are same.

ew of $A$ and ew of $A^{T}$ are the same if $A$ is real: By Schur, $A=QUQ^{*}$, $A^{*}=QU^{*}Q^{*}$. As  $QU^{*}Q^{*}$ is a similarity transformation, and $U^*$ is triangular, the ew of $A$ are still $diag(U)$.

In the case of diagonalizable A, ew decomposition $AS = S \EW$ reveals that rows of $S^{-1}$ are the left ev: For details see ew decomposition section.

\subsection{Characteristic polynomial}
As $(A-\ew I)x = 0$, $det(A-\ew I)=0$. 0 is always an ev.

Other things apart, this implies that ew of a triangular matrix are on its diagonal.

\subsubsection{Mapping polynomials to matrices}
Every polynomial $p(\ew) = \ew^{m} + a_{m-1}\ew^{m-1} .. + a_{0}$ determinant of some matrix; Eg: $\mat{\ew & a_{0}\\ -1 & \ew+a_{1}}$ for m=2; which is $A-\ew I$ for some companion matrix A.

\subsection{Applications}
Domain and range of $A$ are the same space. Useful where iterative calculations: $A^{k}$ or $e^{tA}$ occur.

Physics: evolving systems generated by linear equations; Eg: resonance, stability.

Simpler algorithms: Reduces coupled system into a collection of scalar systems. \why



\section{ew and ev properties}
\subsection{ew properties}
For connection with matrices, distribution etc.. see later section.

\subsubsection{Number of ew}
There are n ew in $C$. Algebraic multiplicity of ew: number of times an ew appears as root of $p(\ew)$.

\subsubsection{0 as an ew}
If $A$ full rank, 0 is not an ew: $Ax=\ew x$ else A's columns would be dependent. If 0 is an ew, $E_{0} = N(A)$.

\subsection{Eigenspace of an ew}
If x is an ev corresponding to $\ew$, so is -x and kx for any scalar k. For every $\ew$, ev span the null space of $A-\ew I$.

An invariant subspace: $AE_{\ew}\subseteq E_{\ew}$. $E_{\ew_{1}} = N(A-\ew_{1}I)$.

\subsubsection{Independence of ev}
ev $x_{1}, x_{2}$ (non-0) for distinct ew $\ew_{1}, \ew_{2}$ are linearly independent: otherwise action of $A$ on collinear $x_1, x_2$ would involve the same scaling.

\subsubsection{Defective matrices}
Geometric multiplicity of $\ew$ $\leq$ algebraic multiplicity: Let n be geometric multiplicity of $\ew$ in A; Select orth vectors in $E_{\ew}$ to get $\hat{V}$; extend it to square V; take $B=V^{*}AV = \mat{\ew I & C \\ 0 & D}$; but $det(B-zI) = det(\ew I-zI)det(D-zI) = (\ew-z)^{n}det(D-zI)$: so algebraic multiplicity of $\ew$ in B $\geq n$, same in A.

If algebraic multiplicity of ew exceeds geometric multiplicity, ew is defective. If $A$ has defective ew, it is defective. Eg: Diagonal matrix never defective.

\subsection{Rayleigh quotient of x}
$r(x) = \frac{x^{T}Ax}{x^{T}x}$. Like solving the least squares problem: $xa\approx Ax$ for a; thereby approx eigenvalue. Range(r(x)): Field of values of numerical range of A: W(A). Includes Convex hull of $\EW(A)$.

\subsubsection{EV as stationary points}
$\partder{r(x)}{x_{j}} = \frac{2(Ax-r(x)x)_{j}}{\norm{x}^{2}}$; their vector, the gradient $\gradient r(x) = \frac{2(Ax-r(x)x)}{\norm{x}^{2}}$. ev are the stationary points: $\gradient r(x) = 0$ iff x is ev and r(x) is ew.

\subsubsection{Geometry}
$r(x):S^{n-1} \to R$. Normalized ev of $A$ are stationary points of r(x) for $x \in S^{n-1}$.

\subsubsection{maxmin thm for symmetric A}
[Courant Fishcer]. $\ew_i$ descending. Then $\ew_i = \max_{S:dim(S) = i} \min_{y \neq 0 \in S} R_A(y)$.


\subsubsection{Quadratic Accuracy of ew wrt ev error}
For $A = A^{T}$, let $q_{i}$ be ev. Then, any $x = \sum_{i} a_{i}q_{i}$, $r(x) = \frac{\sum a_{i}^{2}\ew_{i}}{\sum a_{i}^{2}}$. So, W(A) = Convex hull of $\EW(A)$. So, $max_{x}r(x) = \ew_{max}$. Also, $r(x) - r(q_{i}) = O(\norm{x-q_{i}}^{2})$ as $x \to q_{i}$ or $\forall j: |a_{j}/a_{i}| \leq \eps$.

\subsubsection{Rayleigh quotient of M: Generalization}
If $A$ = m*m, M is m*n: thin, tall, required to be full rank.\\
$r(M) = tr((M^{T}M)^{-1}(M^{T}AM))$.

Take svd: $M = U\SW V^{*}$. Then \\
$\max r(M) = \max_{U} |tr(U^{T}AU)| = \max_{U} \prod |R(u_{i})|$. This happens when U is top k unique ev(A).

\section{ew and matrices: other connections}
\subsection{ew distribution}
Set of ew: Spectrum of A: $\EW(A)$. Spectral radius: $\rho(A) = |\ew_{max}|$. ew follows wigner's semicircle distribution for random matrices. \why

\subsection{ew and the diagonal}
\subsubsection{In Disks around the diagonal}
(Gerschgorin) In complex plane, take disks with center $A_{i,i}$, radii $\sum_{j\neq i} A_{i,j}$; each ew lies in atleast one disk: Take ev v and ew l; take $i = argmax_{i}\ v_{i}$; then $|\ew-a_{i,i}|\leq |\sum_{j\neq i}a_{i,j}\frac{v_{j}}{v_{i}}| \leq |\sum_{j\neq i}a_{i,j}|$. Good estimate of ew!

\subsubsection{Monotonic dependence on diagonal}
Take $A = QTQ^{*}$, take any +ve diagonal D; get $A+D = Q(T+D)Q^{*}$. Used to take $A \notin S_+ to B \in S_+$.

\subsection{Effect of transformations}
\subsubsection{Similarity transformation}
X nonsingular, square; $A \to B = X^{-1}AX$. $A$, B similar if $\exists X: B=X^{-1}AX$.

Change of basis op: See Ax=b as $AXx'=Xb'$ when $Xx'=x$, $Xb'=b$, so $Bx'=b'$.

$p_{X^{-1}AX} = det(\ew I - X^{-1}AX) = det(X^{-1}(\ew I - A)X) = det(\ew I - A) = p_{A}$. So, ew's, their agebraic multiplicities same. Geometric multiplicities same: $X^{-1}E_{\ew}$ is eigenspace of $X^{-1}AX$: if $Ax = \ew x$, $BX^{-1}x = \ew X^{-1}x$.

\subsubsection{Eigenpairs of \htext{$A^{k}$}{..}}
$A^{k} = (QUQ^{*})^{k} = QU^{k}Q^{*}$. So, ew are $\ew_{i}^{k}$. So, ew of $A^{-1}$ are $\frac{1}{\ew_{i}}$: $L^{-1} = (S^{-1}AS)^{-1} = S^{-1}A^{-1}S$.

\subsection{Matrix construction}
\subsubsection{Matrix with certian ew and diagonal entries}
If real $\ew$ majorizes $A$, $\exists$ real symmetric $A$ with $a_{i,i} = a_{(i)}$, such that $\ew_{(i)}$ are ew of A. Pf: By induction; assume true for n-1; Take $g \in R^{n-1}$ interleaved amongst $\ew$ which majorizes a; by ind hyp, take real symmetric $B = QGQ^{*}$ with diagonal a and ew g; can make $A' = \mat{G & y\\ y^{T} & \ga}$ with ew $\ew$; take $A = \mat{Q & 0 \\ 0 & 1}A'\mat{Q^{T} & 0 \\ 0 & 1} = \mat{B & Qy\\ y^{T}Q^{T} & \ga}$ with ew $\ew$ and diag a.

\subsubsection{Extending \htext{$\EW$}{EW} to A with certain interleaving \htext{$\ew(A)$}{ew}}
Take $\ew \in R^{n}$ interleaved between $l \in R^{n+1}$. Can make real $A = \mat{\EW & y \\ y^{T} & a}$ with $\ew(A) = l$.

Pf where $\ew_{i}$ differ: $a = tr(A) - tr(\EW) = \sum l_{i} - \sum \ew_{i}$. Take $det(tI-A) = det[\mat{I & (tI-\EW)^{-1}y \\ 0^{T} & 1}\mat{tI-\EW & -y \\ -y^{T} & t-a}\mat{\EW & y \\ y^{T} & a}]$ (multiplicity two matrices with det = 1) $= [(t-a) - y^{T}(tI-\EW)^{-1}y]det(tI-\EW) = (t-a) - \sum_{i}y_{i}^{2}(t-\ew_{i})^{-1}\prod_{i}(t-\ew_{i}) = p_{A}(t)$.

Find y to make $p_A(l_{i}) = 0$. Characterize y, show it exists: Take $f(t) = \prod (t- l_{i}), g(t) = \prod (t - \ew_{i})$, so $f(t) = g(t)(t-c)+r(t)$, where r(t) has degree $\leq n-1$. $c = \sum_{i}l_{i} - \sum_{i}\ew_{i} = a$; $f(\ew_{i}) = r(\ew_{i})$, so get Lagrange interpolation form of $r(t) = \sum_{i} f(\ew_{i})\frac{g(t)}{g'(\ew_{i})(t-\ew_{i})}$. So, $\frac{f(t)}{g(t)} = (t-a) - \sum_{i} \frac{-f(\ew_{i})}{g'(\ew_{i})(t-\ew_{i})}$; this is $0 \forall t = l_{i}$ and $y_{i}^{2} = \frac{-f(\ew_{i})}{g'(\ew_{i})}$. But, by interlacing, $f(\ew_{i}) = (-1)^{n-i+1}\prod_{j}|\ew_{i} - l_{j}|$ and $g'(\ew_{i}) = (-1)^{n-i}\prod_{j \neq i}|\ew_{i} - \ew_{j}|$ oppose in sign, so $\exists y$.

Pf where $ew_{i}$ recurs: Divide out $(t-\ew_{i})^{k}$ from all, proceed as usual.


\subsection{EW of special matrices}
\subsubsection{Real A: complex ew in pairs}
If $A$ is real, $p_{A}$ has real coefficients.

So, if l is ew of $A$, so is $\bar{\ew}$: complex roots of P appear in pairs. ew simple if its algebraic multiplicity 1.


\subsubsection{Triangular and diagonal A}
For triangular $A$, from $det(A-\ew I)=0$, ew are on diagonal. So, $tr(A) = \sum A_{i,i} = \sum \ew_{i}$. Also, $\prod \ew_{i} = |A|$. For diagonal $A$, eigenpairs are diagonal element $(A_{i,i}, ke_{i})$.

\subsubsection{Nilpotent matrix A}
A is a nilpotent op (see algebra ref). So, all ew are 0; $|A|$, tr(A) are 0. Any singular $A$ = product of nilpotent matrices.

\subsubsection{Singular A}
0 is an ew.

\subsubsection{Semidefiniteness and hermitianness}
See other sections.

\section{Generalized eigenvalue problem}
$Az = \ew Bz$. $det(A-\ew B) = 0$. $\set{A-\ew B}$ is called pencil.

\subsection{General Rayleigh quotient of x}
$R(x) = \frac{x^{*}Ax}{x^{*}Bx}$. $\gradient R(x) = 2(Ax-R(x)Bx)$ is 0 exactly where R(x) is $\ew$, x the ev.

\subsubsection{Of M}
M is tall, thin, with independent columns. \\
$R(M) = tr((M^{*}BM)^{-1} (M^{*}AM))$. Using svd $M = U\SW V^{*}$:\\ get $R(M) = tr_{U}(U^{*}B^{-1}AU)$: same as common rayleigh quotient of $B^{-1}A$: see ev section. Maximized when U is the top ev's of $B^{-1}A$.

\subsection{Reductions to common ew problem}
If B is invertible: $B^{-1}Az = \ew z$: so now an $\ew$ problem. Don't want to invert: so solve some other way.

So, if B is invertible, symmetric, +ve definite: $R(x) = \frac{x^{*}Ax}{x^{*}B^{1/2}B^{1/2}x}$; taking $z=B^{1/2}x$, get $R(z) = \frac{z^{*}B^{-1/2}AB^{-1/2}z}{z^{*}z}$. Max of R(x) is achieved somewhere in R(z) as $z \to x$ is a 1 to 1 map. ew of $B^{-1/2}AB^{-1/2}$ are easier to find as it is symmetric.

\chapter{Matrix to matrix functions}
\section{\htext{$(I-A)^{-1}$}{Neumann} series for square A}
Aka Neumann series. $(I-A)^{-1} = \sum_{k=0}^{\infty} A^{k}$, if it converges. It converges when $A$ has norm $<1$.

\section{Matrix exponentiation for square A}
$e^{A} = \sum_{k=0}^{\infty} \frac{A^{k}}{k!}$: always converges. Also defines $\log A$. If $A$ nilpotent, series is finite.

Using expansion, aggregating suitably: $e^{aX}e^{bX} = e^{(a+b)X}$; If $XY=YX: e^{X}e^{Y} = e^{X+Y}$.

$e^{X}e^{-X} = I$: So exponential of X is always invertible.

$e^{YXY^{-1}} = Ye^{X}Y^{-1}$.

$e^{X^{*}} = (e^{X})^{*}$.

If D diagonal, easy to get $e^{D}$. Thence easily get $e^{A}$ if $A = XDX^{-1}$ (A diagonalizable).

\subsection{Relationship among ew}
As $e^{\ew(A)} = \ew(e^{A}), det(e^{A}) = e^{\sum \ew_{i}(A)} = e^{tr(A)}$.


\part{Generalizations}
\chapter{Tensors}
Independent of chosen frame of reference. An array of numbers is a mere representation (components of the tensor) in a certain basis, not the tensor itself.

\section{Order}
Number of indices for each entry. Observe the difference from dimensions!

Scalar: order 0 tensor. Vector: order 1. Matrices: order 2.

\section{Rank}
\tbc

\bibliographystyle{plain}
\bibliography{linAlg}

\end{document}
