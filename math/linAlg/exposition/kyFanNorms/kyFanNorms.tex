\documentclass[10pt]{amsart}
\input{../../../packages,../packagesMemoir}
\input{../../../macros}
\newcommand{\proofSketch}{\textbf{Proof sketch}}


%opening
\title{Script of a talk on the Ky Fan and Schatten p norms}
\author{vishvAs vAsuki}

\begin{document}
\maketitle

\tableofcontents

\section{Introduction}
\subsection{The theme}
We will introduce the Nuclear norm, the Schatten p norms, the Ky Fan norms, and some of their properties. In particular, we will show that the these norms are indeed matrix norms, that they are convex functions of the matrix entries and that they are unitarily invariant.

\subsection{The intention}
One begins to understand new mathematics only when one can explain it in terms of simpler and familiar mathematics. The intention is to explain these properties in terms of elementary linear algebra, to the extant possible in 1.5 hours. We will get a feel for a good variety of ideas which go into these proofs.

\subsection{The proofs}
The proofs and ideas presented here come from \cite{hornJohnson} and  \cite{hornJohnsonTopics}.

There are over 20 theorems and lemmas involved, and it is a joy to read through them all. We will see proofs of important claims in detail. But, in the interests of time, we will omit proofs to other subsidiary claims.

Also, though I have done the algebra, I have not yet satisfactorily tried to view various claims from the geometric perspective.

\subsection{Outline}
First we will define some norms, consider some ideas about stochastic matrices, majorization and $\sw$ inequalities, then we will show the normness of Ky Fan norms and Schatten p norms.

\subsection{Notation}
Unless specified otherwise, any matrix is assumed to be $m \times n$. Let $q = \min \set{m,n}$. $(a_{i})$ represents a vector whose ith element is $a_{i}$. $(a_{i}) \downarrow$ represents that in this vector, all elements are arranged in descending order.

\subsection{Things to leave on the whiteboard}
The outline. The definition of norms, matrix norms, Ky Fan and Schatten norms, Birkhoff's theorem, majorization.

\section{The norms}
\subsection{The intention}
We will now define some norms of interest.

\subsection{Vector and matrix norms}
Any vector norm obeys non negativity, positive definiteness, homogeneity / scalability ($\norm{cv} = c\norm{v}$) and triangle inequality.

In addition to this, matrix norms obey submultiplicativity: $\norm{AB} \leq \norm{A} \norm{B}$.

\subsubsection{Absolute norm}
Absolute norm: $\norm{a} = \norm{|a|}$. Absoluteness iff monotonicity ($|x|\leq |y| \implies \norm{x} \leq \norm{y}$).

\subsection{Ky Fan (p,k) norms}
Take $\sw_{i}$ in descending order. $\norm{A}_{p,k} = (\sum_{i=1}^{k}\sw_{i}^{p})^{1/p} $ for $p\geq 1$: p norm to top k $\sw$.

\subsection{Schatten p norms}
Apply p norm to singular values. Special case of Ky Fan norm: $\norm{A}_{p,q}  = \norm{A}_{Sp} = (\sum \sw_{i}^{p})^{1/p}$.

\subsubsection{Frobenius (Hilbert-Schmidt, Euclidian) norm}
$\norm{A}_{S2} = \norm{A}_{F}$.

\paragraph*{Optional info}
$(\sum a_{i,j}^{2})^{\frac{1}{2}} = (\sum \norm{a_{j}}^{2})^{\frac{1}{2}} = (tr A^{*}A)^{\frac{1}{2}} = (tr AA^{*})^{\frac{1}{2}} = (tr \SW^{*}\SW)^{1/2} = A_{F}$. So, based on matrix inner product: $\dprod{A,B} = tr(B^{*}A)$.

\subsubsection{Trace (Nuclear) norm}
$\norm{A}_{S1} = \norm{A}_{tr} = \sum \sw_{i} = tr((A^{*}A)^{1/2})$.

\subsection{Convexity}
Convexity of vector norms is a direct consequence of their homogeneity and congruence with the $\triangle$ inequality.

\section{Stochastic matrices}
\subsection{The intention}
This is an important idea we will need in proving important ideas about majorization, and in our proofs of normness. We will introduce the ideas, and will return to the proofs later if we still have time in the end.

\subsection{Bistochastic matrix}
S is bistochastic if $S\geq 0$, every row and column sums to 1.

For permutation matrix P, PS or SP also stochastic.

(Birkhoff): $\set{S}$ = set of finite convex combinations of permutation matrices $P_{i}$.

\subsection{Doubly substochastic matrix Q}
Q is doubly Substochastic if $Q \geq 0$, every row and column sums to at most 1.

\section{Ideas about Majorization}
\subsection{The intention}
Majorization is an important idea we will need in deriving singular value inequalities, and in our proofs of normness. We will introduce the ideas, and will return to the proofs later if we still have time in the end.

\subsection{Strong majorization}
Take $a, b \in C^{m}$, rearrange in descending\\
 order to get the vectors $(a_{[i]}), (b_{[i]})$, and in ascending order to get $(a_{(i)}), (b_{(i)})$.

$a \oleq b$ (b majorizes a) if $\sum_{i=1}^{m} a_{i} = \sum_{i=1}^{m} b_{i}, \sum_{i=1}^{k}a_{[i]} \leq \sum_{i=1}^{k}b_{[i]} \forall k$.

\subsubsection{Differing notations}
One can get an $\equiv$ notion from using ascending order and saying a majorizes b. Note: \cite{hornJohnson} and \cite{hornJohnsonTopics} differ slightly in the way they define majorization notations.

\subsection{Connection with stochastic matrices}
b majorizes a iff $\exists$ doubly stochastic S such that $a = Sb$.

\subsection{Weak majorization}
Weak majorization ($\ogeq$) is like strong majorization, except the $\sum_{i=1}^{m} a_{i} = \sum_{i=1}^{m} b_{i}$ condition is omitted.

\subsubsection{Connection with stochastic matrices}
b weakly majorizes $a \geq 0$ iff $\exists$ doubly substochastic Q: $a = Qb$.

b weakly majorizes a iff $\exists$ doubly stochastic S: $a \leq Sb$.

\subsubsection{Weak Majorization preserved under convex increasing fn}
Take convex increasing scalar fn f. Let f(a) denote the vector $(f(a_{i}))$. If b weakly majorizes a, then $f(b)$ weakly majorizes $f(a)$.

\proofSketch (3.3.8 in \cite{hornJohnsonTopics}):

As b weakly majorizes a ($a \oleq b$), for some doubly stochastic Q, $a \leq Qb$.

Due to monotonicity of f, $f(a) \leq f(Qb)$. So, $f(a) \oleq f(Qb)$.

By Birkhoff's thm, for $\ga_{i} \geq0, \sum \ga_{i} = 1$, $f(Qb) = f((\sum \ga_{i}P_{i})b) \leq \sum \ga_{i}f((P_{i})b)$ (using the convexity of f) $ = \sum \ga_{i}P_{i}f(b) = Qf(b)$. So, $f(Qb) \oleq f(b)$. Hence, we have that $f(a) \oleq f(Qb)\oleq f(b)$. \finish

A corollary: If $0 \leq a, b$ are vectors with entries in descending order, if $\prod_{i=1}^{k}a_{i} \leq \prod_{i=1}^{k} b_{i}$, and if g is such that $g(e^{x})$ is convex increasing, then g(b) weakly majorizes ($\ogeq$) g(a).

\proofSketch: Taking log on both sides, we get $\log a \oleq \log b$. Then, using $f(x) = g(e^{x})$ we get $g(a) \oleq g(b)$. In this sketch, we will ignore cases where $a_{i > k} = 0$.


\section{Singular value \htext{$\SW$}{} properties}
\subsection{Motivation}
Singular value inequalities are important in proving the vector and matrix normness of Ky Fan norms.

\subsection{Unitary invariance}
$\SW \in R^{mn}$ always, so $\SW = \SW^{*}$. $\SW = U^{*}AV$: so, $\SW$ is unitary invariant: $\sw_{i}(A) = \sw_{i}(Q_{1}AQ_{2})$.

\subsection{Trace of A}
$tr(A) = \sum_{i} a_{i,i}$. A linear map: tr(kA + lB) = k tr(A) + l tr(B). $tr(AB) = tr(BA)$. Similarity invariant: $tr(P^{-1}AP) = tr(APP^{-1}) = tr(A)$. So, $tr(A) = tr(QTQ^{*}) = \sum \ew_{i}$.

\subsection{Effect of row or column deletion}
$A_{r}$: A with r rows or cols deleted; $\sw_{k}(A) \geq \sw_{k}(A_{r}) \geq \sw_{k+r}(A)$. (Proof only for 2 hr talk.)

\proofSketch: (3.3.1 \cite{hornJohnsonTopics}) Prove for r=1, get general case thence. Suppose sth col is deleted: For upper bound, use $\sw_{k}(A_{1}) = \max_{S \subset C^{n}, dim(S) = k} \min_{x \in S} \norm{Ax}_{2}$ with extra cond: $x \perp e_{s}$; for lower bound use \\
$\sw_{k}(A_{1}) = \min_{S \subset C^{n}, dim(S) = n-k} \max_{x \in S} \norm{Ax}_{2}$ with extra cond. For row deletion, consider $A^{*}$.

\subsection{The inequality \htext{$\prod_{i=1}^{k} \sw_{i}(X_{k}^{*}AY_{k}) \leq \prod_{i=1}^{k} \sw_{i}(A)$}{..}}
$X_{k}, Y_{k}$ have $\perp$ columns. \proofSketch:

By block mult, for any arbit sq orth X, Y, $S_{k} = X_{k}^{*}AY_{k}$ is the upper left submatrix of $S = X^{*}AY$.

So, $\sw_{i}(X_{k}^{*}AY_{k}) = \sw_{i}(S_{k}) \leq \sw_{i}(S)  = \sw_{i}(X^{*}AY) = \sw_{i}(A)$. So, $|det(X_{k}^{*}AY_{k})| = \prod_{i=1}^{k} \sw_{i}(X_{k}^{*}AY_{k}) \leq \prod_{i=1}^{k} \sw_{i}(A)$.

\subsection{The inequality \htext{$\sum_{i=1}^{k} |\ew_{i}(A)|^{p} \leq \sum_{i=1}^{k} \sw_{i}(A)^{p}$}{..}}
\proofSketch:\\
For square A, take $A = QTQ^{*}$, $T = Q^{*}AQ$, $(\ew_{i}) \downarrow$; and take the k-principal submatrix $T_{k}$. By block multiplication, we get: $T_{k} = Q_{k}^{*}AQ_{k}$.\\
So, $|det(T_{k})| = \prod_{i=1}^{k}|\ew_{i}(A)| = |det(Q_{k}^{*}AQ_{k})| \leq \prod_{i=1}^{k} \sw_{i}(A)$ using a lemma proved earlier. Equality holds for k=m. (3.3.2 \cite{hornJohnsonTopics})

By majorization theorems, $|tr(A)| \leq \sum_{i=1}^{q} |\ew_{i}(A)| \leq \sum_{i=1}^{q} \sw_{i}(A)$. Also, for any $p\geq1, \sum_{i=1}^{k} |\ew_{i}(A)|^{p} \leq \sum_{i=1}^{k} \sw_{i}(A)^{p}$.

\subsection{Polar decomposition}
$m \leq n$: take SVD $A = U [\SW\ 0] [V_{1}\ V_{2}]^{*} = U \SW V_{1}^{*},\\
 P^{2} = AA^{*} = U \SW^{2}U^{*}$: +ve semidefinite; take $P = U \SW U^{*}$: Hermitian +ve semidefinite. So, $A = U \SW V_{1}^{*} = PUV_{1}^{*} = PY$, where Y has orthonormal rows.

So, if $m \geq n$: $A = YQ$ for Hermitian +ve semidefinite Q, Y with orth columns: apply thm to $A^{*}$.


\subsection{The inequality \htext{$\sum_{i=1}^{k} \sw_{i}(AB)^{p} \leq \sum_{i=1}^{k} \sw_{i}(A)^{p}\sw_{i}(B)^{p}$}{..}}
\proofSketch: \\
$A \in C^{m \times p}, B \in C^{p \times n}$. Take $AB = U\SW V^{*}$; $U_{k}^{*}ABV_k = \SW_{k}$. Take polar decomposition of $BV_{k} = X_{k}Q;\ Q^{2} = V_{k}^{*}B^{*}BV_{k}$.

Thus, $det(Q^{2}) \leq \prod_{i=1}^{k} \sw_{i}(B^{*}B) = \prod_{i=1}^{k}\sw_{i}(B)^{2}$, and $det(Q) = \prod_{i=1}^{k}\sw_{i}(B)$. \\
So, $\prod_{i=1}^{k} \sw_{i}(AB) = |det(U_{k}^{*}ABV_{k})| = |det(U_{k}^{*}AX_{k})det(Q)| \leq \prod_{i=1}^{k}\sw_{i}(A) \sw_{i}(B)$ using a lemma proved earlier. (3.3.4 \cite{hornJohnsonTopics})

By majorization theorems, for $p\geq 1, \sum_{i=1}^{k} \sw_{i}(AB)^{p} \leq \sum_{i=1}^{k} \sw_{i}(A)^{p}\sw_{i}(B)^{p}$.

\subsection{k partial isometry}
$A = U \SW V^{*}$ with $\SW = \mat{I_{k} & 0 \\ 0 & 0}$.

\subsection{An identity about partial sum of top k singular values}
$\sum_{i=1}^{k}\sw_{i}(A) = \max \set{|tr(X^{*}AY)|: X^{*}X = Y^{*}Y = I, X \in C^{m \times k}, Y \in C^{n \times k}}\\
 = \max \set{|tr(AC)| C \in C^{n \times m}\texttt{ is rank k partial isometry}}$.

\proofSketch:
We can get C from Y, X by taking $C = YX^{*}$. Then, $\sw_{i}(C^{*}C) = \sw_{i}(XX^{*})= \sw_{i}(X^{*}X) = 1$, and we confirm that C is a rank k partial isometry.

We can also get Y, X from C: use SVD: $C = U\SW V^{*} = U_{k}V^{*}_{k}$.

$|tr(AC)| = |\sum_{i=1}^{q} \ew_{i}(AC)| \leq \sum_{i=1}^{q} |\ew_{i}(AC)| \\
\leq \sum_{i=1}^{q} \sw_{i}(AC) \leq \sum_{i=1}^{q}\sw_{i}(A) \sw_{i}(C) = \sum_{i=1}^{k}\sw_{i}(A)$ using properties shown earlier. Take $A = U\SW V^{*}$, then for rank k isometry $C = V\hat{I}_{k}U^{*}$, $tr(AC) = tr(U\SW\hat{I}_{k}U^{*}) = \sum_{i=1}^{k}\sw_{i}(A)$.

\subsection{The inequality \htext{$\sum_{i=1}^{k}\sw_{i}(A+B) \leq \sum_{i=1}^{k}\sw_{i}(A) + \sum_{i=1}^{k}\sw_{i}(B)$}{..}}
\proofSketch \\
(3.4.3 \cite{hornJohnsonTopics}) :

Let C be some rank k partial isometry. \\
$\sum_{i=1}^{k}\sw_{i}(A+B) = \max \set{|tr((A+B)C)|} \leq \max \set{|tr(AC)| + |tr(BC)|} \\
\leq \max \set{|tr(AC)|} + \max \set{|tr(BC)|} = RHS$.



\section{Proofs of normness}

\subsection{Unitary invariance of norms}
If $\norm{.}$ unitary invariant, by SVD, $\norm{A} = \norm{\SW}$.

\subsubsection{Symmetric gauge fn g}
$g:C^{q} \to R^{+}$ is a vector norm on $C^{q}$, and is also an absolute norm, and is permutation invariant: g(Px) = g(x). So, it is a fn on a set rather than a sequence.

Given a unitarily invariant $\norm{}: g(x) = \norm{X}: X = diag(x)$ is symm gauge. The diag(x) function returns a $m \times n$ matrix whose diagonal elements correspond to the elements of x.

\proofSketch (3.5.18 \cite{hornJohnsonTopics}): Normness of g comes from normness of $\norm{.}$. Permutation invariance and absoluteness come from unitary invariance of $\norm{}$. \finish

Given symmetric gauge g, $\norm{X} = g(\sw)$ is unitary invariant matrix norm.

\proofSketch (3.5.18 \cite{hornJohnsonTopics}): Unitary invariance  of $\norm{.}$ follows from unitary invariance of $\SW$. As g is vector norm, we get +ve definiteness, non negativity, homogenousness. We next prove $\triangle$ inequality.

g is absolute, so it is monotone. $\sw(A + B)$ weakly majorized by $\sw(A) + \sw(B)$, so $\sw(A+B) \leq S[\sw(A) + \sw(B)]$ for some doubly stochastic S. So, using monotonicity of g, $g(\sw(A+B)) \leq g(S(\sw(A)+\sw(B))) = g(\sum_{i} \ga_{i} P_{i}(\sw(A)+\sw(B)))$ (using Birkhoff's thm) $\leq \sum \ga_{i}(g(P_{i}\sw(A)) + g(P_{i}\sw(B))) $ (using homogeneity and $\triangle$ inequality of g) $\leq g(\sw(A)) + g(\sw(B)) = \norm{A} + \norm{B}$, using permutation invariance of g.

\subsection{Ky Fan (p, k) norms}
We have already seen $\triangle$ inequality for (1, k) norm.


\subsubsection{Schatten p norms}
Normness of the Ky Fan (p, q) norms follows easily from the fact that p norms are symmetric gauge functions. So, we are mostly done with them.

\subsubsection{The general case}
The following is optional, and will be presented in case there is time left:

We now show the normness of general (p, k) norms by showing that their dependence on $\sw(A)$ is a symmetric gauge.

Consider the symmetric gauge $\norm{x}_{p,k}$ corresponding to $\norm{A}_{p,k}$. We want to show that this is indeed a symmetric gauge. Only $\triangle$ inequality is nontrivial. :
\proofSketch: 
Take a, b in absolute descending order to get the vectors $a' = (a_{[i]}), b' = (b_{[i]})$.

We can show by induction on k that $\sum_{i=1}^{k}(|a'| + |b'|)_{i} =  \sum_{i=1}^{k} (|a_{[i]}| + |b_{[i]}|) \geq \sum_{i=1}^{k} (|a| + |b|)_{[i]} \geq \sum_{i=1}^{k}|a+b|_{[i]}$. So, $|a'| + |b'|$ weakly majorizes $a + b$.

So, by weak majorization theorems, for $p \geq 1$: $\sum_{i=1}^{k} (|a'| + |b'|)_{i}^{p} = \sum_{i=1}^{k} (|a|_{[i]} + |b|_{[i]})^{p} \geq \sum_{i=1}^{k}|a+b|_{[i]}^{p}$.

So, raising both sides to 1/p, we have that $\norm{|a'| + |b'|}_{p, k} \geq \norm{a+b}_{p, k}$.

Now, consider the vectors $a'_{(k)}$ and $b'_{(k)}$, which are a' and b' with $a_{i}' = b_{i}' = 0\ \forall i>k$. By the $\triangle$ inequality of the p norm, we have that $\norm{|a'| + |b'|}_{p, k} = \norm{|a'_{(k)}| + |b'_{(k)}|}_{p} \leq \norm{|a'_{(k)}|}_{p} + \norm{|b'_{(k)}|}_{p} = \norm{a'}_{p,k} + \norm{b'}_{p,k} = \norm{a}_{p,k} + \norm{b}_{p,k} $. \finish

\subsubsection{Matrix normness}
Matrix normness is seen using inequalities seen earlier:\\
$\sum_{i=1}^{k}\sw_{i}(AB)^{p} \leq \sum_{i=1}^{k}\sw_{i}(A)^{p}\sw_{i}(B)^{p}\leq \sum_{i=1}^{k}\sw_{i}(A)^{p}\sum_{i=1}^{k}\sw_{i}(B)^{p}$.

\section{Interesting ideas about Doubly Substochastic matrix Q}
This is unlikely to be covered.

Q is dbl substochastic iff B has dbl stochastic dilation S: make deficiency vectors $d_{r}, d_{c}$; get difference matrices $D_{r}  = diag(d_{r}), D_{c}  = diag(d_{c})$; get $S = \mat{Q & D_{r}\\ D_{c}^{T} & Q^{T}}$.

$\set{Q}$ equivalent to set of convex combos of partial permutation matrices: Dilate Q to S, get finite convex combo of $P_{i}$, take the convex combo of principal submatrices.

$Q \in C^{n \times n}$ is dbl substochastic iff $\exists$ dbl stochastic $S\in C^{n\times n}$ with $A \geq B$: Take any Q, get finite convex combo of partial permutation matrices; alter each to get permutation matrix; their convex combo is S.

% \section{Proofs about connections of majorization with stochastic matrices}
% If a majorizes $b \in R^{n}$, $\exists g \in R^{n-1}$ interleaved among a such that g majorizes $b' = b_{1:n-1}$.
% 
% Pf: True for 2; suppose $n \geq 2$; take $d \in R^{n-1}$ interleaved among a (ineq A) with $\forall k \in [1, n-2]: \sum_{i=1}^{k} d_{(i)} \leq \sum_{i=1}^{k} b_{i}$ (ineq B); take their set D; $a' = a_{1:n-1}\in D$, D bounded, closed: so compact; D convex; $\norm{a'}_{1} \leq \norm{b'}_{1}$; take $d' = argmax_{d \in D} \norm{d}_{1}$, set $g(t) = \norm{ta' + (1-t)d'}_{1}$ is continuous over [0,1]; if $\norm{d'}_{1} \geq \norm{b'}_{1}$, $\exists t: g(t) = \norm{b'}_{1}$. To show $\norm{d'} \geq \norm{b'}$: if all ineq B are strict, all of ineq A must be equalities: else $\norm{d'} \neq max_{d} \norm{d}$: then, $\norm{a_{2:n}}\geq \norm{b'}$; if some ineq in ineq B is equality, take r = largest k for which this holds; then $\sum_{i=1}^{r}d'_{i} = \sum_{i=1}^{r} b_{i}$, $\forall k> r: d'_{k} = a_{k+1}$; thence again get $\norm{d'} \geq \norm{b'}$.
% 
% If real $\ew$ majorizes a, $\exists$ real symmetric A with $a_{i,i} = a_{(i)}$, such that $\ew_{(i)}$ are ew of A.
% 
% Pf: By induction; assume true for n-1; Take $g \in R^{n-1}$ interleaved amongst $\ew$ which majorizes a; by ind hyp, take real symmetric $B = QGQ^{*}$ with diagonal a and ew g; can make $A' = \mat{G & y\\ y^{T} & \ga}$ with ew $\ew$; take $A = \mat{Q & 0 \\ 0 & 1}A'\mat{Q^{T} & 0 \\ 0 & 1} = \mat{B & Qy\\ y^{T}Q^{T} & \ga}$ with ew $\ew$ and diag a.
% 
% \subsection{Connection with stochatic matrices}
% b majorizes a iff $\exists$ doubly stochastic S: $a = Sb$. Lem 1: If b maj a, can make real symmetric $B = Q\EW Q^{*}$ with diag a and ew b; B is normal matrix, so can say a = Sb for doubly stochastic S. Lem 2: Take a=Sb; as PSP' remains stochastic with permutation matrix ops P, P', wlog assume a, b in ascending order; take $w_{j}^{(k)} = \sum_{i=1}^{k}s_{i,j}$, with $\sum_{i=1}^{n}w_{j}^{(k)} = k$; see $\sum_{i=1}^{k} (a_{i} - b_{i}) = \sum_{j=1}^{n}w_{j}^{(k)}b_{j} - \sum_{i=1}^{k} b_{i} + b_{k}(k - \sum_{j=1}^{n} w_{j}^{(k)}) \geq 0$.
% 
% So, by Birkhoff, b maj a iff $a = Sb = \sum_{i=1}p_{i}(Pb)$.
% 
% \section{Stochastic matrix proofs}
% (Birkhoff): $\set{S}$ = set of finite convex combos of permutation matrices $P_{i}$. Pf: If convex combo of $\set{P_{i}}$, S stochastic. Every $P_{i}$ is extreme point of $\set{S}$. Every non permutation stochastic matrix A is convex combo of stochastic matrices X = A + aB and Y = A - aB; where B and a are found thus: pick nz $a_{ij}$ in row with $\geq 2$ nz entries, then pick nz $a_{kj}$, then pick nz $a_{k,l}$ etc.. till you hit $a_{i',j'}$ seen before; take this sequence T, set $a = \min T$; make $\pm 1, 0$ matrix B by setting entries corresponding to alternate elements in T 1 or -1. $\set{S}$ is compact and convex set with $\set{P_{i}}$ as extreme points.
% 
% \section{Miscellaneous proofs}
% $\norm{.}$ is monotone ($|x|\leq |y| \implies \norm{x} \leq \norm{y}$) iff it is absolute.
% 
% Pf: if monotone, take $y = |x|$; as $|y| = |x|$ get absoluteness. If absolute: take x; for $\ga \in [0, 1]$, replace $x_{i} \to \ga x_{i}$ to get x', replace $x_{i} \to -x_{i}$ to get x''; $\norm{x'} = \norm{2^{-1}(1-\ga)x'' + 2^{-1}(1-\ga)x + \ga x} \leq 2^{-1}(1-\ga)\norm{x''} + 2^{-1}(1-\ga)\norm{x} + \ga \norm{x} = \norm{x}$; by repetition get $\norm{(\ga_{i}x_{i})} \leq \norm{x}$; thus, for $|x| \leq |y|$: $\norm{x} = \norm{(\ga_{i}e^{i\gt_{i}}y_{i})} = \norm{(\ga_{i}|y_{i}|)} \leq \norm{y}$.
% 
% \section{Acknowledgement}
% I am grateful to Prof. Dhillon for this opportunity to delve into a beautiful part of matrix theory.

\bibliographystyle{plain}
\bibliography{../../linAlg}

\end{document}
