\documentclass[10pt]{amsart}
\usepackage{amssymb, graphics, bm, verbatim, algorithm2e}

\input{../../macros}

\input{../../amsartMacros}

%opening
\title{Linear Algebra: Exam reference sheet}
\author{vishvAs vAsuki}

\begin{document}

\section{Triangular triangularization}
$|\Del A| \leq 3n\eps|L||U|$. \why $\tilde{L}\tilde{U} = A + \Del A, \frac{\norm{\Del A}}{\norm{L}\norm{U}} = O(\eps)$. \why

\subsection{With pivoting}
$L_{i,j} \leq 1; \norm{L} =O(1)$; let Growth Factor $\rho = \frac{\max |U_{i,j}|}{\max |A_{i,j}|}$; $\norm{U} = \rho \norm{A}$; so $\tilde{L}\tilde{U} = A + \Del A, \frac{\norm{\Del A}}{\norm{A}} = O(\rho \eps)$.

Maximal instability: $\rho = 2^{m-1}$.

\section{Computer Architecture}
If M = num bits in mantissa, E = num bits in exp. Stores $\pm (1+f/2^{M})2^{e - 2^{E-1} + 1}$. $\eps_{M} = 2^{-M -1} = 2^{-53}$.

\textbf{Single prec}: 1 $\pm$ bit, M= 23 bits, E = 8 bits. \textbf{Double prec}: M= 52 bits, E = 11 bits.

\section{Eigenvalue algs}
\subsection{Power iteration for $A^{T}=A$}
The series $v^{(i)} = \frac{A^{i}x}{\norm{A^{i}x}}$ and $l^{(i)} = r(v^{(i)})$ converge to eigenpair corresponding to largest ew $l_{1}, q_{1}$: as $x = \sum a_{i}q_{i}$.

So, Applying A repeatedly takes x to dominant ev.

\subsubsection{Convergence}
Linear convergence of ev. $\norm{v^{(i)} - \pm q_{1}} = O(|\frac{l_{2}}{l_{1}}|^{i}),\\
 \norm{l^{(i)} - \pm l_{1}} = \norm{v^{(i)} - \pm q_{1}}^{2}$.

\subsection{Inverse iteration}
ev of A and $(A-pI)^{-1}$ same, ew $l_{i}$ shifted and inverted to get ew $(l_{i} - p)^{-1}$. If p near $l_{j}$, using power iteration on $(A-pI)^{-1}$ gives fast convergence.

Good for finding ev if ew already known.

\subsubsection{Convergence}
Linear convergence of ev. \\
$\norm{v^{(i)} - \pm q_{j}} = O(|\frac{p-l_{j}}{p-l_{k}}|^{i}),\ \norm{l^{(i)} - \pm l_{1}} = \norm{v^{(i)} - \pm q_{j}}^{2}$.

\subsubsection{Alg}
Solve $(A-pI)w = v^{(k-1)}$; normalize to get $v^{(k)}$.

\subsection{Rayleigh quotient iteration}
Inverse iteration, where $l^{(i)} = R(v^{(i)})$ used as p (ew estimate).

\subsubsection{Convergence}
Cubic convergence of ev and ew. If $\norm{v^{(k)} - q_{j}} \leq eps$ when $|l^{(k)} - l_{j}| \leq O(\eps^{2})$. So $\norm{v^{(k+1)} - q_{j}} = \\
O(|l^{(k)} - l_{j}|\norm{v^{(k)} - q_{j}}) = O(\norm{v^{(k)} - (\pm q_{j})}^{3})$. $|l^{(k+1)} - (l_{j})| = O(\norm{v^{(k+1)} - q_{j}}^{2}) = O(|l^{(k)} - (\pm q_{j})|^{3})$.

Gain 3 digits of accuracy in each iteration.

\subsection{Simultaneous iteration for $A=A^{T}$}
Aka Block power itern. $\tuple{v_{i}}$ linearly independent; their matrix $V^{(0)}$. $\tuple{q_{i}}$ orth ev of A; cols of $\tilde{Q}$.

\subsection{Convergence}
If $|l_{1}| > .. > |l_{n}| \geq |l_{n+1}|..$, Orth basis of $\linspan{A^{k}v_{1}^{(0)}, .. A^{k}v_{n}^{(0)}}$ converges to $\linspan{q_{1}, .. q_{n}}$: take $v_{i} = \sum_{j}a_{j}q_{j}$, do power iteration.

\subsubsection{Alg}
Take some $Q^{0} = I$ or other orth cols, get $Z = AQ^{(k-1)}$; get $Q^{(k)}R^{(k)} = Z$. Defn: $A^{(k)} = (Q^{(k)})^{T}AQ^{(k)}$, $R'^{(k)} = \prod R^{(k)}$.

$A^{k} = Q^{(k)}R'^{(k)}$: By induction: $A^{k} = AQ^{(k-1)}R'^{(k-1)} = Q^{(k)}R'^{(k)}$.

\subsection{QR algorithm or iteration}
Not QR factorization. Get $Q^{(k)}R^{(k)} = A^{(k-1)}$; $A^{(k)}=R^{(k)}Q^{(k)} = (Q^{(k)})^{T}A^{(k-1)}Q^{(k)}$: Similarity transformation. Works for all A with distinct $|l_{i}|$; easy analysis for $A=A^{T}$.

Defn: $R'^{(k)} = \prod R^{(k)}$, $Q'^{(k)} = \prod_{k} Q^{(k)}$: same as $Q^{(k)}$ in Simult itern alg.

\subsection{Convergence for $A=A^{T}$}
Same as Simultaneous iteration starting with I. \\
$A^{k} = Q^{(k)}R'^{(k)}$: So, finds orth bases for $A^{k}$.

$A^{(k)} = (Q'^{(k)})^{T}AQ'^{(k)}$; $A^{(k)}_{i,i}$ are $R(Q'^{(k)}_{i})$; as $Q'^{(k)}_{i}$ converges, $A^{(k)}_{i,i} \to l_{i}$, off diagonal entries tend to 0; so approaches Schur factorization.

Linear convergence rate: $max_{j}\frac{l_{j+1}}{l_{j}}$.

\section{Hermitian matrix}
Aka Self Adjoint Operator. Symmetric matrix: $A=A^{T}$ generalizes to Hermitian matrix $A=A^{*}$; analogous to $R \subseteq C$. Skew symmetric matrix: $A= -A^{T}$, generalizes to skew Hermitian.

Any B $ = \frac{B+B^{*}}{2} + \frac{B-B^{*}}{2}$: Hermitian + Skew Hermitian.

\subsubsection{Positive definite matrix (pd) properties}
$x^{*}Ax \in R; x^{*}Ax > 0$. So, $A^{*}=A$.

Eigenvalues $l>0$: $lx^{*}x = x^{*}Ax > 0$. If $A = A^{*}$, all eigenvalues $l>0$, A is +ve definite.

\subsection{Cholesky factorization}
$A = R^{*}R$. As $A = LDU^{*}=UDL^{*}$, $L=U^{*}$. So, $A = LDL^{*} = LD^{1/2}D^{1/2}L^{*} = R^{*}R$; $d_{j,j} > 0$ as $a_{j,j}>0$; $r_{j,j} = \sqrt{d_{j,j}} >0$ chosen.

By SVD, $\norm{R}^{2} = \norm{A}$.

\subsubsection{Symmetric Elimination Algorithm}
Do Gaussian elimination + extra column ops to maintain symmetry at each step.

$A = 
\mat{a_{1,1} & A_{2,1}^{*}\\
A_{2,1} & A_{2,2}}
= \mat{1 & 0\\
\frac{A_{2,1}}{a_{1,1}} & I}
\mat{a_{1,1} & 0\\
0 & A_{2,2}-\frac{A_{2,1}A_{2,1}^{*}}{a_{1,1}}}
\mat{1 & \frac{A_{2,1}}{a_{1,1}}\\
0 & I}
=LDL^{*}
$. Get $R^{*}R$ by doing $LD^{1/2}$ at each step.

So, every Hermitian PDM has $R^{*}R$ factorization. It is also unique: $r_{j,j} = \sqrt{d_{j,j}} >0$ fixed by defn; it inturn fixes rest of R.

\subsubsection{Code and Opcount}
R=A; Repeat: do symmetric elimination on submatrix $R_{i+1,i+1}$; do $R_{i}^{*}/\sqrt{r_{i,i}}$. Only Upper part of R stored.

Opcount: $\sum_{k=1}^{m} \sum_{j=k+1}^{m} 2(m-j) \approx \frac{m^{3}}{3}$ flops.

\subsubsection{Stability}
By SVD: $\norm{R}_{2} = \norm{R^{*}}_{2} = \norm{A}_{2}$; $so \norm{R} \leq \sqrt{m}\norm{A}$ \chk. So, R never grows large. So, backward stable : get $\hat{R}*\hat{R}$ for perturbed A. Forward error in R large; but R and $R^{*}$ diabolically correlated.



% 
% \bibliographystyle{plain}
% \bibliography{../linAlg}

\end{document}

