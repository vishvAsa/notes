\documentclass[10pt]{amsart}
\usepackage{amssymb}
\usepackage{graphics}

\input{../../macros}

\newtheorem{thm}{Theorem}[subsection]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}

\theoremstyle{remark}
\newtheorem{defn}[thm]{Definition}
\newtheorem*{notation}{Notation}
\newtheorem{alg}[thm]{Algorithm}
\newtheorem{rem}[thm]{Remark}

%opening
\title{Linear Algebra: Answer to Homework 3}
\author{vishvAs vAsuki}

\begin{document}

\maketitle

\section{5.3}
 Consider the matrix A 
\begin{verbatim}
    -2    11
   -10     5
\end{verbatim}

\begin{rem}
I have verified my calculations with Matlab, and used it to perform some tedious arithmatic. However, I followed the strategy I would on paper.
\end{rem}

\subsection{a}
The real SVD of A is:
\begin{verbatim}

U =

   -0.7071   -0.7071
   -0.7071    0.7071


S =

   14.1421         0
         0    7.0711


V =

    0.6000   -0.8000
   -0.8000   -0.6000

\end{verbatim}

This was calculated by considering the fact that $AA^{*} = U \Sigma^{2}U^{*}$, and $A^{*}A = V \Sigma^{2}V^{*}$. So, U, V and $\Sigma$ can be found by calculating the eigenvalue decomposition of $AA^{*}$ and $A^{*}A$.

\textbf{Method used for finding eigenvalues and eigenvectors}: Eigenvalues of a square matrix B may be found by solving the polynomial corresponding to $det(B-lI) = 0$. Eigenvectors can then be found by solving for x in the equation $(B-lI)x = 0$.

For verification, the value for $\Sigma^{2}$ found in this manner was:
\begin{verbatim}
    50     0
     0   200
\end{verbatim}

\subsection{b}
The singular values are: 14.1421 and 7.0711.
The left singular vectors are the columns of V shown above.
The right singular vectors are the columns of U shown above.

\includegraphics{ellipse.jpg}
Shown in the figure are the vectors corresponding to the columns of V and the columns of matrix

\begin{verbatim}
US =

  -10.0000   -5.0000
  -10.0000    5.0000
\end{verbatim}

\subsection{c}
$\norm{A}_{1} = 16$.
This is obtained from the largest column sum of A.

$\norm{A}_{2} = 14.1421$.

This is obtained in the following manner:
Take vector in the unit ball x*=[a $\sqrt{1-a^{2}}$]. Then, we get (Ax)* = [-2a + 11$\sqrt{1-a^{2}}$ -10a + 5$\sqrt{1-a^{2}}$]. We then maximize the norm of this vector.

$\norm{A}_{\infty} = 15$: $\norm{Ax}_{\infty}$ is maximum when x* = [-1, 1].

$\norm{A}_{Frob} = \sqrt{\sum_{i=1}^{n}\sum_{j=1}^{n} A_{i,j}} = 15.8114$.

\subsection{d}
$A^{-1} = (U\Sigma V^{*})^{-1} = V\Sigma^{-1}U*$.
Upon performing this calculation, we find:
\begin{verbatim}
    0.0500   -0.1100
    0.1000   -0.0200
\end{verbatim}

\subsection{e}
Any single eigenvalue and eigenvector satisfy the equation $Ax=lx$. So, $(A-lI)x=0$. So, $det(A-lI)$.
This is the determinant of the matrix:
\begin{verbatim}
    -2-l    11
   -10     5-l
\end{verbatim}

So, we solve for l in the characteristic equation: $(-2-l)(5-l) + 110 = 0$ to get the eigenvalues $l_{1}, l_{2}$: $1.5000 \pm 9.8869i$.

\subsection{f}
Indeed, upon calculation, we verify that $l_{1} l_{2} = det(A) = 100$.

Indeed, upon calculation, we verify that $\sigma_{1} \sigma_{2} = |det(A)| = 100$.

\subsection{g}
The area of the ellipsoid: $\pi\sigma_{1} \sigma_{2} = 314.16$

\section{5.4}
$A \in C^{m\times m}$ has an SVD A=USV*. Find the eigenvalue decomposition of the $2m \times 2m$ hermetian matrix T = 
\[ \left[ \begin{array}{cc}
0 & A^{*} \\
A & 0 \end{array} \right]\] 

\subsection{Answer}
The answer as written earlier was incorrect.


\section{6.2}
Let E be the $m\times m$ matrix that extracts the 'even part' of an m-vector: Ex = (x+ Fx)/2 where F is the $m \times m$ matrix which flips $(x_{1}, \dots x_{m})^{*}$ to $(x_{m}, \dots x_{1})^{*}$. Is E an orthogonal projector or an oblique projector, or not a projector at all? What are its entries?
\subsection{Answer}
F is a matrix whose secondary diagonal is filled with 1 and whose non-secondary diagonal elements are filled with 0.

E = 0.5(F+I). All the entries on its primary and secondary diagonals are non zero. All other elements are 0.

In case m is even, all primary and secondary diagonal entries are 0.5.

In case m is odd, all primary and secondary diagonal entries are 0.5, except for $E_{\ceil{m/2}, \ceil{m/2}}$, which is 1.

In either case, $E=E^{2}$; so E is a projector. Also, in either case, $E=E^{*}$; so E is an orthogonal projector.

\section{7.5}
Let A be an $m\times n$ matrix $(m\geq n)$, and let $A=\hat{Q}\hat{R}$ be a reduced QR factorization.


\begin{notation}
ith column of A is represented by $a_{i}$. $r_{i}^{*}$ represents the ith row of R.
\end{notation}


\subsection{1}
\begin{thm}
A has rank n if and only if all the diagonal entries of $\hat{R}$ are nonzero.
\end{thm}

\begin{proof}
$A=\hat{Q}\hat{R}$.

If A has rank n, the vectors $\set{a_{i}}$ span an n dimensional space. Suppose for the sake of contradiction that $r_{i,i} = 0$ for some i. Then it would imply that the space spanned by i linearly independent vectors $a_{1} \dots a_{i}$ is spanned by only i-1 vectors $q_{1} \dots q_{i-1}$, which is an absurdity. Hence, if A has rank n, $\forall i: r_{i,i} \neq 0$.

Suppose that $\forall i: r_{i,i} \neq 0$. Now we consider the space spanned by the matrix $A=\hat{Q}\hat{R}$. All vectors in $\set{q_{i}}$ are mutually $\perp$, and therefore independent. So, the rank of $\hat{Q}$ is n, and $\set{q_{i}}$ span an n dimensional space. For every $q_{i}$, there is atleast one vector $a_{i}$ (especially $a_{i} = Qr_{i}$) which depends on $q_{i}$ (that is, has a component in the direction of $q_{i}$). Hence, the columns of A span a space whose dimensions are at least n. But, as A is an $m\times n$ matrix $(m\geq n)$, its rank is at most n. So, A has rank n.
\end{proof}

\subsection{2}
\begin{thm}
$\hat{R}$ has k nonzero diagonal entries for some k with $0\leq k <n$. Then, $rank(A) \geq k$.
\end{thm}
\begin{proof}
$a_{i} = Qr_{i}$. More specifically, every $a_{i}$ can be expressed as a linear combination of vectors $q_{1} \dots q_{i-1}$. Suppose that $r_{i} = 0$ for some i. Then $a_{i}$ can be expressed as a linear combination of vectors $q_{1} \dots q_{i-2}$. 

Proof to be completed.
\end{proof}

\section{Question}
\sf (Gram-Schmidt Process) Let
 \[
  v_{1} = \left[ \begin{array}{c}1 \\ \varepsilon \\ 0 \\ 0 \end{array} \right], \;\;\;
  v_{2} = \left[ \begin{array}{c}1 \\ 0 \\ \varepsilon \\ 0 \end{array} \right], \;\;\;
  v_{3} = \left[ \begin{array}{c}1 \\ 0 \\ 0 \\ \varepsilon \end{array} \right],
 \]
 and $\varepsilon$ be such that $fl(1 + \varepsilon^{2}) = 1$.
 \begin{enumerate}
   \item[\underline{a}] Apply Classical Gram-Schmidt and show that the
   computed vectors \underline{are not} numerically orthogonal, i.e., computed
   vectors have dot products much larger than $\varepsilon$.
   \item[\underline{b}] Apply Modified Gram-Schmidt and show that the
   computed vectors \underline{are} numerically orthogonal, i.e., computed
   vectors have dot products $= O(\varepsilon)$.
 \end{enumerate}

\subsection{Classical gram Schmidt:}
We simulate the following code:
\begin{verbatim}
for j=[1:n]
    v = A(:,j);
    for i=[1:j-1]
        R(i,j) = Q(:,i)'*A(:,j);
        v=v-R(i,j)*Q(:,i);
    end
    R(j,j)=norm(v,2);
    Q(:,j)=v/R(j,j);
end
\end{verbatim}
$v_{1}^{*}$ = [1 e 0 0].

$v_{2}^{*}$ = [1 0 e 0].

$v_{3}^{*}$ = [1 0 0 e].

When j=1, i=1:
$v_{1}^{*}$ = [1 e 0 0].
$R(1,1) = (fl(1 + e^{2}))^{0.5} = 1$.
$q_{1}^{*}$ = [1 e 0 0].

When j=2:
$v_{2}^{*}$ = [1 0 e 0].
When i=1:
R(1,2) = 1.
$v_{2}^{*}$ = [0 -e e 0].
$R(2,2) = e\sqrt{2}$.
$q_{2}^{*} = [0\ -2^{-0.5}\ 2^{-0.5}\ 0]$.

When j=3:
$v_{3}^{*}$ = [1 0 0 e].
When i=1:
R(1,3) = 1.
$v_{3}^{*}$ = [0 -e 0 e].
When i=2:
R(2,3) = 0.
$v_{3}^{*}$ = [0 -e 0 e].
$R(3,3) = e\sqrt{2}$.
$q_{3}^{*} = [0\ -2^{-0.5}\ 0\ 2^{-0.5}]$.

Now, we check orthogonality:
$q_{1}^{*}q_{2}= -2^{-0.5}e$.
$q_{1}^{*}q_{3}= -2^{-0.5}e$.
$q_{3}^{*}q_{2}= 2^{-1}$.

The computed vectors \underline{are not} numerically orthogonal, i.e., computed vectors have dot products much larger than $\varepsilon$.

\subsection{Modified gram Schmidt:}
We simulate the following code:
\begin{verbatim}
for j=[1:n]
    R(j,j)=norm(V(:,j),2);
    Q(:,j)=V(:,j)/R(j,j);
    for i=[j+1:n]
        R(j,i) = Q(:,j)'*V(:,i);
        V(:,i)=V(:,i)-R(j,i)*Q(:,j);
    end
end
\end{verbatim}
$v_{1}^{*}$ = [1 e 0 0].
$v_{2}^{*}$ = [1 0 e 0].
$v_{3}^{*}$ = [1 0 0 e].

When j=1:
$v_{1}^{*}$ = [1 e 0 0].
$R(1,1) = fl((1 + e^{2})^{1/2}) = 1$.
$q_{1}^{*}$ = [1 e 0 0].
When i=2:
R(1,2) = 1.
$v_{2}^{*}$ = [0 -e e 0].
When i=3:
R(1,3) = 1.
$v_{3}^{*}$ = [0 -e 0 e].

When j=2:
$v_{2}^{*}$ = [0 -e e 0].
$R(2,2) = e2^{0.5}$.
$q_{2}^{*} = [0\ -2^{-0.5}\ 2^{-0.5}\ 0]$.
When i=3:
$R(2,3) = e2^{0.5}$.
$R(2,3)q_{2}^{*} = [0\ e\ e\ 0]$.
$v_{3}^{*} = [0\ -2e\ -e\ e]$.

When j=3:
$v_{3}^{*} = [0\ -2e\ -e\ e]$.
$R(3,3) = e6^{0.5}$.
$q_{3}^{*} = [0\ -2(6^{-0.5})\ -6^{-0.5}\ 6^{-0.5}]$.


Now, we check orthogonality:
$q_{1}^{*}q_{2}= -2^{-0.5}e$.
$q_{1}^{*}q_{3}= -2(6^{-0.5})e$.
$q_{3}^{*}q_{2}= 0$.

The computed vectors \underline{are} numerically orthogonal, i.e., computed vectors have dot products $= O(\varepsilon)$.
% \bibliographystyle{plain}
% \bibliography{../linAlg}


\end{document}
