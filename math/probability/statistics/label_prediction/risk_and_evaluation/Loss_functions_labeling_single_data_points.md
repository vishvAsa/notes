+++
title = "Loss functions: labeling single data points"

+++
Different measures of goodness/ error functions are appropriate for different scenarios.

## Loss functions: vector labels
Loss functions in this case are often defined to penalize deviation from the actual label symmetrically.

See loss functions in regression section.

## Loss functions for classification
Below we mainly consider the loss functions \\(l(\hat{y}, y)\\) (where \\(\hat{y}\\) estimates label y) which are without regularization to account for prior belief. Loss functions used in regression can directly be applied to this case: Eg: like squared difference.

Good loss functions are realistic, maybe convex and smooth too - so that the corresponding empirical risk minimization problem (for picking classifiers) they lead to is tractable.

## 0/1 loss
\\(l() = I[\hat{y} \neq y]\\). The corresponding risk will be \\(Pr(\hat{y} \neq y) = E[I[\hat{y} \neq y]]\\), the misclassification rate.

The minimal risk classifier has the form: \\(h(X) = \argmax_L Pr(L|X, S)\\). The maximization procedure is called **Decoding** in the context of probabilistic graphical models.

However, the corresponding empirical risk minimization is non-convex, is NP hard and cannot is not approximable to a constant fraction of goodness. So, auxiliary loss functions are used.

### Minimal risk: Binary classification
Aka Bayes risk. Suppose that data is generated by the model specified by specifying the pdfs \\(f_{X|Y=1}\\), \\(f_{X|Y=0}\\), \\(Pr(y)\\). Then, the best possible classifier is one which has accurate knowledge of the generative model, and even this classifier, in general, has a non zero risk. Its risk is given by \\(\sum_y Pr_{x: Pr(y|x) \geq 1/2}(\lnot y)\\), or by \\(E_x[\min_y Pr(y|x)]\\).


### Connection to log loss risk: binary classification
Whenever \\(I[\hat{y} \neq y]\\), we know that \\(Pr_t(y|x) \geq 1/2\\). So, \\(E_{x, y}[-\log Pr_t(y|x)] \geq (-\log Pr_t(y|x)) Pr(\hat{y} \neq y) \geq \log 2 Pr(\hat{y} \neq y)\\). Hence, upper-bound on log loss risk is also an upper bound for \\(Pr(\hat{y} \neq y)\\).

## Log loss
Suppose that our predictor is based on the model \\(f_{Y|X,T=t}\\). Then log loss is \\(-log f_{Y|X,T=t}(y|x)\\). This punishes the model for assigning low probabilities to an observation. Minimizing log loss corresponds to maximum likelihood estimation. The corresponding risk is \\(E_{x, y}[-log f_{Y|X,T=t}(y|x)]\\).
