<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>&#43;Extra data on Vishvas&#39;s notes</title>
    <link>https://vishvAsa.github.io/notes/math/probability/statistics/label_prediction/extra_data/</link>
    <description>Recent content in &#43;Extra data on Vishvas&#39;s notes</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://vishvAsa.github.io/notes/math/probability/statistics/label_prediction/extra_data/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>With alternate labels</title>
      <link>https://vishvAsa.github.io/notes/math/probability/statistics/label_prediction/extra_data/labels_from_other_viewpoints/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://vishvAsa.github.io/notes/math/probability/statistics/label_prediction/extra_data/labels_from_other_viewpoints/</guid>
      <description>&lt;h2 id=&#34;data-point-neighborhood-approach&#34;&gt;Data point Neighborhood approach&lt;/h2&gt;&#xA;&lt;p&gt;Make data pts vs data pts matrix which measures similarities; filled using labels from various, usually similar, viewpoints. Thence predict label from target viewpoint. Eg: Amazon: &amp;lsquo;others who bought this also bought that&amp;rsquo;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;collaborative-filtering&#34;&gt;Collaborative filtering&lt;/h2&gt;&#xA;&lt;h3 id=&#34;latent-factor-approach&#34;&gt;Latent factor approach&lt;/h3&gt;&#xA;&lt;p&gt;Assume small number of latent random variables, which combine together to form various view points and data points, cause the labels.&lt;/p&gt;&#xA;&lt;h4 id=&#34;low-rank-factorization&#34;&gt;Low rank factorization&lt;/h4&gt;&#xA;&lt;p&gt;Let values of latent RV behind data pt j be \(v_{j}\), behind viewpoint i be \(u_{i}\); then get \(Y_{i,j} = u_{i}^{T}v_{j}\). Want to have few random variables, so want \(Y = U^{T}V; U \in R^{q \times N}; V \in R^{q \times D}\), a low rank factorization of Y. Thence fill missing values.&lt;/p&gt;</description>
    </item>
    <item>
      <title>With extra unlabelled points</title>
      <link>https://vishvAsa.github.io/notes/math/probability/statistics/label_prediction/extra_data/additional_unlabelled_points/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://vishvAsa.github.io/notes/math/probability/statistics/label_prediction/extra_data/additional_unlabelled_points/</guid>
      <description>&lt;p&gt;Assume points belonging to the same class cluster together, using unlabeled data with labeled data, you can draw better decision boundaries around clusters belonging to various class: probability density is important in classification.&lt;/p&gt;&#xA;&lt;h2 id=&#34;generative-approaches&#34;&gt;Generative approaches&lt;/h2&gt;&#xA;&lt;p&gt;\tbc&lt;/p&gt;&#xA;&lt;h2 id=&#34;label-propagation-on-graphs&#34;&gt;Label propagation on graphs&lt;/h2&gt;&#xA;&lt;p&gt;Make the similarity graph G=(V, E) of the n points, \(\set{X_i}\). Some nodes \(X_T\) have labels, others don&amp;rsquo;t have labels; let \(k\) be the indicator vector with \(k_i = [i \in T]\); take K = diag(k).&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
