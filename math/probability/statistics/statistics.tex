\documentclass[oneside, article]{memoir}

\input{../packages}
\input{../packagesMemoir}
\input{../macros}

%opening
\title{Statistics}
\author{vishvAs vAsuki}

\begin{document}
\maketitle

\tableofcontents

\part{Introduction}
\chapter{Themes}
Statistics is the study of the collection, organization, and interpretation of data.

\section{Statistical inference}
Aka inductive statistics, inferential statistics.

\subsection{Inference from data subject to randomness}
This is the process of drawing conclusions from data that are subject to random variation, for example, observational errors or sampling variation. Probability theory provides us a way of handling this uncertainty.

Also see the Inference survey for other scenarios where random variation is lesser.

\subsection{Modeling problems}
Some problems are very closely tied to particular applications; eg: many vision problems, etc.. Here, the art is often in picking the right model; ie in proper abstraction. Eg: Is it better to use linear dimensionality reduction or manifold learning? Is it better to cast it as a regression problem or a classification problem? Often, simple models work surprisingly well. Also see comments in the `probabilistic models' survey.

The decision about what you model and what you don't is important. Often, prior work is advanced by designing a model which modeled more aspects of the problem than previous models.

Once the problem is modeled, one must find a suitable estimation algorithm.

See also comments

\subsection{Solving Modeled problems}
This is algorithm design and analysis. See algorithms survey too.

Here, you take already-modeled well-specified problems, and try to find upper and lower bounds on resources (time, sample points, memory, randomness .. ) required for various estimation tasks.

Note that finding upper bounds on resources often involves designing new estimation algorithms to solve the problem. This is often done by modifying an algorithm for a closely related model.

Eg: Pattern recognition in a Stream: very limited memory and processing time. Eg: sensors in bridges.

\subsubsection{Degrees of abstractness}
\paragraph*{Domain specific models}
Usually solutions tied to particular domains tend to have short term, shallow impact. But, sometimes the general modeling technique and the estimation algorithm in them can be generalized to other domains, leading to greater impact. Also, the problem solved itself may be very important from a non-data-mining point of view; like the astrophysics or biology points of view.

\paragraph*{Abstract models}
Impact is often intangible: it improves the intuition/ understanding of problems by practitioners. Good solutions to important abstract problems tend to have long-term, deep impact.

\section{Research effort}
Read many papers; Critique the problems posed and the solutions presented; alter problems and argue why it is interesting; criticize solutions, develop better solutions for original problem.

\subsection{Analysis of efficiency and complexity}
For learning theory research, see colt ref. Statistical learning theory: find convergence rate in estimating parameter.

\subsection{Modeling and Experimentation}
\subsubsection{Purpose}
Modelling the problem often requires exploration and experimentation.

Show that the techniques, even if mathematically well motivated, works well in practice. If it fails, in what cases does it fail?

\subsubsection{Avoiding unnecessary work}
One must approach the problem in a mathematically principled, disciplined manner. Doing so can avoid expending much unnecessary effort. One must take advantage of prior research and predetermined thought patterns.

\subsubsection{Degree of experimentation}
This depends highly on abstractness of the problem. If the problem is very specific to some domain, eg: vision or social network analysis etc; experimentation is much. One has to check the goodness of one's model with the ground truth.

\subsubsection{Data preparation}
Relevant mainly in case of domain-specific research; Usually not present in the case you try to answer more abstract questions.

Huge effort often involved in data-preparation.

\subsection{Following research}
ICML, KDD, NIPS. Application conferences: CVPR etc.. Theory: archiv, colt.

Data mining conferences tend to be more about applications and clustering than machine learning conferences.


\section{Software}
Rapidminer, R, Matlab are frequently used.

Keep up with developments in efficient software to solve common problems: for example, by joining the R package mailing list.

\chapter{Decision theory}
Certain elements common to probability density estimation, distribution structure learning (including classification) etc.. can be studied within the abstract framework of decision theory.

\section{Agents: Actions, policies}
How should an agent act in the face of uncertainty, given some observations? Our objective is to find good decision procedures for the agent.

Like a game against nature. See game theory reference for adversarial games.

\subsection{State and parameters}
\subsubsection{State space}
The state of nature changes, possibly in response to actions made by an agent. The agent must act optimally in some sense in the presence of uncertainty.

\subsubsection{Parameter space T}
Some parameters $\gth \in T$ describe the state of nature (the ground truth), especially as it relates to the action-space of an agent; and they are unknown.

\subsubsection{State transitions}
Some parameters describe state transitions. These are aka population parameters. In some problems, we are to estimate some properties of a population by drawing random samples from it. This unknown parameter/ pattern of the population, $t = g(\gth)$, is a constant. Eg: mean, variance of heights.

\subsection{Action space A}
Given any change in its knowledge, the agent can act. Its action is chosen from the action space $A = \set{a}$. Actions can change the state of nature and the parameters which specify it.

\subsubsection{Common examples}
In case of density estimation, given the data point $x$, $A$ could be the choice of the density function $f_{\hat{t}}(x)$ as a function of the estimated density $\hat{t}$. In this case, the state of nature is usually not affected by our guess about its parameters.

In hypothesis testing, it could be the probability of the observation $x$, which is a function of the hypothesis chosen earlier.

In case of classification, $A$ would correspond to the various possible labellings of a data point $x$.

\subsection{Goodness of actions}
\subsubsection{Loss function L}
$L:T\times A \to \Re$. You pick the loss function to best model the situation faced by the agent, and also for mathematical tractability - this is often the modeler's job.

\subsubsection{Examples}
Squared error between prediction and label in case of classification.

$-f_{\hat{t}}(x)$ in case of probability distribution modeling.

\subsection{Decision procedure d}
\subsubsection{Mapping observation D to actions}
Denote an observation by the random variable $D$. Decision procedures take as input $o \in ran(D)$ and return an action $a \in A$. Their pre-image can be often be replaced with sufficient statistics drawn from observations.

Note that the observation $D$ to which a decision procedure is supposed to respond \exclaim{is different from the training set of prior observations which may have been used to learn $d$!}

\subsubsection{Deterministic procedures}
Deterministic decision procedures make decisions which are a function of the new observation.

A deterministic decision procedure is $d:ran(D) \to A$.

\subsubsection{Randomized procedures}
A randomized decision procedure takes decisions with some randomness, given new observations. So, the randomized procedure is in the convex hull of certain decision procedures.

If the decision procedure randomized, range is the set of random variables with range $A$.

\paragraph{Combining decision procedures}
Randomized procedures can often be written as a 2-step meta-procedure with step 1: random choice of decision procedures, and step 2: application of the decision procedure chosen.

\subsubsection{Examples}
In case classification, the classification rule can be considered to be a decision procedure.

In the case of distribution estimation, given a point (the observation), the estimated pdf can be considered to be the decision procedure.

In case of hypothesis testing, a certain hypothesis can be considered to be the decision procedure.

\section{Risk R of decision procedure d}
\subsection{Motivation and setting}
We want to choose a decision procedure from $H_n$ (so named because it is often learned after looking at a sample of size $n$). We want to pick decision procedure with least expected loss; that motivates the following definition. This is essentially the problem of statistical inference.

\subsection{Risk}
$R:T\times\set{d} \to \Re$. $R(\gth,d) = E_{D}[L(\gth,d(D))]$. In case $d$ is randomized, the expectation is also over random bits used by $d$.

In case of the estimation or prediction problem, if you use squared distance as $L$, you get the bias - variance decomposition: See estimation section.

\subsection{Uncertain ground truth case}
\subsubsection{The need}
Suppose that $\gth$ is constant every time you apply the chosen $d$. Even so: When you pick $d$, you don't know $\gth$, the ground truth.

Also, maybe $\gth$, which decides the goodness of response, changes with the actions taken by the decision procedure.

For example, in classification, $\gth$ corresponds to the label $y$ corresponding to the example $x$ seen by the classifier. Even if the classifier knew the distribution $f_{Y|X}(y|x)$ generating the examples, it cannot in general be certain about $y$ given $x$.

\subsubsection{Frequentist and epistemological approaches}
Rooted in the two distinct but overlapping interpretations of probability, we observe two overlapping approaches to statistical inference. The frequentist approach tries to deal with uncertainty by relying on sampling, while only partially restricting the hypothesis space.

Bayesian/ epistemological inference approach tends to posit and quantify prior beliefs about the ideal decision, and use this together with sample evidence to reach a conclusion.

\subsubsection{Prior beliefs about ground truth}
We can model the uncertainty in the ground truth using a probability distribution $\gth \distr P_T$ over $T$; so now take\\ $R'(d) = E_{D, P_T}[L(\gth,d(D))] = \int_T E_D[L(\gth,d(D))]P_T(\gth) d \gth$.

As probability theory is being fully used to model uncertainty, this is called Bayesian risk evaluation.

\subsubsection{Prior beliefs about best d}
For every $\gth$, there is a best decision procedure $d(D)$. So, alternatively, one model uncertainty in what the best decision procedure is directly as $d \distr P'(d)$ and look at $\gth = f(d_1)$. Then, one can write the risk \\$R'(d) = E_{D, P'}[L(f(d_1),d(D))] = \sum_{d_1} E_D[L(f(d_1),d(D))]P'(d_1)$.

\subsubsection{Additive form}
Let $g(d; d_1) = E_D[L(\gth,d(D))] = E_D[L(f(d_1),d(D))]$. \\Then, $R'(d) = E_{P'}g(d; d_1)$. If the loss $g(d; d_1)$ had a sharp drop around $d$, or if $P'(d)$ was highly concentrated around $d_1$, we could approximate this as $R'(d) = g(d; d_1) P'(d)$.

For convenience for use in optimization problems, one often takes logarithms on both sides to get: $R''(d) = \log g(d; d) + \log P'(d)$.

This roughly motivates the following form used in practice: $R''(d) = R(d) + l r(d)$, where $L$ is loss function, r() is regularizer. The regularizer, r() ensures that the prior belief about $\gth$ is taken into account while evaluating the risk of decision procedure $d$.

Example: Suppose that we are estimating $\mean$: eg: avg age. But, prior belief is that it is $\mean_0$. Thence a decision procedure: $R(\mean, \hat{\mean}(D))  = 0.2 \mean_0 + 0.8 n^{-1}\sum X_i$.

\paragraph{Strict restrictions}
Finding $\min_d R(d) + l r(d)$ is same as $\min R(d) : r(d) \leq c_l$. This is a strict restriction on the choice of $d$, unlike a more flexible prior assumption about the pdf of $d$ which is the hallmark of epistemological/ Bayesian inference; where irrespective of a low prior probability assigned to a certain $d$, the weight of evidence could lead to choosing that $d$.

\subsection{Geometry of R}
Visualize $R(\gth, d)$ with $\gth \in T$. Make a function space, label each dimension with some $\gth \in T$. For any fixed $d$, $R(\gth, d)$ is a vector in this space. Given set $S = \set{d}$, can get set $S'$ of all randomized decision procedures derivable from these. Risks $R(S')$ are points in the convex hull of $R(\gth, d) \forall d\in S$. In 2 d: it is a convex polygon: only straight edges.

\subsection{Empirical risk}
Aka empirical avg loss. $\hat{R}(d;D) = n^{-1}\sum_{x_i \in D} L(\gth, d(x_i))$. You have: $R(d) = E_D[\hat{R}(d;D)]$.

Where we have prior beliefs about the nature of the best $\gth$ or $d$, empirical risk is used with a regularization function to define an alternate risk function. See Bayesian risk evaluation for details.

\subsection{Minimal risk}
\subsubsection{Definition}
The lowest (expected) risk any decision procedure $d$ can achieve is often called the Bayes Risk.

It is not necessarily $0$. For example, suppose that we were considering classification of points drawn from the distribution $f_{Y|X}(y|x), f_X(x)$. Even if the decision procedure had complete knowledge of $f_{Y|X}(y|x), f_X(x)$, its risk in general would not be 0 as there is the possibility that $\hat{y} = argmax_y f_{Y|X}(y|x)$ is the wrong labeling for $x$.

\subsubsection{Risk Consistency of d}
Let $D_i$ be data of $i$ sample points. Then does $d(D) \to^p \gth$ [In case of the parameter estimation task]? Or does $R(\gth, d) \to^p minRisk$?

\subsubsection{Risk persistence}
Ground truth $\gth$. Let $\hat{t}$ be the chosen decision procedure. \\Take $\hat{\gth} = \argmin_{t\in H_n} R(t)$. How close is $R(\hat{t})$ to $R(\hat{\gth})$?

As $n \to \infty$, usually want $\hat{t} \to \gth $ or better: $R(\hat{t}) - R(\gth)\to 0$. But it may not be possible as decision procedures can't get to it, or maybe $\gth$ changes with n.

Approximation error $R(\hat{\gth}) - R(\gth)$. Then does $d$ at least minimize estimation error $R(\hat{t}) - R(\hat{\gth})$?

\subsubsection{In High dimensional setting}
Let the dimensionality of data-points be $p$. Suppose $p>>n$ and we still want a decision procedure which works well. What is risk persistence here? Hence a new notion: As $n \to \infty$, we still want check if $R(\hat{t}) - R(\hat{\gth}) \to 0$; while $p$ scales with $n$.


\section{As a POMDP}
POMDP's are well suited to abstract active learning; but it is informative to consider it more generally.

\subsection{State and observation}
The ground truth, or parameter $T$ corresponds to the state of the world in case of a POMDP. The observation $D$ gives some clue about the current state.

\subsection{Transitions}
Unlike active learning problems, often the state transitions and observations thereof are independent of the agent's actions. So, the agent often has no control over input distribution. 'Parameters' in decision theory may confusingly include both those describing the ground truth and those describing the transition function.

\subsection{Action and Loss}
The loss function corresponds to the reward which depends on current state and the action taken.

\subsection{Policy}
The decision procedure $d$ corresponds to the POMDP's policy; except that the belief state is not explicitly considered for our purposes.

\subsection{Risk vs value}
In the case of MDP's, it is common to assess policies using the total expected reward over a possibly infinite horizon, so a discounting factor $g \in [0, 1]$ is needed. [Expected] risk of a decision procedure focuses on the expected reward.

\chapter{Procedure for choosing decision procedure d}
\section{Overview, motivation}
Some discipline is necessary in order to avoid common errors which consume effort and resources. The below describes steps common to many problems including density estimation and classification.

Formulate the problem in decision theoretic terms properly - pick an appropriate 'risk/ loss function' (possibly incorporating prior belief), a candidate feature set. Then fix a model family (possibly after a literature survey) and develop a model selection procedure. Then use empirical risk estimation using cross validation in order to avoid overfitting to training data. Repeat.

\section{Picking the right hypothesis space H}
\subsection{Hypothesis space H}
Often we constrain ourselves to picking a decision procedure from a certain limited set, the hypothesis space $H$.

$H$ is usually specified by imposing a structure which must be satisfied by all decision procedures in it; so it may also be called a model family.

$H$ can be further restricted by specifying the loss function appropriately: this is described elsewhere.

\subsection{Motivation, Factors to consider}
Picking a limited hypothesis space $H$ is often necessary because considering all possible hypotheses may be computationally infeasible. Also, we may want $H$ to be convex, so that we can then efficiently minimize empirical risk over it.

Prior beliefs/ requirements about the ideal decision procedure also influence the choice of $H$.

Constraining $H$ just for the sake of avoiding overfitting is considered elsewhere.

\subsubsection{Approximation vs estimation error tradeoff}
$\hat{\gth} = \argmin_{t\in H_n} R(t)$. If $H_n$ large, we can reduce approximation error $|R(\param) - R(\hat{\gth})|$, but the decision procedure learning process will find it tougher to get to $\hat{\gth}$ in terms of time, memory or number of samples required. In other words, estimation error $|R(\hat{\gth}) - R(d)|$ high. \exclaim{Larger hypothesis classes generally outperform smaller ones - if given enough data.}

If $H_n$ too small, approximation error is high, estimation error low. \exclaim{When training data is limited, smaller hypothesis class is better. But, Misleading if assumption about the best decision procedure is false.}

\subsection{Parameters}
A decision procedure may be concisely specified using a small number of parameters $\param$ - perhaps as a function over sufficient statistics extracted from observations ($S =s$). Other decision procedures require $O(|s|)$ of parameters to be specified.

Hypothesis classes are called non-parametric or parametric based on the decision procedures which constitute them. Decision procedure learning methods may similarly be parametric or non-parametric, based on the hypothesis class used.

Non-parametric hypothesis classes are usually bigger and more flexible.

Many of the advantages and disadvantages associated with using parametric and non-parametric classes therefore follow from comparison of the use of large and small $H$ described earlier.

\subsubsection{Common structural assumptions}
Sparsity. Low matrix rank. Block structure: Eg: zeros appear in blocks/ entire rows/ columns; row sparsity.

\section{Loss function choice}
Choosing a risk or loss function enables us to compare decision procedures.

Constraining $L$ in order to avoid overfitting during empirical risk minimization is considered elsewhere.

\subsection{Evaluation}
When ultimately evaluating a decision procedure, one should use the loss function without prior belief biases (unlike the one possibly used in empirical risk minimization). This actual risk of the decision procedure chosen is called the generalization error.

In research reporting: Final evaluation/ test data must never be used in fitting (hyper)parameters for this results in overfitting to the test/training set combination.

\section{Theoretically find the best d}
\subsection{Best d for fixed \htext{$\theta$}{ground truth}}
Use geometry of $R$ described earlier. If $\gth$ were fixed, then the best $d$ would correspond to the point of the convex hull $R(S')$ with least $R(\gth, d)$: This will always be a corner.

\subsection{Minimum expected (Bayesian) risk}
Pick $d$ with min expected risk.

\subsubsection{Geometry}
Let dim(T) be finite.\\ If $\pi(\gth)$ is the prior, then risk is $\sum_i \pi(\gth_i)R(\gth_i,d) = c$ where $d$ is the best response for a certain $\gth$. So, this is a certain point in the 'base' of the convex hull $R(S')$. So set of risks of such decision procedures are represented in the 'base' faces of the convex polyhedron.

Visualize this in 2D.

Comparing geometries: Every bayesian risk minimizing decision procedure with full support is also admissible.

\subsection{Choose an admissible procedure}
d is admissible if $\nexists d': [\forall \gth R(\gth, d') \leq R(\gth, d)]$, $\exists \gth: [\forall d': R(\gth, d') < R(\gth, d)]$.

\subsubsection{Geometry}
Take convex hull $R(S')$. Given $d$, to find a d' which 'dominates' $d$ for some fixed $\gth_i$, while being as good as $d$ for all other $\gth$: drop a line through $d$ which is $\perp$ axis corresponding to $\gth_i$. The lowest point in the hull along this line dominates every other point in the hull.

So, possible admissible procedures correspond to base of the convex hull, except faces which are perpendicular.

So, every admissible procedure is a min Bayes risk procedure for some prior.

\subsection{The adversarial setting: minimax procedure}
See game theory. Ye assume that the adversary knows what you pick, assume that adversary is rational, and choose the $\hat{d} = \inf_d \sup_{\gth} R(\gth, d)$.

\subsubsection{Geometry}
Take convex hull $R(S')$. Find the 'lowest' point in the hull which meets the ball $\norm{\gth}_\infty = c$ (max norm: looks like a square).

\subsection{Compare risk profiles of d over range of \htext{$\gth$}{ground truth}}
$R(\gth, d)$ can change for different values of $\gth$. So, can plot $R(\gth, d)$ vs $\gth$ curves for various $d$ and compare.

\section{Empirical risk minimization}
Aka model selection, training the model, learning the parameters (if the hypothesis class is parametric).

Solve: $\hat{t} = argmin_t \hat{R}(t, D)$. So, you try to get best fit to training data from $H$.

If both $H$ and $\hat{R}(t, D)$ are convex, you have a convex optimization problem.

\subsection{Fit to D vs generalization ability}
A problem with empirical risk minimization is the limited amount of data available. This means that minimizing $\hat{R}(t, D)$ may be quite different from minimizing $R$.

\subsubsection{Training error}
$\hat{R}(t, D)$, derived only from a loss function, not incorporating any bias about the decision procedure, is aka the training error. It is useful in diagnozing underfitting and overfitting.

\subsubsection{Overfitting}
Sometimes, an algorithm may pick a decision procedure which minimizes empirical risk on the training data/ training error (ie maximizes fit to $D$), but has high risk $R$ (low generalization ability).

As in this case, the decision procedure varies highly with the training data $D$, it is called the 'high variance' case.

\subsubsection{Underfitting}
The decision procedure chosen by the algorithm may have a high generalization error and high training error. This could be due to the non-convexity of the empirical risk function.

This case is aka the 'high bias case'.


\subsection{Overfitting and model complexity}
\subsubsection{Model complexity}
Complex model classifies training data excellently but performs poorly on test data. So, the complex model is unstable.

So, look for simple hypothesis, in Occam's razor style; minimize description length. Keep low model complexity/ level of specialization while still fitting the training set well.

\subsubsection{Limiting number of parameters}
As number of data points $N$ increases, this problem becomes less. Rough heuristic: $N \geq 5$ or 10 times the number of adaptive parameters in model.

\subsubsection{Polynomial regression example}
Eg: Fitting a polynomial with parameters $w$ to a sine curve: what should the degree $d$ be?

For a given $N$, generalization ability increases (error on test set decreases) with $d$, then levels off until $d$ = N-1 when it sharply drops (error increases): overfitting: simple interpolation occurs there, and magnitude of coefficients $(w_{i})$ sharply increases there. So use regularization, with $w_{0}$ omitted in regularization term.

\subsection{Avoiding overfitting}
\subsubsection{Altered risk function}
When we have prior belief about the best decision procedure, it can be incorporated into the expression for the risk function to be used for empirical risk minimization.

Take $t_n = argmin_t \hat{R}(t,(X_i)) + \gl r(t)$, where $\hat{R}$ is the original risk function, $r()$ is regularizer and $\gl$ is the regularization parameter.

\subsubsection{Altered loss function}
The altered risk function can be interpreted as arising from an altered loss function constructed especially for empirical risk minimization - one incorporating the prior belief.

\subsubsection{Other derivations}
This alternate objective function may also be derived by other means: for example using the Bayesian/ Schwartz or Akaiki Information Criteria (BIC or AIC) in case of parametric density estimation.

\subsubsection{Hyper-parameters}
Hyper-parameters such as the regularization parameter represent the prior belief about the decision procedure. They could include choice between model families.

\subsection{Statistical efficiency analysis}
If $L$ is convex, differentiable, strongly convex around t' wrt certain $C$ (see vector spaces ref). Then take $A = H_n$ as the hypothesis space and B a certain space outside it, so that decomposability holds wrt r(): $\forall a\in A, b\in B: r(a + b) = r(a) + r(b)$. Then, pradeep etal bound distance $d(t', t_n)$.


\subsubsection{Accuracy of empirical risk estimate}
\core  If the $H$ is small and if $\hat{t}$ is learned using many samples, the empirical risk is close to actual risk.

If $G$ finite, can bound $Pr(\sum_i(L(t, x_i) - E[L(t, x)]) \geq \eps)$ using martingale concentration bound (Hoeffding). Can then apply union bound to get bound on $Pr(\sup_t \sum_i(L(t, x_i) - E[L(t, x)]) \geq \eps)$. [The union bound is very intuitive when probability is viewed as a measure.]

If $G$ infinite, take $\eps$ covering wrt $\norm{.}_\infty$ to get $G_\eps$, take union bound over that. Thence bound: $N(\eps, G, \norm{.}_\infty) exp(\frac{-2n\eps^{2}}{9 B^{2}})$.

These same principles are used in 'Occam razor' and the 'VCD Occam razor' in computational learning theory.

\subsubsection{Bound deviation from optimum h: bound empirical risk error}
\core  If the $H$ is small and if $\hat{t}$ is learned using many samples, the risk due to $\hat{t}$ is close to that of $\gth$.

Thence, $R(\hat{t}) - R(\gth)= R(\hat{t}) - \hat{R}(\hat{t}) + \hat{R}(\hat{t}) - \hat{R}(\gth) + \hat{R}(\gth) - R(\gth) \leq 2 \sup_t (R(t) - \hat{R}(t))$: as $\hat{R}(\hat{t}) - \hat{R}(\gth) \leq 0$ from empirical loss minimization.

Thence, $$R(\hat{t}) - R(\gth) \leq 2n^{-1}\sup_t \sum_i(L(t, x_i) - E[L(t, x)]).$$ So, want to bound $Pr(\sup_t \sum_i(L(t, x_i) - E[L(t, x)]) \geq \eps)$. Take function space $G = \set{L(t,.) \forall t}$.

\subsection{Check generalization ability}
Aka validation.

\subsubsection{Motivation}
Given limited data and having picked a decision procedure $\hat{t}$, one is often motivated to approximate the generalization ability. Then, one can judge whether over-fitting or under-fitting has happened.

\subsubsection{General procedure}
One partitions the limited data in some way into training data and validation data, which is aka 'hold out set', a stand-in for the data the decision procedure will ultimately be evaluated on.

Then, learn parameters of different models on first part of training data and test for generalization ability by measuring the empirical loss (without any regularization for prior biases about the decision procedure) on the hold-out set.

\subsubsection{Multiple rounds for robustness}
For the sake of robustness, this can be repeated many times, and the empirical risk associated with a given model is taken to be the average of empirical risks measured during these runs. The model with the least average empirical risk is considered the best.

\subsubsection{Cross validation}
A particular variant is k-fold cross validation. Here, the data is divided into $k$ equal sets. There are $k$ different validation rounds. In each round, one of these $k$ sets is designated the 'hold-out' set, while the union of the remaining sets is designated the training set.

Cross-Validation is better than simple validation because the latter could involve picking parameters overfitted to a particular training/ test set combination.

\subsection{Tuning risk minimization}
\subsubsection{Diagnosis}
High generalization error is often due to overfitting or underfitting. It is easy to distinguish the two by comparing the training error (t) with the cross validation error (v) (both of which should not include the prior belief/ regularization component). In the former (high bias case), $t \approx v$ and both are high. In the latter (high variance) case, $v >> t$.

According to the diagnosis, one can determine whether more suitable hypothesis spaces and feature sets should be tried, or whether one can achieve better results using just the currently available data.

\subsubsection{Picking hyperparameters}
Theoretically, these would come from the risk function where some weight is given to the prior belief.

In practice, we often do not know what weight should be assigned to penalty for violation of prior belief. So, $\gl$ is often learned (ie: the strength of the belief is calculated) using validation.

\section{Combining Decision procedures}
Rather than picking $d \in H$ with the minimum risk, it is sometimes possible to combine multiple decision procedures in $H$ to produce a randomized decision procedure with lower risk. This combination may be based on a 'bestness' probability distribution over $H$. Eg: combination of several classifiers or rankers to produce superior classifiers/ rankers.

\section{Offline vs online learning}
If new information is provided one at a time, the agent may want to keep improving its decision rule after each new observation. This is online learning.

\section{Interpreting the decision procedure selected}
This is important if the decision procedure is being learnt in order to figure out the laws of nature: what genes are triggered in a certain situation? Models learnt from training data is often very hard to interpret/ match with intuition.

Sparse solutions are often easier/ more rational to interpret.

\chapter{Sample}
A statistical population exists. Thence, a sample of $N$ points is drawn, from which we attempt to infer properties of the population.

\section{Properties}
Sample statistics are considered elsewhere.

\subsection{Sample bias}
Pick 2 people: he is either Indian or Chinese; but there exist over 180 other countries.

\subsection{Completeness, accuracy of examples}
It is possible that, some points are not completely specified. For a certain point $X$, the component $X_j$ may not be specified. This ambiguity allows optional auxiliary data to be considered.

\subsection{Independence of data points}
In some cases, $\set{X_{i}}$ are usually iid $\distr D'$.  Eg: Predicting whether a visitor to a store is likely to be a customer or a criminal.

In other cases, the input points $\set{X_i}$ are not identically distributed.

\subsubsection{Sequential}
Firstly, sequential data points can be ordered: $(X_i)$. In addition, $X_i \nperp X_{i-1}$.

So, data points need not be considered sequential merely because of the presence of a Time/ Position feature. Eg: Identifying words with spelling mistakes in a sentence.

\subsubsection{Adversarial}
Or they may be chosen adversarially : See game theory ref.

\subsubsection{Active choice}
Or, as in the case of active learning problems, the learner can take actions to change input distribution. Reinforcement learning is considered in the AI survey.

\subsection{Labeling of the data.}
Some features, aka the label, may be a (unknown) function of the others. Thence, deducing dependence of label on other features is the prediction problem.

\subsection{In case of small sample}
Usually insufficient data to guess shape of distribution; can only see large effects: so need large sample to see small effects; only extreme outliers stand out remarkably.

\subsubsection{Few high dimensional data-points}
$D >> N$: so the parameter space to explore is huge: maybe 1 parameter for each feature. Yet want to be able to estimate the parameters generating the observations. This is only possible as there is low dimensional intrinsic structure in the data; can do well using a small hypothesis space.

\subsubsection{Examples}
Take the brain activity vs neuronal activity matrix. Activity-levels of millions of neurons is feature for each data-point. Brain activity is highly defined by a very small number of spiking neurons.

Gene expression vs exterior condition matrix.

Social networks: Individual activity vs group action matrix.

\part{Simplification, exploratory analysis}
Whatever the statistical problem is, exploratory analysis is often the first step in solving the problem.

\chapter{Data exploration}
One often studies the empirical distribution of the data and estimates the central tendency, range, median, mode, characteristics of outliers, number of missing values. It is further described in the distribution structure learning part.

One may also cluster the data.

\chapter{Data preparation}
\section{Motivation}
Data preparation often involves massaging attribute values to fit the requirements of operators/ models. For example, some operators cannot handle continuous values, others cannot handle polynomial values, some have problem with missing data.

\section{Changing the range}
Ploynominal features can be converted to binomial features using a binary representation.

Binominal features can be treated as numeric inputs.

Continuous valued features can be converted to discrete valued features by binning them. The bins may be defined by a regular grid on the range, or irregularly to ensure roughly equal cardinality.

\section{Dealing with missing values}
Continuous missing values can be replaced with average values  or with 0's as appropriate under the circumstances. Nominal missing values can be replaced with the mode.
Or missing value may be imputed using various predictive models.

\section{Saling, centering, allowing bias}
\subsection{Motivation}
To interpret the model parameters learned using the training data (eg: to see which input component has more predictive value, and to deduce its bias), it is often beneficial to apply a transformation to the input vector.

In the discussion below, we assume that the input has undergone this transformation.

\subsection{Centering to 0}
Let $\mean$ be the mean of the given training points $\set{\hat{X^{(i)}}}$. Thence derive the transformation $X = \hat{X} - \mean$.

\subsection{Scaling}
Let $S$ be a diagonal matrix with diagonal elements $1/\stddev_i$, where $\stddev_i$ is the standard deviation of the input component $i$. Thence, get the transformation $SX$. 

\subsection{Constant variable}
We assume that the input RV $X$ includes the constant 'variable' $X_0 = 1$. 

\chapter{Finding a simpler, more useful representation of the data}
\section{Feature extraction}
Use some $\ftr$: d-dim X formed by input vars $\to$ m-dim Feature space; $\ftr$ is a vector function. Basis of feature space are the 'basis functions' $(\ftr_{i})$. \tbc

\subsection{Dimensionality reduction}
Remove irrelevant/ less relevant features, merge duplicate features. Eg: synonymous words in documents.

\subsubsection{Importance}
Treating duplicate features as if they were different harms ability to classify or cluster data points. Irrelevent features also harm predictive ability, by acting as noise.

\paragraph*{Data visualization}
Project high dimensional data to 2D or 3D space. Also see graph drawing/ embedding.

\subsubsection{Extant of dimensionality reduction}
The best dimension should match the  inherent dimensionality of the data.

If there are not-very-useful features, there will be a steep drop in the performance gain by including those features. Often this is not the case, and the choice is rather arbitrary.

\section{Using Kernel function to implicitly map data to a feature space}
See vector spaces ref for info about kernels.

\subsection{The Kernel trick}
Aka Kernel substitution.

Can reformulate hypothesis model, perhaps a predictor, and any associated optimization objective such that input vector $x$ enters only in terms of inner products. Then can substitute this product with kernel function $k(x, x')$; implicitly using some $\ftr(x)$; can try out various kernels to achieve good performance. Can also use +ve semi-definite d*d kernel matrix K to define inner product in feature space.

\subsection{Use}
Learning problem; classification or clustering; may be easier to solve in some kernel. Eg: Can't find linear separator for 2 concentric rings of points in original space, but can do so in a high dimensional space.

Save space: If features are high dimensional, may not want to explicitly form feature vectors. Just use the kernel function.

\section{Casting data into a graph}
Take data $\set{X_{i}}$; use similarity measure $S(X_{i}, X_{j})$; Cast into weighted similarity graph: Set $w_{u, v} = f(S(u, v))$. Casting data into a graph simplifies the clustering task: there you only care about the distance.

\subsection{\htext{$\eps$}{eps} neighborhood graph}
$(u, v) \in E \equiv d(u, v) < \eps$, \\
where $d(u, v) = g(S(u, v))$. Usually unweighted as $\eps $ distance does not matter.

\subsection{k nearest neighbor directed graph}
$(u, v) \in E \equiv v \in KNN(u)$.

\subsection{Fully connected graph}
Useful if S(u, v) like Gaussian kernel: $e^{-\norm{u-v}^{2}/(2\stddev^{2})}$.

\section{Find similar features}
\subsection{With Clustering}
Coclustering does this.

\subsection{Find covariance of various features}
\subsubsection{Sample points and their mean}
Each sample pt $X^{(i)}$ is a $d$ dim vector. The mean, $E[X] = \mean$; sample mean is $\bar{X} = n^{-1} \sum_i X^{(i)}$.

\subsubsection{Sample Covariance matrix C}
Estimates $\covmatrix$. So, $C_{i,j} = (n-1)^{-1}\sum_k (X_i^{(k)} - \bar{X_i})(X_j^{(k)} - \bar{X_j})$ estimates $cov(X_i, X_j)$. So, $C = (n-1)^{-1}\sum_i (X^{(i)} - \bar{X})^{T}(X^{(i)} - \bar{X})$.

So, $C$ is symmetric +ve semi-definite; and if $n<d$, $C$ is singular.

\section{Identify a good metric}
\subsection{Generalized interpoint distance}
Aka Mahalonobis distance. Take covariance matrix C, $E[X] = E[Y] = \mean$; X, Y sample points. $d(X, Y) = ((X-Y)^{T}C^{-1}(X-Y))^{1/2}$. Negates bias due to having features which mostly say the same thing while finding distance.

\chapter{Dimensionality reduction}
\section{General motivations}
Perhaps one wants to find closest vectors to a given vector - perhaps for the purpose of executing the nearest neighbor algorithm.

Computational efficiency - as in the case of Most variable subspace identification (PCA).

Noise reduction - it could be that many of the features in a vector are not very informative.

\section{Latent factor modeling}
\subsection{Problem}
Here, one derives generative models to describe  affinity of one discrete variable (say $U$) with another (say $V$): eg: features and objects, documents and words.

In the process, one derives low dimensional representation of both these entities. 

\subsubsection{Matrix view}
Suppose that you are given a matrix $A$ whose entires represent affinities between two types of entities. One may need to find a model the strength of this association which is robust to noise in the observations/ which is succinct.

\subsubsection{Linear model}
A linear model would be: $A_{ij} = \dprod{u_i, v_j}$, where $u_i$ and $v_j \in R^k$ are low dimensional representations of entities $i$ and $j$. Precisely, given that those entities are represented by $A_{i,:}, A_{:,j}$, we want to find $U^{k}, V^{k}$ such that $A \approx U_k^{T} V_k$.

\subsubsection{Motivation}
So the motivations described for dimensionality reduction in general apply. In addition, this can be used to find unobserved affinities between entities.

\subsection{Matrix factorization by SVD}
Taking the top singular vectors, we know that: $\norm{A - U_k \SW_k V_k ^T}_2^2$ is minimzed.

This is a very common form of 'Latent semantic analysis'.

\subsection{Non-negative matrix factorization}
There could be other constraints such as requiring that the lower dimensional representations be non negative. Non negative matrix factorization is considered elsewhere.

\subsection{Probabilistic modeling}
Probabilistic models for affinities between the two entity types are considered in the probabilistic models survey.

\section{Linear dimensionality reduction}
A linear map (Eg: a projector) is used on the data to reduce dimensions. So, ratio of distances amongst points are preserved.

\subsection{Most variable subspace identification}
Aka Principal component analysis (PCA).

\subsubsection{Problem}
Suppose that you have a $m \times n$ data matrix $A$ of $n$ $m-$dimensional data points. Suppose that we want a very good low dimensional representation of these data points. We have a bunch of points, and we want to pick $k < m$ orthogonal axes along which the data has the greatest variability.

One can visualize most of the data points as being contained in a m-dimensional hyperellipse, whose top $k$ axes we want to use to represent the data points in a low dimensional space.

\subsubsection{Motivation}
Suppose one needs to compare a $v \in R^m$ with all column vectors in $A$ - as in the case of object matching. Computing $n$ inner product is an $O(m^2n)$ operation. By dimensionality reduction, we want to turn this into an $O(k^2n)$ operation. Besides, the latter may be a way to overcome noise - ie ignore unimportant features.

\subsubsection{Preprocessing, problem statement}
One can always get a matrix $B$ with rows centered around the mean. This will enable us to write the covariance matrix as $C = n^{-1}BB^{T}$. We want to find a linear transformation $L$ such that $\norm{LL^{T} - BB^{T}}_F^2$ is minimized.

\subsubsection{Solution}
Using the properties of the SVD. $B = U \SW V^{*}$, $BB^{T} = U\SW^{2}U^{*}$. This covariance matrix can be approximated by $nC_k = U_k\SW_k^{2}U_k^{*}$. So, from $\EW_k = \SW_k^{2} = U_k^{*}BB^{T}U_k$, we see that the low dimensional transformation of $B$, which ensures that most of the variability in the data is preserved, is given by the orthogonal map $U_k^{*}B$.

\subsubsection{Best target dimension k}
Sometimes, can pick the top $k$ ev, so that there is a steep gap betweek $\ew_q$ and $\ew_{q+1}$.

\subsubsection{Comments}
\exclaim{So, the top ew/ sw of the covariance matrix define the subspace of highest variability.}

\subsection{Factor analysis}
Model thus: $x - \mean = Lf + \eps$. $x \in R^{d}, L \in R^{d*k}$. Reducing $x$ to $k$ dimensional vertex. Ideally, cov(f) = I, $f \perp \eps, E[f] = 0$. $L$ is loading matrix, f are factors.

Arrange centered data points as columns of X-M. Then, trying to factor this into LF.

\section{Supervised linear dimensionality reduction}
Labelled data. Done for classification etc..

\subsection{Linear discriminant analysis}
\subsubsection{The problem}
(Fisher) Want to do dimensionality reduction for the purpose of classification. If $x \in R^{d}$, want to project data points to some k-1 dimensional hyperplane; what is the best hyperplane to do this?

You want to maximize after-projection inter-class scatter: separate means widely, but minimize after-projection intra-class scatter.

Usually k-1 dim hyperplane desired: then you can find a hyperplane between every pair of classes. So you project with $y = W^{T}x$ for orthogonal $d*(k-1)$ dim W.

\subsubsection{The solution}
Before projection: Take $S_{T} = \sum_{x}(x - m)(x - m)^{T}$: total scatter; $S_{W} = \sum_{i=1}^{k} \sum_{x \in C_{i}}(x - m_{i})(x - m_{i})^{T}$: within class scatter matrix; $S_{B} = \sum_{i=1}^{k}n_{i}(m_{i}-m)(m_{i}-m)^{T}$: between class scatter matrix. So, $S_{T} = S_{W} + S_{B}$. Note: $S_B$, the sum of $k$ rank 1 matrices, has rank k-1; so only k-1 ew's are non 0. If $k$ = 2, can define differently: $S_B = (m_1 - m_2)(m_1 - m_2)^{T}$.

After projection scatters will be: $S_{W}' = W^{T}S_{W}W, S_{B}' = W^{T}S_{B}W$. Find $\max_{W} \frac{|W^{T}S_{B}W|}{|W^{T}S_{W}W|}$ or maybe $\max_{W} tr((W^{T}S_{W}W)^{-1}(W^{T}S_{B}W))$, with W having (k-1) independent columns. Maximized by top k-1 generalized ev: see linear algebra ref.

If $S_{W}$ is invertible, same as ev problem $S_{W}^{-1}S_Bx = \ew x$: solution is the top k-1 ev; but matrix is assymetric, so finding ev harder. If just projecting to a line, can also get problem $S_{W}^{-1/2}S_{B}S_{W}^{-1/2}x = \ew x$. See linear algebra ref.

\section{Non-linear dimensionality reduction}
\subsection{Kernel PCA}
Represent data in feature space corresponding to some kernel, use PCA.

\subsection{Manifold learning}
Consider the swiss roll/ tissue paper roll dataset. This is a 2 dimensional manifold in a 3-dim space.

\tbc

\subsection{Measuring goodness}
How well are k-NN properties preserved? Neighborhood rank preserved even if magnitude is not.

\chapter{Cluster data points}
\section{The clustering problem}
Given $N$ points, want $k$ clusters. Often, $k$ is not known.

\subsection{Use}
Summarizing data is important for generalization, understanding future data points.

Supplying labels to unlabeled data; thence classifying new data-points. Eg: Face recognition using Eigenfaces. Use to estimate distribution support and thence in novelty detection.

\subsection{Criteria: Continuity vs compactness}
Consider a starfish: The continuity critierion will identify the 5 arms as 5 clusters, but the comactness criterion will fail. Consider points produced from two gaussians with distinct centers: the compactness criterion appears better.

\subsection{Extensions}
Coclustering data-points along with features.

\subsubsection{Find Non-redundant clusterings}
Aka Disparate clusters. Want clusterings based on unrelated criteria. Eg: can classify people based on sex, race etc.. $k$ Disparate clusters can be thought of as lying in $k$ orthogonal subspaces.

\subsubsection{With background clutter}
Consider clustering stars in the night sky to find galaxies. Important for clustering algorithm to ignore clutter.

\subsection{Evaluation of clustering}
In case the $k$ true labels are known: Just use ways of evaluating classification. Can always do this by generating artificial data.

\subsection{Challenges}
\subsubsection{Curse of dimensionality}
As dimensions/ features grow, more difficult to group similar things together; there could be irrelevant or noisy features.

Use dimentionality reduction techniques.

\subsubsection{Number of clusters k}
The actual number depends on the data. Choice of $k$ is mostly based on heuristics.

Some methods take $k$ as input, others discover $k$ themselves.

\subsubsection{Identify important clusters}
Clusters which matter remain distinct at different levels of coarsity.

\paragraph*{Cluster visualization}
Look at clusters in 2D or 3D at varying coarsity levels to identify important clusters.

\subsection{Approaches}
\subsubsection{Views of the data}
Visualize data points as points in feature space.

Or as 'data points (X) vs features (Y)' matrix A. In case features are binary (eg: word present or absent), get contingency table P(X, Y), an estimate of the joint probability matrix.

\subsubsection{Density estimation}
\paragraph*{Parametric}
Try to find distribution of data in the input space. Usually fit some multimodal distribution: the different modes define different cluster representatives.

Eg: Fit a mixture of $k$ Gaussians centered at various spots in the input space.

Advantage of having a parametric model: can extrapolate what the data will look like with more sampling.


\paragraph*{Non-Parametric}
Can do non-parametric density inference, then do density shaving: ignore points with low density, identify the modes.

\paragraph*{Relative performance}
Non parametric methods usually perform better, given enough data-points.

\subsubsection{Centroid based vs agglomerative clustering}
Described fully in another section.

Centroid based clustering methods are usually fast: the costliest step is often assignment of points to clusters: O(kn). Agglomerative methods usually involve comparison between all cluster-pairs for the purpose of agglomeration; so are slower: $O(n^{2})$.

Centroid based methods require $k$ to be known before hand, they need initial points. Agglomerative methods find varying number of clusters: from $N$ to 1; it is up to the user to know where to stop - this can be difficult.

\section{Agglomerative clustering}
Bottom up approach. Start off with $N$ clusters: 1 for each point; pick nearest pair of clusters; merge them; repeat till you have $k$ clusters.

\subsection{Intercluster metrics}
Distance between means. Or between closest pair of points: tends to produce elongated clusters: clustering by continuity criterion. Or between farthest pair of points: clustering by compactness criterion. These correspond to the 2 clustering criteria.

\section{Centroid based clustering}
Use centroids or cluster representatives to cluster.

\subsection{Mean: Best cluster representative wrt Bregman div}
Given Bregman div $d_f$ based on convex function f. Show by easy algebra that $n^{-1}\sum_i d_\gf(X_i, z) - n^{-1}\sum_i d_\gf(X_i, \mean) = d_f(z, \mean) \geq 0$. So, mean is best cluster representative wrt 2 norm: 2-norm is also a bregman divergence.

\subsection{k means clustering}
\subsubsection{Objective}
\paragraph*{As minimizing within cluster scatter}
Find $k$ centroids, make $k$ partitions (Vorinoi tesselations) in the input space: \\
$S' = (S_{i}') = argmin_{S} \sum_{i=1}^{k} \sum_{x_{j} \in S_{i}} d(x_{j}, \mean_{i})$.

\paragraph*{As maximizing inter-cluster scatter}
Use scatter matrices/ scalars \\
$S_B, S_W, S_T$ as in LDA. For any bregman divergence: $S_T = S_B + S_W$. Implicitly tries to maximimze $S_B$ : Between cluster scatter.

\subsubsection{Algorithm}
Start of with $k$ vectors $(m_{i}^{0})$ as means of $(S_{i})$; At time t, you have: $(m_{i}^{t})$. Reassign all points to the $S_{i}^{t}$ corresponding to the closest $m_{i}^{t}$; calculate new means $(m_{i}^{t+1})$ as the centers of these $(S_{i}^{t})$; repeat.

If $d$ is any Bregman div, $k$ means minimizes this at each iteration: Alg finds better clustering, Mean is best cluster representative.

Time: O(kndt): very fast.

\subsubsection{As low rank factorization with alternating minimization}
Let X be the $d \times n$ data matrix. Doing $X \approx MW$, where M is the $d \times k$ means matrix, and $W \in \set{0, 1}^{k\times n}$ denotes membership. For strict partitioning, there is the constraint $w_i \in I_k$.

So, k-means is equivalent to solving $\min_{M, W} \norm{X - MW}_F$ by alternatively minimizing M with W fixed, and W with M fixed subject to constraints on W.

\subsubsection{Drawbacks and extensions}
This is a greedy algorithm, does local minimization of the objective function. Highly sensitive to initial conditions. Can end up with empty clusters, with bad initialization. So, have varied initialization strategies.

Fails to cluster data points arranged in concentric rings. So use the kernel trick here: get kernel $k$ means.

\subsection{With GMM}
You model the observed data with $k$ normal distributions $(D_i)$ specified by means $(\mean_i)$, covariances $(\covmatrix_i)$, prior probabilities of a point being generated by $(D_i)$, aka mixture weights, $(p_i)$. If you find the best parameters for this model, you can assign points to the cluster associated with the $D_i$ most likely to have generated it.

Start with some guessed parameters. Repeat the following steps iteratively: a] Assign each point to the $D_i$ most likely to have generated it: do $\min_i (\log p_i) (x-m_i)^{T}\covmatrix_i^{-1}(x-m_i)$: same as minimizing a weighted 'Mahalonobis distance'; $(\log p_i)$ can be seen as shrinking $D_i$ appropriately by acting on $\covmatrix_i^{-1}$. Let $n_i$ count the points assigned to cluster i. Update $p_i = n_i/n$. b] Update parameters: $\mean_i = n_i^{-1}\sum_{x_j \in i} x_j; \covmatrix_{i, (j, k)} = \frac{1}{n_i-1}(x_i,j - \mean_j)(x_i,k - \mean_j)$: note unbiased estimater used in estimating covariances.

\subsubsection{Generalizing k-means}
k-means corresponds to GMM clustering with each Normal Distribution in the model constrained to being spherical: at each step you assign the point to the cluster with $\mean = \argmin_{m_i} (x-m_i)^{T}(x-m_i)$.

\subsection{With non-negative matrix factorization}
Let X be the $d \times n$ data matrix. Doing $X \approx MW$, where $M \in R_+^{d \times k}$ means matrix, and $W \in R_+^{k\times n}$ denotes membership strength.

\subsection{Finding the initialization points}
Bad initialization points can lead to bad clusters, good ones lead to good clusters. Density estimation useful here. \tbc



\section{Co-clustering}
Cluster both the rows and columns of P simultaneously. Thus dealing with duplicate/ synonymous/ irrelevant features simultaneously while clustering.

\subsection{Objective: Information loss minimizing}
Find maps $C_{X}: \set{x_{1}, .. x_{m}} \to \set{\hat{x_{1}}, .. , \hat{x_{k}}};\\
 C_{Y}: \set{y_{1}, .. y_{m}} \to \set{\hat{y_{1}}, .. , \hat{y_{l}}}$. $(C_{X}, C_{Y})$ is a coclustering; yields corresponding joint distribution matrix $p(\hat{X}, \hat{Y})$. Best co clustering has minimum mutual information loss: $\min I(X;Y) - I(\hat{X},\hat{Y}) = K(p(X,Y)||q(X,Y))$ where $q(X, Y) = p(\hat{X}, \hat{Y})p(X|\hat{X})p(Y|\hat{Y})$.

\subsubsection{The monotonic optimizer}
$K(p(X,Y)||q(X,Y)) = \\
K(p(X,Y,\hat{X},\hat{Y})||q(X,Y,\hat{X},\hat{Y}))$.\\
For any clustering, q preserves marginals and conditionals: \\
$q(\hat{x}, \hat{y}) = p(\hat{x}, \hat{y}), q(x, \hat{x}) = p(x, \hat{x}), q(y, \hat{y}) = p(y, \hat{y}), p(x) = q(x), p(x|\hat{x}) = q(x|\hat{x})$ etc..

So, $K(p(X,Y,\hat{X},\hat{Y})||q(X,Y,\hat{X},\hat{Y})) =\\
 \sum_{\hat{x}} \sum_{x : C_{X}(x) = \\
 \hat{x}} K(p(Y|x)||q(Y|\hat{x}))$; \\
 similar form in terms of $K(p(X|y)||q(X|\hat{y}))$.

Thence information theoretic coclustering alg: Start with $C_{X}^{(0)}, C_{Y}^{0}$; at step t, for each row $x$,  set $C_{X}^{(t+1)}(x)= argmin_{\hat{x}} K(p(Y|x)||q^{(t)}(Y|\hat{x}))$; recompute distributions $q^{(t+1)}$; at step t+2 similarly recluster columns finding local minima; repeat. This minimizes objective function monotonically. Experiments on document clustering tasks show better clustering than 1D clustering.


\section{Using Graph clustering}
For graph clustering methods, see graph theory ref.

\part{Distribution structure learning}
\chapter{Problems}
\section{Conditional distributions and notation}
Got observations of events: RV $X$ took values $\set{x_{i}}$, deduce/ model the process causing those events. In general, we want to model the conditional distributions $f_{X_r|X_{\lnot r}}$. Often, we use the alternate notation $Y = X_r$ and $X = X_{\lnot r}$.

\section{Connection to modeling marginal density}
Note that $X_{\lnot r}$ may be empty, so that marginal/ unconditional distribution modeling - which is estimating $f_X(X)$ - is a special case of conditional distribution modeling.

Techniques which are suitable for modeling conditional distributions can be directly applied to such special cases. Techniques specialized for modeling unconditional distributions can be applied to modeling one conditional distribution $f_{X_r|X_{\lnot r} = x_{\lnot r})}$ at a time.

\subsection{Problem structure}
For discrete probability distribution $p$, valid values of $p$ form the probability simplex. Also, expectation is linear in $p$, variance is concave in $p$.  So, can specify many convex optimization problems using these constraints.

\chapter{Estimating parameters}
\section{Estimate parameters using statistics}
The distinction between choosing parametric and non-parametric approaches are considered in the decision theory section.

\subsection{Statistic, estimator}
A statistic $\hat{t} = \hat{g}(X)$ is a function of the sample X; an observable random variable. When it is used to estimate some parameter, it is called an estimator. t can be estimated by estimating $\gth$.

\subsubsection{Point estimation of the parameter}
If $\hat{t}$ tries to approximate $t$, it is an estimator.

\subsection{Distribution of a statistic}
Aka Sampling distribution. Standard deviation of sampling distribution called standard error.

Find by manual calculation of probabilities of values of $\set{X_{i}}$; or by simulation or assume $\set{X_{i} \distr N(\mean, \stddev^{2})}$, using mgf $n^{-1}\sum Y_{i} \distr N(\mean, \stddev^{2}/n)$.

\subsection{Summarize Central tendency}
Sample and population expectation \\
($\bar{X}, \mu = E[X]$), median (m with $F(m) = 1/2$), mode.

\subsubsection{AM, GM, HM}
Suppose we have $n$ numbers $(a_i)$. Arithmetic mean is $n^{-1}\sum_i a_i$: the name reminisces the arithmetic series. Geometric mean is $\prod_i a_i^(n^{-1})$, and harmonic mean is $n^{-1}(\sum_i a_i^{-1})^{-1}$.

Using weights $p_i \in [0, 1]$ such that $\sum_i p_i = 1$, these quantities can be generalized to define weighted arithmetic, geometric and harmonic means.

$\mu \geq GM \geq HM$. This and other inequalities are considered in the complex analysis survey.

\subsubsection{Modeling accuracy}
If one is trying to use the sample mean to quantify the 'average' phenomenon, it is important to pick the right random variable.

From Tao: 'For instance, consider the question of what the population density of the United States is. If one does a simple average, dividing the population of the US by the area of the US, one gets a density of about 300 people per square mile, which if the population was spread uniformly, would suggest that each person is about 100 yards from the nearest neighbour. Of course, this does not conform to actual experience. It is true that if one selects a random square mile patch of land from the US at random, it will contain about 300 people in it on the average. However, not all such patches are equally inhabited by humans. If one wants to know what what density the average human in the US sees, rather than the average square mile patch of land, one has to weight each square mile by its population before taking an average. If one does so, the human-weighted population density now increases to about 1400 people per square mile - a significantly different statistic.'

\subsubsection{Combining arithmetic means of subpopulations}
(From Tao's blog.) When combining averages of small sub-populations together to form an average of the combined population, one needs to weight each sub-average by the sub-population size in order to not distort the final average. If the sub-populations being averaged over vary, this can then lead to Simpson's paradox.

Eg: it turns out that in most departments, women had a slightly higher success rate in their applications than men, but in the university as a whole, women had a lower success rate. The ultimate reason for this was that women tended to apply to more competitive departments, which lowered their overall average success rate. 

\subsection{Other statistics and parameters}
\subsubsection{Summarize variability or dispersion}
Sample and population Variance \\
($S^{2}, \stddev^{2}$), standard deviation ($S, \stddev$). Also, range: max - min.

\subsubsection{Order statistics}
max or nth order statistic $X_{(n)}$, min or first order statistic $X_{(1)}$, kth smallest sample point $X_{(k)}$.

$f_{X_{(k)}}(x) = \frac{n!}{(k-1)!(n-k)!}F_{X}(x)^{k-1}(1-F_{X}(x))^{n-k}f(x)$: consider ways of selecting the kth smallest sample point while ignoring ways of ordering the rest, probability of k-1 of them being smaller and n-k being larger. Treat like any pdf; can find corresponding cdf by integration.

Also, $f_{X_{(j)}, X_{(k)}}(x, y) =\\ \frac{n!}{(j-1)!(k-j-1)!(n-k)!}F_{X}(x)^{k-1}(F(y)-F(x))^{k-j-1}(1-F_{X}(x))^{n-k}f(x)f(y)$.

\subsubsection{Other statistics}
Proportion $\bar{p}$, p: $n\bar{p} \distr bin(n, p) \to N(np, np(1-p))$; $\bar{X_{1}} - \bar{X_{2}}, \mean_{1} - \mean_{2}$; $\bar{p_{1}} - \bar{p_{2}}, p_{1} - p_{2}$. Good pivotal quantity for these: $\frac{\hat{t}-t}{\stddev_{\hat{t}}} \distr N(0, 1)$.

\section{Estimator properties}
Properties of good estimators: low bias, low variance, completeness, consistency, sufficiency.

\subsection{Bias}
$B(\hat{t}) = E[\hat{t}] - t$. Easy to find unbiased estimator given biased estimator.

\subsection{Mean square error: Bias variance decomposition}
$mse(\hat{t}) = E[(t-\hat{t})^{2}] = var[\hat{t}] - B(\hat{t})^{2}$ using bias definition.

\subsection{Relative efficiency of unbiased estimators}
$eff(\hat{t}_{1}, \hat{t}_2) = var[\hat{t}_{2}]/var[\hat{t}_{1}]$. To compare variances of estimators.

\subsection{Consistency of unbiased estimators}
$\hat{t}_{n}$: derived from sample of size n. Consistent if $\forall \eps: lt_{n \to \infty} Pr(|\hat{t}_{n} - t| \leq \eps) = 1$. Using Chebyshev's thm, consistency if $lt_{n \to \infty}var[\hat{t}_{n}] = 0$. ie: $\hat{t_{n}} \to_{p} t$: $(\hat{t_{n}})$.

Let $\hat{t_{n}'} \to_{p} t'$. $\hat{t} + \hat{t'} \to_{p} t + t;\ \hat{t}\hat{t'} \to_{p} tt';\ \hat{t}/\hat{t'} \to_{p} t/t'$. Also, if $g() \to R$ continuous, $g(\hat{t_{n}}) \to g(t)$.


\subsection{Sufficiency of unbiased estimator}
\subsubsection{Motivation from MLE}
Given sample vector $X=x$, suppose that we want to estimate $T=t$ using estimator $\hat{T}=\hat{t}$. So, we want to find $t$ maximizing $f_{T|X}(t|x) = \frac{f_{X|T}(x|t)f_T(t)}{f_{X}(x)}$.

We may use MLE (assuming all $t$ equally likely), find t maximizing $L(t|x) = f_{X|T}(x|t) = f_{X|\hat{T}, T}(x|\hat{t}, t)f_{\hat{T}|T}(\hat{t}|t)$.

So, if $f_{X|\hat{T}, T}(x|\hat{t}, t)$ is independent of $T$: $f_{X|\hat{T}, T}(x|\hat{t}, t) = f_{X|\hat{T}}(x|\hat{t})$, same as maximizing $f_{\hat{T}|T}(\hat{t}|t)$; can discard $X=x$ after getting $T=t$. So, this sufficient statistic summarizes all info in a sample about a parameter.

Not necessarily unbiased. All good estimators, which are unbiased, are functions of sufficient statistic.

\subsubsection{To show sufficiency if distribution family known}
Show $f_{X|\hat{T}, T}(x|\hat{t}, t) = f_{X|\hat{T}}(x|\hat{t})$; its form is not a function of t. So, show factorization $f_{X|T}(x|t) = f_{X|\hat{T}, T}(x|\hat{t}, t)f_{\hat{T}|T}(\hat{t}|t) = g(\hat{t})f(\hat{t},t)$; Or show $\frac{f_{X|T}(x|t)}{f_{\hat{T}|T}(\hat{t}|t)}$ not a function of t.

\subsubsection{To find sufficient statistic}
Start with $f_{X|T}(x|t)$, factorize it into \\
$g(...)f(\hat{t},t)$; in the part which is a $f(.., t)$, find sufficient statistic $\hat{t} = h(X)$. Eg: $f(.., t) = e^{-\sum X_{i}}$, $\sum X_{i}$ be the minimal sufficient statistic; if $f(.., t) = e^{-\sum X_{i} - \sum X_{i}^{2}}$ $\sum X_{i}$ and $\sum X_{i}^{2}$ are joint sufficient statistics.

\subsection{Statistical efficiency}
How many observations do you need to get error $d(\hat{t}, t) < \eps$?

\section{Find estimator for some parameter}
Also see model selection techniques: where you estimate all parameters which define the model from the data.

\subsection{From sufficient statistic}
Find sufficient statistic, then construct unbiased estimator as a function of it.

\subsection{Minimum variance unbiased estimator (MVUE)}
(Rao-Blackwell)\\
For parameter t, take any estimator $\hat{t}$, sufficient statistic U; $\hat{t'} = E[\hat{t}|U]$ (Rao-Blackwellization); then $E[\hat{t'}] = t, var[\hat{t'}] \leq var[\hat{t}]$: $var[\hat{t'}] = E[E[\hat{t}|U]^{2}] - t^{2} \leq E[E[\hat{t}^{2}|U]] - t^{2} = var[\hat{t}]$. Rao Blackwellization can't improve variance further.

So, an unbiased estimator which is a function of the sufficient statistic yields MVUE. To find MVUE do this.

\subsection{Method of moments}
Assume sample moments are good estimators of population moments. Set $E[Y|\gth] = \frac{\sum Y_{i}}{n}$; thence get $f(\gth) = \frac{\sum Y_{i}}{n}$; then solve for $\gth$. If number of parameters in $\gth$ is high, find other moments: $E[Y^{k}|\gth] = \frac{\sum Y_{i}^{k}}{n}$. Result is usually consistent, but not always sufficient.


\section{Confidence Interval}
\subsection{Definition}
Find intervals of probable values: confidence intervals $(\hat{t_{1}}, \hat{t_{2}}): Pr(\hat{t_{1}} \leq t \leq \hat{t_{2}}) \leq 1-a$. Can also use one sided intervals using only upper or lower confidence limits: $(-\infty, \hat{t_{2}}), (\hat{t_{1}}, \infty)$.

Contrast with point estimation.

\subsection{General procedure}
\subsubsection{Pivotal quantity for estimate}
Suppose you have a point estimator $\hat{t}$, which can be expressed as a function of some pivotal quantity $q$. This quantity has the following property: $q = g(t, .. )$, which is a function of $t$ whose distribution function does not depend on $t$, but may depend on other known/ guessable parameters - like sample size.

Sometimes, the pivotal quantity is the estimator itself.

\subsubsection{Procedure}
For a given sample, one can find $q$ \exclaim{- the pivotal quantity}, find or bound its distribution function $p$ and finally find suitable confidence interval $q \in (a, b)$ which translates to the confidence interval estimate: $t \in (\hat{t_{1}}, \hat{t_{2}})$.

\subsection{Pivotal quantity deviation bounds}
Theoretical calculations can provide deviation bounds for the pivotal quantity distribution - Eg: the use of Chernoff deviation bounds in case of binomial random variables.

\subsubsection{By repeated sampling}
If one can sample repeatedly from the actual distribution, one can estimate the confidence interval for a given estimator.

\subsubsection{By Bootstrap sampling}
\paragraph{Process}
If one is not able to take repeated samples from the actual distribution, one can repeatedly sample (with replacement) from a uniform distributions over the available sample set.

The justifcation is that this distribution is close to the original distribution, so conclusions drawn from it are not too erroneous.

\paragraph{Properties}

\tbc

\subsection{Pivotal quantity for ratio of variances}
With F sampling distribution: $\frac{S_{1}^{2}\stddev_{2}^{2}}{S_{2}^{2}\stddev_{1}^{2}} \distr F_{n_{1}-1, n_{2}-1}$.

\chapter{Mean, variance of real valued RV}
\section{Mean: estimation}
\subsection{Consistency}
Aka Law of large numbers

Let $\set{X_{i}}$ iid. $\hat{X}_{n} = n^{-1}\sum^{n} X_{i}$. As $var[\hat{X_{n}}] = \stddev^{2}/n \to 0$ as $n \to 0$, Weak law: $\hat{X}_{n}$ is a consistent estimator of $\mean$.

\subsection{Normalness of estimator distribution}
Aka Central limit theorem (CLT)

Take estimator $U_{n} = \frac{\bar{X} - \mean}{\frac{\stddev}{\sqrt{n}}}$. $lt_{n\to \infty} Pr(U_{n} \leq u) = \int_{-\infty}^{u} \frac{1}{\sqrt{2\pi}}e^{-t^{2}/2}dt$: so approaches CDF of N(0,1): See convergence of moment generating function below. So, as n increases, $var[\bar{X}]$ becomes smaller: visualize pdfs of $X, \bar{X}_{30}, \bar{X}_{50}$; see how curve becomes more normal and gets thinner and taller. Generally, can use CLT when $n>30$.

\subsubsection{Proof showing MGF \htext{$M_{U_{n}}(t) \to$}{..} MGF of N(0, 1)}
iid $\set{X_{i}}$. $m_{U_{n}}(t) = E[e^{\frac{t(\sum X_{i} - n\mean)}{\sqrt{n}\stddev}}] = \prod E[e^{\frac{t}{\sqrt{n}}}(\frac{X_{i} - \mean)}{\stddev}] = m_{Z}(t/\sqrt{n})^{n}$: implicitly defining Z with $E[Z] = 0, var[Z] = E[Z^{2}] = 1$.

But, by Taylor, $m_{Z}(t/\sqrt{n}) = \\
m_{Z}(0) + m_{Z}'(0)(t/\sqrt{n}) + m_{Z}''(h)(t/\sqrt{n})^{2}(1/2!) \\
= 1  + E[Z]t + m''(h)(\frac{t^{2}}{2n})$
 for some $h\in (0, t/\sqrt{n})$; so $m_{Z}(t/\sqrt{n}) = 1 + m''(h)(\frac{t^{2}}{2n}) \to 1 + \frac{t^{2}}{2n}$ as $n \to \infty$. So, $m_{U_{n}}(t) \to (1 + \frac{t^{2}}{2n})^{n} \to e^{t^{2}/2}$, MGF of N(0, 1).

\subsection{Normal distr: Pivotal quantity to estimate mean}
Student's t distribution used to estimate $\mean$ when distribution is assumed to be Normal, n is small and $\stddev$ is unknown. Tables only go up to n = 30 or 40. If $\stddev$ were known, would use normal distribution, or if $n > 30$ would estimate $\stddev$ and use normal distribution tables.

As $(n-1) \frac{S^{2}}{\stddev^{2}} \distr \chi^{2}_{n-1}, \sqrt{n}\frac{\bar{X} - \mean}{S} \distr t_{n-1}$. \tbc

\subsection{Goodness of empirical estimate}
Can apply Chernoff bounds and Azuma Hoeffding inequality etc.. to judge goodness of empirical estimate.

\section{Variance estimation}
\subsection{The biased and unbiased estimators}
$S^{2} = n^{-1}\sum (X_{i} - \bar{X})^{2}$ biased: $B[S^{2}] = n^{-1}E(\sum X_{i}^{2} -2\bar{X}\sum X_{i} + n\bar{X}^{2}) - \stddev^{2} = n^{-1}(nE[X^{2}] -2E[n\bar{X}^{2}] + nE[\bar{X}^{2}]) - \stddev^{2} = n^{-1}(n \stddev^{2} + n \mean^{2} - n var[\bar{X}] + n \mean^{2}) - \stddev^{2} \to n^{-1}(n-1)\stddev^{2} - \stddev^{2}$ from central limit thm. So, defined as $S^{2} = (n-1)^{-1}\sum (X_{i} - \bar{X})^{2}$ to get unbiased estimator. Difference small as $n \to \infty$.

\subsection{Normal distr: Pivotal quantity to estimate variance}
$N(\mean, \stddev^{2})$ assumed. If $S^{2} = \frac{\sum (X_{i} - \bar{X})^{2}}{n-1}$, then $(n-1) \frac{S^{2}}{\stddev^{2}} \distr \chi^{2}_{n-1}$ \why.

So, can use this as pivotal quantity.

\section{Sequential data Sample statistics}
\subsection{k-step Moving averages}
Suppose that $ran(X_i) \in R$, and that the sample size is $n$.

\subsubsection{Simple moving average}
This is simply the mean of the last $k$ ${X_i}$.

\subsubsection{Exponential Weighed}
Here, one uses an exponentially decreasing weight (with decreasing $i$) while taking a weighted average of $k$  ${X_i}$.

\subsubsection{Applications}
This is useful while predicting stock prices, for example.

\chapter{Density estimation}
\section{Importance}
Fitting a model to observations, ie picking a probability distribution from a family of distributions, is an important component of many statistics tasks where one reasons about uncertainty by explicitly using probability theory. Such tasks are labeled 'Bayesian inference'.

\section{Choosing the distribution family}
\subsection{Observe empirical distribution}
Draw a bar graph, see what the curve looks like.

\subsection{Given expected values of fns \htext{$\set{E[\ftr_{i}(X)] = \mean_{i}}$}{..} and a base measure h}
Suppose we want to modify h as little as possible, under KL divergence, so that it has $E[\ftr(X)] = \mean$. If h is U, then this is same as finding a maximum entropy distribution with $E[\ftr(X)] = \mean$. Then, the solution belongs to the exponential family generated by h and $\ftr()$: see probabilistic models ref.

\subsection{Given dependence among features}
Use graphical models - see probability ref.

\section{Parametric density estimation}
Described in a separate chapter.

\section{Non parametric Probability Density estimation}
Estimate distribution on input space using $N$ samples $(x_{i})$, without limiting attention to a certain set of distributions.

\subsection{Histogram and the Kernel histogram}
A distribution from bar-graph of frequency vs input interval. Can simply use a histogram.

\subsection{Kernel density estimation}
(Parzen). $p(x) = \frac{1}{Nh}\sum_{i=1}^{N} K(\frac{x-x_{i}}{h})$, for kernel K. A smoothening of the histogram using kernels.

\subsubsection{Kernel function for density estimation}
Non negative real valued integrable K(x) satisfies: $\int_{-\infty}^{+\infty}K(u)du = 1$ (ensures PDF qualities during density estimation); $K(-u) = u$ (ensures mean of the PDF is the data point during density estimation). So, K(x) akin to kernel of an integral operator: see functional analysis ref.

\subsubsection{Using Gaussian radial basis functions}
Aka Gaussian kernel.\\
 $K(u) = \frac{1}{2\pi}e^{-\frac{u^{2}}{2}}$: Gaussian function with mean 0, variance 1.

Taking 1 Gaussian distribution/ adding 1 bump for each data point. h, controlling the variance of the bump, called the smoothing parameter/ bandwidth.

\exclaim{Can approximate any distribution by mixture of Gaussians!}

\section{Estimate probability measures}
\subsection{Use empirical measures}
\subsubsection{Empirical measure}
The estimated measure using $n$ samples for a given function is $v_{n}(f| D) = n^{-1} \sum I_{f}(X_i)$, where $D = \set{X_i}$ is a set of samples drawn iid from $v$. 

\subsubsection{Goodness of estimate: single event}
By law of large numbers, as $\lim_{n \to \infty} E[\frac{\sum f(X_{i})}{n}] = E_X[f(X)] = v(f)$. Also, we can use the Hoeffding inequality (see probabilistic models survey) to bound $Pr_D(|v(f) - v_n(f|D)| \geq \eps)$ and see that it decreases exponentially with increasing $n$ and $\eps$.

\paragraph*{Bound variability in estimate}
From central limit theorem, we know that, as $n \to \infty$, the sample distribution approaches $N(v(f), \frac{\stddev}{\sqrt{n}})$.

Also, we can use: $Pr_{D, D'}(|v_n(f|D) - v_n(f|D')|\geq \eps) \leq Pr_{D, D'}(|v_n(f|D) - v(f)|\geq \eps/2 \lor |v_n(f|D') - v(f)|\geq \eps/2) = 2Pr(|v_n(f|D) - v(f)| \leq \eps/2)$ and use the Hoeffding inequality again.

\subsubsection{Goodness of estimate for a class of events}
Let $F = \set{f}$ be a class of events (or binary functions) defined on the input space $X$, on which $v$ is a measure. Let $E_{F}(m)$ be max dichotomy count: see boolean functions survey.

(Vapnik, Chervonenkis).

Then $Pr(\sup_{f \in F} |v_n(f|D) - v(f)| > \eps) \leq 8 E_{F}(n)exp(\frac{-n\eps^{2}}{32})$.

\pf{  If we were to use the union bound and the Hoeffding inequality naively, we would have a factor of $2|F| >> 8E_F(n)$ on the RHS. So, we want to get to a point where we need only take the union bound over a small subset of $F$.

So, first we show that \\
$Pr_D(\sup_{f \in F} |v_n(f|D) - v(f)| > \eps) \leq 2Pr_{D, D'}(\sup_{f \in F} |v_n(f|D) - v_n(f|D')| > \eps/2)$, for sample set $D'$ acquired in the same way as $D$. Lemma \pf{$Pr_{D, D'}(\sup_{f \in F} |v_n(f|D) - v_n(f|D')| > \eps/2) \geq Pr_{D}(\sup_{f \in F} |v_n(f|D) - v(f)| > \eps)Pr_{D'}(|v_n(f|D') - v_n(f)| < \eps/2| |v_n(f'|D) - v(f')| > \eps)$; and the latter factor can be seen to be small: $<1/2$ using Chebyshev inequality. This is called the 'ghost sample technique'.}

So, now we need only bound $Pr_{D, D'}(\sup_{f \in F} |v_n(f|D) - v_n(f|D')| > \eps/2)$. One way to deal with this is to take the union bound now over the set of all dichotomies induced over $D \union D'$ to get the bound: $E_F(2n)Pr(D, D')( |v_n(f|D) - v_n(f|D')| > \eps/2)$, which can then be bounded as described elsewhere.}

\paragraph*{Sharper bound}
The sharper bound we require can be obtained using a different analysis, involving $E_F(n)$ instead. The process of picking $D, D'$ and calculating $n^{-1}|\sum_i f(X_i) - \sum_i f(X'_i)|$ is equivalent to the process of picking $D, D'$, and then picking $n$ bits $s$, and then finding $n^{-1}|s_i(\sum_i f(X_i) - \sum_i f(X'_i))|$ : in other words, we do this $n$ times: pick a pair of points and assign it to $D$ and $D'$ at random. So, we bound $Pr_{D, D',s }(\sup_{f \in F} n^{-1}|s_i(\sum_i f(X_i) - \sum_i f(X'_i))|> \eps/2) \leq Pr_{D, D',s }(\sup_{f \in F} [|n^{-1}\sum_i s_if(X_i)| \geq \eps/4 \lor |n^{-1}\sum_i -s_if(X'_i)| \geq \eps/4]) = 2Pr_{D, s}(\sup_{f \in F} |n^{-1}\sum_i s_if(X_i)| \geq \eps/4)$. A union bound over $E_F(n)$ members of $F$ and a Hoeffding inequality can now be applied to bound this.

\subsection{Estimate CDF using empirical CDF}
(Glivenko Cantelli). Pick $\set{Z_i} \distr F$, the CDF. \\
Then $Pr(\sup_z |F_n(z) - F(z)| > \eps) \leq 8(n+1)exp(\frac{-n\eps^{2}}{32})$. Pf: Apply VCD theorem with open intervals as classifiers.

Answer to: What $n$ do you need to achieve low error? Thence Borel - Cantelli: $\lim_{n \to \infty} \sup |F_n(z) - F(z)| = 0$ with probability 1.

\chapter{Parametric density estimation}
\section{Problem and solution ideals}
\subsection{Density estimation using a distribution class}
Suppose that parameter $t$ specifies the distribution $f_t(x_r|x_{\lnot r})$, where $x_{\lnot r}$ can be empty! Let $T$ be parameter space spanned by such $t$; it represents the class of distributions which can be specified in this form.

Given finite data set $\set{x^{(i)}}$, we want to approximate an unknown target distribution $D(x_r|x_{\lnot r})$ which may not belong to this distribution family using $T$. The approximation can be a weighted combination of combination of distributions in $T$.

\subsubsection{Related problems}
Note that estimating $t$ which is good at predicting the value of $x_r$ given $x_{\lnot r}$ by doing $h(x_{\lnot r}) = \max_{x_r} f_t(x_r|x_{\lnot r})$ is a separate problem, where a different estimation procedure which minimizes the classification error $Pr(h(x_{\lnot r}) \neq x_r)$ (corresponding to the 0/1 classification loss) may be used.

\subsection{Solution ideas}
Empirical risk minimization, for various forumulations of risk functions which in someway also incorporate prior belief about the best $t$.

\section{Approximation with Normal distribution}
Aka Laplace approximation. Suppose that we have the probability distribution $p(x) = Z^{-1}f(x)$.

\subsection{Algorithm}
Here, one finds a mode / strict local maximum $x'$ of the distribution $p(x)$, which corresponds to a mode of $f(x)$, using numerical techniques. Then, one creates a normal distribution $N(\mean = x', \stddev)$ around this point.

\subsubsection{2nd order approximation of log f}
$\log N(x', \covmatrix)$ is a quadratic function. So, we try to find $\covmatrix$ such that $\log N$ approximates the 2nd order approximation of $\log f$.

As $\gradient f(x')= 0$, taking the quadratic approximation \\$\log f(x) \approx \log f(x') + (f(x'))^{-2} (x-x')^{T}\gradient^{2} f(x')(x - x')$. So, \\$f(x) \approx f(x')exp((f(x'))^{-2} (x-x')^{T}\gradient^{2} f(x')(x - x'))$

The RHS can now be used to construct $\covmatrix$. As $x'$ is a mode, \\$-2(f(x'))^{-2}\gradient^{2} f(x') \succ 0$ as expected.

\subsection{Properties}
\subsubsection{Estimating Z}
Z can be approximated to equal the normailizer of N, which is easily calculated. Note that knowledge of Z is not required to get the approximation $N$.

\subsubsection{Non-uniqueness}
Different modes yield different approximate distributions.

\section{Log loss minimization}
Aka Maximum likelihood estimation (MLE).

\subsection{Optimization problem, estimate}
Likelihood function: $L:T \to [0,1]$. \\
$L(t|\set{x^{(i)}}) = f_t(\set{x_r^{(i)}}|\set{x_{\lnot r}^{(i)}}) = \prod f_t(x^{(i)}|x_{\lnot r}^{(i)})$.

$\hat{t} = argmax_{t} L(t|x)$. This may be a biased estimator; but is always a sufficient statistic, as it is defined on $L(t|x)$. Often, an equivalent optimization problem: minimizing log-likelihood is used.

\subsubsection{Functional Invariance property}
If you want to estimate $g(t)$, $g(\hat{t})$ is the MLE of g(t). From definition.

\subsubsection{Avg Log likelihood function}
Take $l(t|\set{x^{(i)}}) = \ln(L(t|\set{x^{(i)}}))$, and do $\min_t -n^{-1}l(t|X)$. Useful as often $L$ and distribution of X are from exponential family.

Example: In case of $N(\mean, \stddev^{2})$, $\ln f(\set{x^{(i)}}|\mean, \stddev^{2}) = -\frac{1}{2\stddev^{2}}\sum(x^{(i)} - \mean)^{2} - \frac{N}{2}\ln (\frac{\stddev^{2}}{2 \pi})$; by maximization, MLE is $\mean = \bar{x}, \stddev^{2}= N^{-1}(x^{(i)} - \mean)^{2}$: because biased estimator is used, $N^{-1}(x^{(i)} - \mean)^{2}$ often underestimates.

\subsection{Other perspectives}
\subsubsection{As log loss risk minimization}
The negative log-likelihood of a single sample-point, $-\log L(t|x)$, is also called the 'log loss' in the general decision theoretic framework. So, by doing maximum likelihood estimation, we are actually minimizing empirical log-loss.

\subsubsection{Priors as regularizers}
If you add regularizer $r(t)$, you are imposing a prior distribution on $t$; so you are doing bayesian inference. The optimization problem becomes: $\min l(t|X) + r(t)$.

\subsubsection{As empirical code-length divergence minimization}
Let \cF be a class of distributions, \\
let $D$ be the actual distribution of $X$. In the limit where $n \to 
\infty$, maximum likelihood estimation tries to find $\argmin_{F \in \cF} E_D[- \log F(X)]$. This is the same problem as finding $\argmin_{F \in \cF} E_D[- \log F(X)] - E_D[-\log D(x)] = \argmin_{F \in \cF} KL(F||D)$. So we are finding a member of \cF, with minimum code-length divergence to $D$.

\subsection{Derivatives of log likelihood}
\subsubsection{Score function : Sensitivity of log Likelihood}
$V(t, X) = \gradient_t{\log L(t|X)} =  L(t|X)^{-1} \gradient_t L(t|X)$: variability of $L(t|X)$ normalized by $L(t|X)$: like conditioning in numerical analysis.

\paragraph*{Mean wrt X}
Under some regularity conditions, \\
$E_X[V(t, X)] = \int_x (f_{X|T}(x|t)^{-1} \gradient_t f_{X|T}(x|t)) f_{X|T}(x|t))dx =\\
 \gradient_t \int_x f_{X|T}(x|t)) dx = \gradient_t 1 = 0$.

\subsubsection{Variance wrt X of sensitivity score of likelihood}
Aka Fisher Information matrix. As $E_X[V(t, X)] = 0$, $I(t) = E_X[V(t, X)^{2}] = E_X[(\gradient_t{\log L(t|X)})^{2} | t]$. Measures information about t in the observable RV X. If I(t) is high for RV X, then the absolute value of the sensitivity score is high in expectation.

If conditions like those in $E_X[V(t, X)] = 0$ hold,\\ $E_X[(f_{X|T}(x|t))^{-1} \gradient_t^{2} f_{X|T}(x|t)] = 0$ will hold, and so \\$I(t) = -E_X[(\gradient_t^{2}{\log L(t|X)})^{2} | t]$.


\subsection{Computational cost}
If this optimization problem can be accurately and efficiently: great!

\subsubsection{Computing partition function}
Suppose only $f(x, t)$ proportional to th epdf $f_t(x_r|x_{\lnot r})$ is specified. Doing \\
$\max_r \frac{f(x, t)}{\sum_y f(y, t)}$ is not the same as doing $\max_r f(x, t)$, as the normalizer (aka partition function) $Z(t) = \sum_y f(y, t)$, even though independent of $x$,  varies with $t$.

If range of $x$ is huge, and Z(t) does not have a closed-form solution, computing $Z(t) = \sum_y f(y, t)$ can become costly and MLE is impractical.

\paragraph*{Ease in case of conditional probabilities}
Suppose that $range(X_t)$ is actually small. Here, rather than having to sum over the entire range of $X$,  which may be $range(x_r)^{|V|}$ in size, we just sum over $range(x_r)$ values to get $Z(t) = \sum_y f(y, t)$.

\subsubsection{Pseudolikelihood maximization}
In case finding/ maximizing $f_t(x_r|x_{\lnot r})$ is hard due to the need to compute $Z(t)$, but finding $f_{X_{r_j}|X_{\lnot r_j}}$ is easy. So we can consider maximizing the pseudo-likelihood function $\prod_{r_j} f_{X_{r_j}|X_{\lnot r_j}}$ instead.

\section{Non uniform model for P(t)}
Aka Maximum a posteriori (MAP).

\subsection{Objective, estimate}
Posterior probability of model considering observations $\propto$ likelihood of observations given the model $\times$ prior probability of model.

So, solve : $argmax_{t} f_{X|T}(x|t)f_T(t)$, or $argmax_{t} f_X(x)$ + regularizer.

\subsection{Relation to MLE}
MLE is a special case: it ignores prior distribution/ assumes it are uniform. So, often superior MLE. Eg: In MLE, upon seeing 4 H in coin tosses, you would conclude that the coin will always come up H.

The MLE optimization objective with a regularizer is a case of MAP, where the regularizer implitly defines a prior.

\subsection{Defining prior distributions}
Often, the prior distribution on $T$ may be specified, and the regularizer associated with the risk function may be derived thence.

\subsubsection{Hyperparameters for prior distribution of parameters}
$f_{T|C}$ may have hyperparameters C, which may inturn come from distribution $f_{C|D}$ with hyperparameters $D$. Eventually must fix (hyper)parameter, perhaps with n-fold cross validation.

Eg: Can have hyper-parameters: $\stddev$ for label noise , $\ga$ for prior over $\param$.

These are akin to parameters, but of a special kind.

\subsubsection{Conjugate prior for a likelihood}
You got distribution family $f_{X|T=t}$. Conjugacy: If $f_{X|T=t}$ and $f_T(t)$\\
 have the same form, finding $Pr(t|X) = f_{(X_{i})|t}((x_i))f_T(t) =  \prod_{i} f_{X|T=t}(x_i)f_T(t)$ simpler. Then can update these probabilities with each incoming observation $X_{i}$ easily.

Eg: For k-categorical distribution: $Pr(X_{j}=x|p) = \prod_{i=1}^{k} p_{i}^{x_{i}}$, dirichlet distribution is the conjugate: $f_{P|A=a}(p) = \prod p_{i}^{a_{i}-1}$ for hyperparameters a.

\section{Model combination}
Aka Fully Bayesian approach. We may first determine posterior distribution over parameters $f_{T|S}$ in the training stage, where $S$ is the training set. We may then arrive at the model $f_{L|X, S} = \int_{t \in T} f_{L|X, T=t}f_{T|S}(t)$.

So you use an ensemble of hypothesis distribution models.


\section{Information criteria}
Bayesian/ Schwartz information criterion (BIC): $\min 2^{-1}L(D, t) + p \log n$, where $p$ is the number of parameters needed to specify $t$, and $n$ is the number of samples.

\subsection{Use}
These are good if certain 'Gaussianness' assumptions hold, otherwise, the result of using them can be misleading. They allow us to pick $t$ without having to, for example, do cross-validation, which would have been necessary if $\min L(D, t) + l r(t)$ were used instead.

\chapter{Support estimation}

\section{Estimate support of a distribution D}
Find set $S'$ such that $Pr(x \notin S')<p \in (0,1]$, given sample $S$. Can be solved by probability density estimation techniques, but actually simpler.

Visualization: take the input space; draw solid ovals around sampled points; the algorithm will draw a dotted oval around these, which will represent the support of the distribution.


\subsection{With soft margin kernel hyperplane}
Aka One Class SVM or OSVM.

Given $N$ examples $\set{x_{i}}$; project to some feature space associated with kernel $k(x,y) = \ftr(x)^{T}\ftr(y)$; want to find hyperplane $w^{T}\ftr(x) - \gr$ such that all points in the support fall on one side of the hyperplane, outliers fall on the other side: support identifier $f = sgn(w^{T}x - \gr)$; so, allowing a soft margin, want to solve $\max_{\gr, w} \frac{\gr}{\norm{w}} + C\sum \gx_{i}$ such that $w^{T}\ftr(x_{i}) + \gx_{i} \geq \gr, \gx_{i} \geq 0$; $\equiv$ obj function: $\min_{w, \gx, \gr} \norm{w}^{2}/2 + \frac{1}{\gn N} \sum \gx_{i} - \gr$, for some coefficient $0 \leq \gn \leq 1$.

Thence get Lagrangian: \\
$L(w, \gx, \gr, \ga, \gb) =  \norm{w}^{2}/2 + \frac{1}{\gn N} \sum \gx_{i} - \gr - \sum \ga_{i}(w^{T}\ftr(x_{i}) + \gx_{i} - \gr) - \sum \gb_{i}\gx_{i}$ with $\ga, \gb \geq 0$.

Set derivatives wrt primal vars $w, \gx, \gr$ to 0 to get: $w = \sum_{i}\ga_{i}\ftr(x_{i}); \ga_{i} = \frac{1}{\gn N} - \gb_{i} \leq \frac{1}{\gn N}, \sum_{i} \ga_{i} = 1$. Thence, the support identifier becomes \\
$f = sgn(\sum_{i}\ga_{i}k(x_{i},x) - \gr)$; dual optimization problem becomes \\
$max_{\ga} - 2^{-1} \sum_{i,j}\ga_{i}\ga_{j}k(x_{i}, x_{j})$ subject to $0 \leq \ga_{i} \leq (\gn N)^{-1}, \sum_{i}\ga_{i} =1$. Solving this, discover w; then recover $\gr$ using $\gr = w^{T}\ftr(x_{i})$ for $x_{i}$ with $\ga_{i} \neq 0; \gb_{i} \neq 0$ (support vector with $\gb_{i} > 0$): $\exists x_{i}$ as $\sum \ga_{i} = 1; \ga_{i} \geq 0$.

\subsubsection{Choosing kernel, tuning parameters}
$\gn \propto$ softness of the margin, number of support vectors, thence the runtime, sensitivity to appearence of novelty.

With Gaussian kernel, any data set is seperable as everything is mapped to same quadrant in feature space.

\oprob How to decide width of Gaussian kernel to use? Can you use information about the abnormal class in choosing the kernel?

\subsubsection{Comparison with thresholded Kernel Density estimator}
If $\gn = 1, \ga_{i} = 1/N$, support identifier $f = sgn(\sum_{i}\ga_{i}k(x_{i},x) - \gr)$ same as one using a Kernel (Parzen) Density estimator. What happens when $\gn < 1$?


\subsubsection{Comparison with using soft margin hyperspheres}
For homogenous kernels, $k(x,x)$ is a constant and the dual minimization problem \\
$\min_{\ga}\sum_{i,j}\ga_{i}\ga_{j}k(x_{i}, x_{j}) - \sum_{i}\ga_{i}k(x_{i}, x_{i})$ and the support identifier \\
$f = sgn(R^{2} - \sum_{i,j}\ga_{i}\ga_{j}k(x_{i}, x_{j}) + 2\sum_{i}k(x_{i}, x)- k(x,x) )$ is equivalent to the minimization problem derived from the hyperplane formulation. So, all mapped patterns lie in a sphere in feature space; finding the smallest sphere containing them is equivalent to finding the segment of the sphere containing the data points, which reduces to finding the separating hyperplane.

\subsubsection{Connection to binary classification}
Hyperplane $(w, \gr = 0)$ \\
separates $\set{(x_{i}, 1)}$ from $(-x_{i}, -1)$ with margin $\gr/\norm{w}$ and vice-versa.

\subsection{Using soft margin hyperspheres}
Aka Support vector data description. Here one solves: 
$\min_{R, \gx, c} R^{2} + \frac{1}{\gn N} \sum_{i}\gx_{i}$ subject to $\norm{\ftr(x_{i} - c)}^{2}  - \gx_{i}\leq R^{2}, \gx_{i}\geq 0$.

After using the Lagrangian, finding the critical points and substituting, this leads to the dual $\min_{\ga}\sum_{i,j}\ga_{i}\ga_{j}k(x_{i}, x_{j}) - \sum_{i}\ga_{i}k(x_{i}, x_{i})$ subject to $0 \leq \ga_{i} \leq \frac{1}{\gn N}, \sum \ga_{i} = 1$, and the solution $c = \sum \ga_{i}\ftr(x_{i})$ corresponding to support identifier $f = sgn(R^{2} - \sum_{i,j}\ga_{i}\ga_{j}k(x_{i}, x_{j}) + 2\sum_{i}k(x_{i}, x)- k(x,x) )$ \chk.

\subsection{Using Clustering}
Cluster the sample, draw boundaries around the clusters. Eg: Use $k$ means clustering.

\section{Novelty detection}
\subsection{Problem}
Aka Outlier detection.

In general, we want to find outliers - unlikely data-points according to the conditional distributions $f_{X_r|X_{\lnot r}}$.

\subsubsection{As One class classification}
view as a problem where there are multiple classes, but all training examples are from one class only.

\subsubsection{Motivation}
Outliers are detected either to focus attention on them or to remove them from consideration.

\subsection{Using density estimation}
Do density estimation; call apparently improbable data points novel.

\subsection{Using support of the distribution}
Find distrubution support, call anything outside the support an outlier.

\subsubsection{Ransack}
One learns a model $M$ (either $f_{X_r|X_{\lnot r}}$ or $E[f_{X_r|X_{\lnot r}}]$) using data set $S$.

Then, one finds $S' \subset S$ for which $err(M;x) \geq t \forall x \in S'$.

$S'$ is then added to the set of outliers. 

Finally, one repeats the entire procedure till the set of outliers is stable.

\subsection{Boundary methods}
\subsubsection{K nearest neighbors}
Estimate local density of $x$ by taking avg distance to $k$ nearest neighbors; similarly estimate local density of each neighbor; call $x$ novel if its local density is much smaller than that of the neighbors.

\subsubsection{Support vector data description}
\tbc

\subsubsection{PCA}
Simplify the data using PCA. \tbc


\chapter{Conditional independence structure: discrete case}
\section{Problems}
In all of the following, we suppose that $f_X(x)$ is given by a graphical model.

\subsection{Model Estimation}
The most ambitious goal is to estimate the parameters associated with the distribution. This can often be accomplished by minimizing the log loss associated with the conditional distribution $f_{X_i|X_{\nbd(i)}}$, once the structure of the underlying graphical model has been estimated (ie feature selection is done).

\subsection{Edge recovery}
Deduce the graph encoding the conditional independence relationships among features.

\subsubsection{Ising models: Signed edge recovery}
Take the ising model. Deduce not just the edge, but also the sign of the correlation/factor for interaciton between the nodes. Eg: Maybe want to deduce relationship voting patterns of legislators.

\subsection{High dimensional case}
Often have few (f(d)) samples from high-dimensional (d-dim) data, so $f(d) << d$.

\subsubsection{Measuring performance}
Redefine statistical consistency: let both $d$ and f(d) tend to $\infty$.

\section{Approaches}
\subsection{Learn closest tree}
(Chow, Liu).
\subsubsection{Aim}
Here, the aim is to learn a tree structured graphical model which is closest to the underlying distribution in terms of KL divergence.

\subsubsection{Algorithm}
Construct a complete graph, where each edge is labeled with the mutual information estimated from the given sample. Then, run the minimum spanning tree algorithm over this graph.

\section{Learn neighborhoods}
\subsection{For ising models}
Consider for each node the conditional probability distributions $f_{X_i|X_{\nbd(i)}}$, suppose that in the underlying graphical model, each edge is associate with a parameter $\gth_{ij}$. Suppose that the log loss is $l(X, \gth) = \hat{E_x}[-log (f_{\gth_i}(x_i| x_{\nbd(i)}))]$. We can use log-loss minimization with l1 regularization : $\min_{\gth_i} l(X, \gth_i) + \gl \norm{\gth_i}_1$ to learn the (sparse) neighborhood of each node (ravikumar et al). This $l_1$ regularized logistic regression problem, in the case of exponential family distributions (including binary distributions) is a convex problem and can be efficiently solved.

\subsubsection{Results}
For signed edge recovery, in case the distribution satisifies certain special conditions, signed edge recovery and good parameter estimation are guaranteed. The l1 regularization ensures that $n = O(d\log p)$ samples suffice to get a good model-estimate, where $p$ is the number of nodes and $d$ is the max-degree of the underlying graphical model.

Otherwise, $O(p)$ samples would be required, which is not feasible for high-dimensional model-selection.

\subsubsection{Caveats}
However, experiments suggest that picking the right choices of $\gl$, and thresholding after optimization can be very tricky. The $\gl$ suggested may be applicable only for models with large $p$.

\subsubsection{Analysis technique}
To prove the guarantees, one considers the optimality criteria of the convex optimization problem mentioned earlier, which says $\gradient l(X, \gth_i) + w = 0$ for some subgradient vector $w$. Then, we can begin to construct a solution $\gthEst_i, w$ satisfying this condition, based on knowledge of actual parameters $\gth^*_i$. One then shows that the remaining optimality conditions  and the claimed properties are satisfied.

\subsection{For discrete graphical models}
In this case, each edge is associated with $(m-1)^{2}$ parameters $\gth_{ij}$, and each node is associated with $m-1$ parameters, where $m$ is the size of the state-space. The algorithm for the Ising model case can be extended to the case of discrete graphical models, except that we will need to use $l1/l2$ regularization: $\norm{\gth_{ij}}_{2}$.

This is multi-class logistic regression with l1/l2 regularization, which is a convex program. The same caveats and advantages as earlier apply.

\chapter{Hypothesis testing}
\section{Model selection given 2 models}
Aka Confirmatory data analysis: Test hypotheses, as against Exploratory data analysis: Find hypotheses worth testing.

Which process is more likely to have generated the data? Which model is better at explaining the observations? Model selection, with only 2 models.

\section{Hypotheses}
\subsection{Null hypothesis}
$H_{0}: t = t_{0}$ or $t\leq t_{0}$

\subsection{Alternate hypothesis}
$H_{a}$; can be 1 sided like $t > t_{a}$ or 2 sided: $t \neq t_{0}$ or $|t-t_{0}| \geq k$.

\subsection{The decision}
So, you decide if parameter $t \in T_1$ or if $t \in T_2$.

\section{Experiment/ Test}
Pick sample; find value of estimate test statistic $\hat{t}$; accept $H_{a}$/ reject $H_{0}$ if $|\hat{t} - t_0| > |t' - t_0|$; fail to reject $H_{0}$ otherwise. Critical value t' defines $H_{0}$ rejection region. Visualize as shaded area under $\hat{t}$ pdf curve. So, you always do hypothesis testing assuming $H_{0}$ is true.

\subsection{Errors}
\subsubsection{Type 1}
Erroneously accept $H_{a}$: $\ga = Pr(\hat{t} > t'|t=t_{0})$. Say $\ga (= 0.05?)$ level of significance.

\subsubsection{Type 2}
Erroneously fail to reject $H_{0}$: $\gb = Pr(\hat{t} \leq t'|t=t_{a})$.

\subsection{Tradeoff}
Trying to decrease type 1 error involves increasing t'; But that increases type 2 error rate. Visualize error zones with regions in 2 bell curves with means slightly apart.

To simultaneously decrease both, must increase sample size.

In case of $X \distr N(\mean, \stddev^{2}), t=\mean$, can write $\hat{t} > t'$ as $z = \frac{\hat{t} - \mean}{\stddev/\sqrt{n}} > \frac{t' - \mean}{\stddev/\sqrt{n}}; z \distr N(0, 1)$. Given $ \mean, \stddev, \ga, \gb$, can solve for t', n using $N(0, 1)$ table. For small sample size, can use t distribution.

\subsection{p-value of the statistic}
Given a sample, got $\hat{t}$, for what $\min t'$ we will reject $H_{0}$ based on it? The corresponding $\ga$ is p-value.

\subsection{Power of a test}
Take $H_{0}: t = t_{0}; H_{a}: t = t_{a}$, fix $t'$. $power(t) = 1 - \gb(t)$: ability to detect if $H_{a}$ is true. $power(t_{0}) = \ga$ \chk. So, power curve has a minimum at $t_{0}$ \chk.

\section{Test design}
Consider goodness of test with $\ga, \gb, power(t)$.

\subsection{Best test for given \htext{$\ga$}{alpha}}
(Neyman-Pearson): Testing $H_{0}: t = t_{0}, H_{a}: t = t_{a}$. Likelihood ratio test: $L = \frac{L(t_{0}|\hat{t})}{L(t_{a}|\hat{t})} \leq? h$, $Pr(L \geq h) = \ga$. This is the most powerful test.

\subsection{Difference in differences}
Suppose that using experiment A, where one compares hypotheses H1 and N (for null hyp), it is determined that N cannot be dismissed. In experiment B, one compares H2 and N and observe that N can be dismissed. From this, one cannot conclude that, while comparing H2 and H1, H1 can be dismissed: it is possible that the difference in evidence supporting H1 and H2 is small.

One should instead conduct an experiment comparing H1 and H2 directly. This has been a common mistake in medical research as of 2011!

\part{Label prediction/ identification}
\chapter{Problems}
Aka supervised learning. There are a variety of prediction problems depending on the combination of problem components described below.

\section{Core problem}
\subsection{Input and response variables}
A label/ target/ response/ dependent $L$ depends on some predictor/ input/ independent variable $X$ (a set of features/ covariates).

\subsection{Range of X and L}
Input space is $D$ dimensional.

$range(L)$ may be a subset of a vector space. It may be continuous or discrete.

\subsection{Labeling rule sought}
The agent/ decision rule produced by the learning algorithm must label $L$ some unlabeled test point(s) $X$, after some observations/ examples/ training points $S$. As for decision theory in general, such a labeling rule may be randomized or deterministic.

\section{Action space}
$range(L)$ constitutes the action space in a decision theoretic view of the problem. It is possible for the action space to be expanded to include stating indecision about a label.

\section{Actual phenomenon}
\subsection{Randomized function}
In general, the labeling process can be seen a randomized function $c:ran(X) \to $ the set of RV's over range(L).

\subsection{Volatility in form}
IN some problems, the randomized labeling process $c$ changes with previous labelings of the labeling process ( and therefore with observations $S$). Eg: Predicting the position of a plane 5 seconds in the future, given its positions in the past few seconds.

In many other problems, the labeling process $c$ remains independent of observations.

\subsection{Deterministic Labeling function}
For simple phenomena, a deterministic function $c$ suffices to relate $X$ and $L$, so that $L = c(X)$.

If $ran(L)$ is discrete, $c$ is a discrete function, aka discriminant function: $c: dom(X) \to \set{C_{k}}$.

\subsubsection{Features}
The labeling function can often be expressed using a feature mapping function $\ftr(X)$. The various dimensions of $ran(\ftr)$ aka features of the input.

\subsection{General noise model}
\subsubsection{Using a randomized noise function}
Usually, the following model can be used to described the phenomenon: $Y = f(X), L = g(Y)$, where $f$ is a deterministic labeling function, $g$ is a randomized function, called the noise function, which maps $ran(f)$ to a random variable $L$.

$g$ is usually considered to be symmetric around the expectation.

\subsubsection{Using a Noise variable}
Dependence of $L$ on $X$ can be written in terms of a deterministic noise application function $h$ and a random noise variable $N$, $L = h(f(X), N)$, where $f$ is a deterministic labeling function.

\subsection{Noise in case of vector labels}
Let $L = h(f(X), N)$ describe the dependence of $L$ on $X$, as described above. If $ran(L)$ is part of a vector space, $h()$ can often be described arithmetically.

\subsubsection{Additive noise}
An additive noise application model is common: $L = f(X) + N$, and $N$ is usually has a symmetric (usually normal) distribution centered around $0$.

\subsubsection{Multiplicative noise}
Multiplicative noise models of the form $h(Y) = NY$ are also interesting. In this case, $N$ is centered at 1.

\section{Example/ training points}
The general properties/ peculiarities of a sample (eg: correlatedness, completeness, online vs offline learning) in general is considered elsewhere; here we are concerned with peculiarities associated with samples in case of the prediction problem.

\subsection{Labeled}
Given $N$ example points $S = \set{(X_{i}, L_{i})}$. When such labeled examples are provided as part of the problem, the problem is called supervised learning.

\subsection{Unlabeled}
It is possible that we are additionally given a set $U$ of unlabeled points. In such a case, the problem is called 'semi-supervised learning'. The reason maybe that sometimes, easy to get data points, but expensive to label them; or maybe labels are noisy.

\subsection{Alternative labels}
For some data points belonging to the input space $ran(W)$, labels $K_i$ may be provided. So, examples are pairs $(W_i, K_i)$. Let the underlying labeling functions be $f_K: ran(W) \to$ RV's with range $ran(K)$ and $f_L: ran(X) \to $ RV's with range $ran(L)$.

When the two input spaces and labeling functions are related, this additional data helps in predicting $L$. Aka transfer learning problem. Eg: Such examples may help deduce relevant features.

In case of cross domain learning, $ran(K) = ran(L)$, but possibly $ran(W) \neq ran(X)$. Eg: search query result relevance identification.

In case of cross category learning, $ran(K) \neq ran(L)$.

In some applications both the label range and the input space may be the same, but the classificaiton functions may still be different. Eg: Netflix movie ratings by various people.

Eg application: Robot learns to stand using new legs faster using lessons learned when learning to stand using old legs.

\section{Distribution on test points}
This is an essential factor in calculating the risk of labeling rules.

This 'test distribution' usually usually is close to the training data distribution.

\subsection{Transduction vs induction}
If the test points are known during training: transduction; eg: semi-supervised learning: the labeling problem is called transduction.

Otherwise, the labeling problem is called one of induction. This is a harder problem.

\chapter{Risk and evaluation}
It may be essential to model $Pr(L|X, S)$; this is called \textbf{Inference} in the context of probabilistic graphical models.

\section{Loss functions: labeling single data points}
Different measures of goodness/ error functions are appropriate for different scenarios.

\subsection{Loss functions: vector labels}
Loss functions in this case are often defined to penalize deviation from the actual label symmetrically.

See loss functions in regression section.

\subsection{Loss functions for classification}
Below we mainly consider the loss functions $l(\hat{y}, y)$ which are without regularization to account for prior belief. Loss functions used in regression can directly be applied to this case: Eg: like squared difference.

Good loss functions are realistic, maybe convex and smooth too - so that the corresponding empirical risk minimization problem (for picking classifiers) they lead to is tractable.

\subsection{0/1 loss}
$l() = I[\hat{y} \neq y]$. The corresponding risk will be $Pr(\hat{y} \neq y) = E[I[\hat{y} \neq y]]$, the misclassification rate.

The minimal risk classifier has the form: $h(X) = \argmax_L Pr(L|X, S)$. The maximization procedure is called \textbf{Decoding} in the context of probabilistic graphical models.

However, the corresponding empirical risk minimization is non-convex, is NP hard and cannot is not approximable to a constant fraction of goodness. So, auxiliary loss functions are used.

\subsubsection{Minimal risk: Binary classification}
Aka Bayes risk. Suppose that data is generated by the model specified by specifying the pdfs $f_{X|Y=1}$, $f_{X|Y=0}$, $Pr(y)$. Then, the best possible classifier is one which has accurate knowledge of the generative model, and even this classifier, in general, has a non zero risk. Its risk is given by $\sum_y Pr_{x: Pr(y|x) \geq 1/2}(\lnot y)$, or by $E_x[\min_y Pr(y|x)]$.


\subsubsection{Connection to log loss risk: binary classification}
Whenever $I[\hat{y} \neq y]$, we know that $Pr_t(y|x) \geq 1/2$. So, $E_{x, y}[-\log Pr_t(y|x)] \geq (-\log Pr_t(y|x)) Pr(\hat{y} \neq y) \geq \log 2 Pr(\hat{y} \neq y)$. Hence, upper-bound on log loss risk is also an upper bound for $Pr(\hat{y} \neq y)$.

\subsection{Log loss}
Suppose that our predictor is based on the model $f_{Y|X,T=t}$. Then log loss is $-log f_{Y|X,T=t}(y|x)$. This punishes the model for assigning low probabilities to an observation. Minimizing log loss corresponds to maximum likelihood estimation. The corresponding risk is $E_{x, y}[-log f_{Y|X,T=t}(y|x)]$.

\section{Loss functions: labeling multiple data points}
\subsection{Confusion matrix}
For qualitative evaluation, make k*k confusion matrix $C$ with $C_{i,j}$ as number of points belonging to class i predicted as belonging to class j.

\subsection{True and false positives}
$U \dfn $ set of all points. $y(c) \dfn$ points belonging to class c. $\hat{y}(c) \dfn$ set of points predicted to belong to class c.

True positives: $tp(c, \hat{y}) \dfn |y(c) \inters \hat{y}(c)|$. False positives: $fp(c, \hat{y}) \dfn |\hat{y}(c) - y(c)|$.

False negatives: $function(c, \hat{y}) = |y(c) \inters (U - \hat{y}(c))|$. True negatives: $tn(c, \hat{y}) = |(U - y(c)) \inters (U - \hat{y}(c))|$.

\subsection{Precision, recall, specificity}
Micro averaged precision : $P(\hat{y}) = \frac{\sum_{c}tp(c, \hat{y})}{\sum_{c} (fp(c, \hat{y})+tp(c, \hat{y}))}$.

Recall, or completeness or sensitivity or true positive rate:\\ $R(\hat{y}) = \frac{\sum_{c}tp(c, \hat{y})}{\sum_{c} (function(c, \hat{y}) + tp(c, \hat{y}))}$. Measures ability to identify items belonging to class c.

Specificity: $S(c, \hat{y}) = \frac{tn(c, \hat{y})}{tn(c, \hat{y}) + fp(c, \hat{y})}$. Measures ability to discard items not belonging to class c. $1 - S(c, \hat{y})$ is false positive rate.

F-measure, the harmonic mean of precision and recall, is also used to evaluate success.

\subsubsection{Emphasis on one '+ve' class}
If you only care about performance from the perspective of one class, as in the case of link prediction, let c range over only that class in the summations above.

\subsubsection{Sensitivity - specificity tradeoff}
Ideally, want to increase both sensitivity and specificity. But to increase sensitivity, the classifier often needs to take more risks in classifying an entity as 'positive'. There will be many cases where -ve entities are declared +ve: there is decrease in specificity.

Visualize two normal curves over 1-D feature: one for the -ve case and one for the +ve case. On observing a feature, a classifier uses a cutoff to identify +ve cases. Compare with tradeoff between type-1 and type-2 errors in hypothesis testing.

\subsubsection{Sensitivity vs 1-specificity curve}
Aka Receiver operating characteristic (ROC) curve. Take a parametrized family of predictors/ tests to identify +ve cases; the predictors are distinguished by the cutoff they choose in making classifications using the same scores for items. ROC considers the sensitivity vs specificy tradeoffs of various tests belonging to this family.

So, for all tests, you plot sensitivy and 1- specificity on a graph, and join these points by a straight line : a piecewise linear function starting at 0, and ending at 1; not a step function.

\paragraph*{Area under curve (AUC)}
Ranges of sensitivity and 1-specificity are [0, 1]. All curves startat 0, where every item is classified -ve, and end at 1, where every item is classified +ve.

A good test (family) is as close as possible to the left axis: parameters can be tuned to increase sensitivity without sacrificing specificity too much. An ideal test has area under the curve (AUC) 1. The higher the AUC, the better the test family. A random predictor has AUC 0.5.

So, AUC measures the test's discrimination, or the ability to separate +ve cases from the -ve cases.

\subsubsection{Precision/ recall tradeoff}
Recall monotonically increases with number of points classified as +ve. Precision monotonically decreases with the number of false positives. Often precision and recall have inverse relationship: if you classify all points as +ve, you have very high completeness but bad recall.

Often want to see how these change with classification parameters: so draw plots.


\chapter{General Solution properties}
\section{Empirical risk minimization vs expert systems}
One way to predict $L$ given $X$ is to learn the process by which $L$ is generated: one can manually create a labeling algorithm by listing out rules which influence labeling: for example by studying expert human animals.

Often, you cannot know enough about how stuff works in order to have explicit rules. Eg: Expert system for handwriting detection can be tougher to make than a training a statistical model. And, repeatedly, in various fields, statistics based automatic learning of rules has outperformed manually developed expert systems. Eg: Natural language processing.

Here, we mainly consider statistical learning.

\section{Hypothesis classes}
General discussion about hypothesis classes described in the Decision Theory chapters applies.

Suppose that the labeling rule, on observing $X$, produces the labeling rule $Y$.

\subsection{Probabilistic models}
As the hypothesis class, one may choose a family of probabilistic models for $f_{Y|X}$ specified either directly as in the case of discriminative probabilistic modeling, or indirectly by specifying a generative model: that is, modeling $f_{X,Y}$ (which may involve modeling $f_{X|Y}, f_Y$).

Model/ parameter selection for such hypothesis classes is described in the distribution structure learning part.

\subsection{Mean or Mode models}
Alternately, rather than modeling the pdf $f_{Y|X}$, one may choose a hypothesis class composed of labeling functions of the form $f(X; W) \to ran(Y)$ where $W$ is the parameter variable.

This may actually correspond to proposing deterministic labeling functions which return either label mode or the mean (in case of vector labels). In the latter case, we may also be interested in specifying or modeling $var_{Y}[Y|X=x]$ to indicate our confidence in the estimate.

\subsubsection{Comparison to probabilistic models}
Depending on the loss functions, the risk of different randomized labeling rules with the same expectation may be different. So, when it is reliably possible to do so, modeling the pdf $f_{Y|X}$ is probably a better as one can then evaluate confidence, risk etc.. involved in making a classification decision.

Yet, task of modeling the expectation or mode of $Y$, being relatively simpler, lends itself to better modeling and learning.

\subsection{Probabilistic models: comparison}
Often, modeling $f_{Y|X}$, requiring fewer modeling assumptions, may be more easily and accurately done.

But, modeling $f_{X,Y} = f_{X|Y}f_Y$ gives us more expressiveness. For example, in case of unbalanced data it may be important to explicitly model $f_Y$; and it can be estimated easily using the empirical distribution based on $\set{(X_{i}, L_{i})}$.

\subsubsection{Discriminative model corresponding to generative model}
Consider the form of the discriminative model $f_{Y|X, W}$ yielded by the generative model $f_{X|Y} f_Y$, which is parametrized by $W$. When derived using the generative model, the range of $W$ (and therefore $f_{Y|X, W}$) is restricted by the form of $f_{X|Y} f_Y$. So, the hypothesis class $D$, where $W$ has no restrictions, is atleast as large as the hypothesis class constituted by labeling rules corresponding to the generative mode. So, the pros and cons of hypothesis classes of different sizes, as discussed in the decision theory chapter, apply.

\subsubsection{Ease in using unlabeled points}
Suppose that the distribution family $Pr(y, x|w)$ is parameterized by $w$. Then, in picking the best $w$, one often maximizes the likelihood of observing datapoints $\set{(x^{(i)}, y^{(i)})}$.

If we have unlabeled observations $\set{x^{(j)}}$, as in semi-supervised learning to make predictions: you can then select $w$ which account for these unlabeled points along with labeled points!


\section{Discrete deterministic labeling rules}
\subsection{Decision surfaces}
One can view discrete labeling rules $h$ as dividing $ran(X)$ into decision regions separated by decision boundaries/ decision surfaces, which correspond to all $x$ such that $Pr(Y=y_1|X=x, S) = Pr(Y=y_2|X=x, S)$ where $S$ is the training data.

\subsection{k-ary classifier from binary classifier}
k-ary classification reducible to binary classification in many ways.

Can learn many 'one against rest' classifiers; and then assign the class corresponding to the deepest distance the point achieves from the decision hyperplane.

Can learn many one against one classifiers; and then use majority vote for prediction. This approach often results in ambiguous regions.

\subsection{Curse of dimensionality}
\tbc Exponential increase in volume associated with adding extra dimensions: can't calculate and record $f_{X|C_{k}}(x)$ or $Pr(C_{k}|x)$ for exponential number of cases as you don't have so many examples. More difficult to answer 'How are data points belonging to same class similar?'

See also curse of dimensionality subsection in the clustering problem.

\chapter{With additional unlabeled data-points}
Assume points belonging to the same class cluster together, using unlabeled data with labeled data, you can draw better decision boundaries around clusters belonging to various class: probability density is important in classification.

\section{Generative approaches}
\tbc

\section{Label propagation on graphs}
Make the similarity graph G=(V, E) of the n points, $\set{X_i}$. Some nodes $X_T$ have labels, others don't have labels; let $k$ be the indicator vector with $k_i = [i \in T]$; take K = diag(k). 

\subsection{Quadratic criterion}
\subsubsection{Binary labels}
Denote proposed labels of the n points as bits in a binary vector f. Let y be the label vector, with $y_T = c(X_T)$ and $y_{V - T}$ having arbitrary values.

$\min_{f \in \set{0, 1}^{n}} \sum_{(i, j) \in E}W_{i, j}(f_i - f_j)^{2} + l(f-y)^{T}K(f-y)$. This finds an f which is smooth in that $f_i$ and $f_j$ are similar if they are connected by an edge in G, but also which agrees with $y_T$ as much as possible.

\subsubsection{Discrete labels}
Denote the $L$ possible labellings thus: $\set{e_i}$: as L-dim indicator vectors. Denote labels of the n points as columns in the binary matrix F, where $\forall i : F_{:,i} \in \set{e_j}$. Let Y be the label matrix of similar dimensions, with columns $Y_{:, T} = c(X_T)$.

$\min_{F_{:, i} \in \set{0, 1}^{n}} \sum_{(i, j) \in E}W_{i, j}(F_{:,i} - F_{:,j})^{2} + l\sum_i (F_{:,i}-Y_{:,i})^{T}K(F_{:,i}-Y_{:,i})$. This is akin to solving $L$ binary label propogation problems simultaneously: propogating 1 label bit at a time. \exclaim{Note that the binary case is a special case of this.}

\exclaim{Combinatorial hardness in both cases!}

\subsection{Rewriting using Graph Laplacians}
The objectives are equivalent to $\min f^{T}Lf + l (f-y)^{T}K(f-y)$ and \\
$\min tr(F^{T}LF) + l tr((F-Y)^{T}K(F-Y))$. $\sum_{(i, j) \in E}W_{i, j}(f_i - f_j)^{2} = f^{T}Lf$ is shown in graph theory ref; and the latter equivalence is easily obtained by using this as a component.

Using the normalized graph laplacian $N$, get the objective $\min tr(F^{T}NF) + l tr((F-Y)^{T}K(F-Y))$. In graph theory ref, see that $f^{T}Nf$ is a normalized smoothness measure.

\subsection{Real relaxation: solve linear system of eqns}
In both the binary and the general discrete case, drop the constraints $f \in \set{0, 1}^{n}, F_{:, i} \in \set{0, 1}^{n}$. Allow any real value; solve this relaxed problem to get f or F, and get to the closest binary f' and F'.

From optimality conditions, the solution is $(L+K)F = KY$.

\subsection{Low rank approximation for fast solution}
(L+K) is $n \times n$, and solving this system of equations is $O(n^{3})$ naively. Instead, can use low rank factorization $TT^{*}$ where $T \in R^{n \times k}$ to solve this in $O(nk^{2})$.

But, finding low rank approximation requires finding sv (same as finding ev for symmetric matrices), which is $O(kn^{2})$. Can surpass this using eigenfunctions. \why

\chapter{Using labels from other viewpoints}
\section{Data point Neighborhood approach}
Make data pts vs data pts matrix which measures similarities; filled using labels from various, usually similar, viewpoints. Thence predict label from target viewpoint. Eg: Amazon: 'others who bought this also bought that'.

\section{Collaborative filtering}
\subsection{Latent factor approach}
Assume small number of latent random variables, which combine together to form various view points and data points, cause the labels.

\subsubsection{Low rank factorization}
Let values of latent RV behind data pt j be $v_{j}$, behind viewpoint i be $u_{i}$; then get $Y_{i,j} = u_{i}^{T}v_{j}$. Want to have few random variables, so want $Y = U^{T}V; U \in R^{q \times N}; V \in R^{q \times D}$, a low rank factorization of Y. Thence fill missing values.

Can use SVD: finds best rank $k$ solution to $\norm{Y - (U\SW )V^{*}}_F$. instead want to find $\norm{W \kron (Y  -  U^{T}V)}$, where W is mask matrix to indicate known values in Y.

\subsection{Association rule mining}
Look at co-occurances of various items. Eg: Walmart purchases.

Very slow.

\chapter{Vector labels prediction: Regression}
\section{General problem}
Many (x, y) pairs (observations), h(x, w) form (eg: degree of polynomial) known, parameters $w$ in h(x, w) is unknown. Want to find $w$. range(h) is not finite, it is usually continuous.

y is the response variable, $w$ is called the regression vector. The matrix formed by $x$ is often called the design matrix.

Many such continuous valued models are described in probabilistic models reference.

\section{Linear regression}
\subsection{The problem}
h(x, w) is linear in $w$ (Eg: $w_1 x^{3}+ w_2 $x$ + w_3=0$: $\ftr_{i}(x) = x^{i}$).

Make matrix A with each row as feature vector $\ftr(x)$ at data point $x$; take b at diff points; coefficients as variable vector $w$.

\subsection{The solution}
Want to tune $w$ so that it yields least deviation from b. You measure this using various loss functions.

\subsubsection{Quadratic loss function}
Get least squares problem $\min e(w) = \min \norm{Aw - b}_{2}$. This is symmetric, but is sensitive to outliers.

\subsection{Maximum likelihood estimate with Gaussian noise}
If you view y as h(x, w) + gaussian noise n, least squares solution is also the maximum likelihood solution.

Noise distribution symmetric about the mean is not sufficient to lead to least squares solution: $\min e(w) = \min \norm{Aw - b}_{4}$ symmetrically penalizes deviation from mean just as well.

\subsection{Imposing prior distributions on w}
Solutions below assume quadratic loss function to measure deviation from b. Priors implied by regularizers in $\min e(w) = \min \norm{Aw - b}_{2} + p(w)$ where p is some penalty function. Usually $p(w) = \norm{w}_{k}$.

\subsubsection{Quadratic regularizer}
Assuming gaussian noise, the maximum a-posteriori solution yields the ridge regression problem.

\subsubsection{Priors which prefer sparse w}
Can use lasso, or compressed sensing. See optimization ref.

\paragraph*{Statistical efficiency}
N samples, $d$ dimensions. $E[\norm{\hat{\gth} - \gth^{*}}_2] \leq \sqrt{\frac{s \log d}{N}}$.

\subsection{Solution}
See optimization ref.

\chapter{Prediction with fully labeled data}
Goals and formulations are presented elsewhere.

\section{Binary classification}
See the many hypothesis classes/ parametrized model families in the colt ref, boolean functions ref.

\section{Non parametric methods}
\subsection{k nearest neighbors}
A discriminative, non parametric approach. Number of examples: N. Samples: $S = (x_{1}, .. x_{N})$. There are $k$ classes. To classify $x$,  find $k$ nearest neighbors in S; take their majority vote.

So, can't ever throw away data points.

\section{Linear models for discrete classification}
Linear separability of data sets or feature space: Decision surfaces are (p-1) dim hyperplanes in feature space.

$h(x, w) = f(w^{T}x + w_{0}) = f(v^{T}x')$ with $x' = (x, 1)$: $f$ is activation function or link function; $w$ is the weight vector; $w_{0}$ is the bias. So without loss of generality, we can restrict ourselves to considering only hyperplanes passing through the origin.

Decision surfaces are h(x, v) = constant or $v^{T}x'$ = constant, so linear in $x$; Not linear in terms of $w$ due to possible non linearity of $f$: so called generalized linear model.

For binary classification, this becomes a halfspace: see boolean function ref and colt ref.

For geometric properties of separating hyperplane, see boolean function ref.

\subsection{Arbitrary separator from fully separable training set}
When the training set is fully separable, one can pick one of the many separating hyperplane easily, for example, using linear programming. For other such algorithms, see computational learning theory ref.

To select the best among the candidate hyperplanes, one can use some sort of regularization, as in maximum margin classifiers.

\subsection{Winnow: multiplicative update}
Let weight of $c$ be $W$. Set weights $a_{i} = 1$. Multiplicative update rule: if $x_{i}$ agrees with c(x), set $a_{i} = a_{i}(1+\del)$; set $a_{i} = a_{i}/(1+\del)$ for others. $mb = O(W^{2} \log n)$. \why Sometimes better than additive update rule used in the perceptron algorithm.

This is very similar to the 'panel of experts' algorithm. Also see note in the section on perceptron algorithm comparing this with the 'panel of experts' algorithm.

\subsection{Perceptron learning alg for halfspaces}
\subsubsection{The problem}
The classifier: A hyperplane c through origin perfectly classifies labeled data; unit vector $u \perp c$ defines c; $c(x_i) = sgn(\dprod{u, x_i})$.

The data: $x \in R^{n}$; $\norm{x_i} = 1$; $S = \set{x_i}$. Geometric margin of X wrt u: $g = \min_{x \in S} |\dprod{u, x}|$; or $sgn(\dprod{u, x_i})\dprod{u,x_{i}} \geq g$. Note: g = function(S).

Want to find $c$.

\subsubsection{The algorithm}
$u_{0} = 0$. Additive update rule: If mistake on $x_i$: $u_{i+1} = u_{i} + sgn(\dprod{u, x_i})x_i$: hyperplane orientation tilted towards correcting the mistake.

\subsubsection{Convergence to u}
$mb = O(g^{-2})$. In general, if $\norm{x_i} \leq R$; $mb = O((\frac{R}{g})^{2})$.

By induction, using update rule expression for $u_t$: $\norm{u_t} = \norm{u_t}\norm{u} \geq \dprod{u, u_t} \geq tg$. So, if the length of $u_t$ is not increasing too much, may be perceptron is getting closer to u as more mistakes made.

Also, by induction, the update rule and the fact that $u_{t-1}$ misclassified $x_{t-1}$, causing the update: $\norm{u_{t}}^{2} \leq t$.

So, $(tg)^{2} \leq t$; and $t\leq g^{-2}$.

\subsubsection{Comparison}
Perceptron Algorithm is usually faster than than LP. Is exponential when $g \leq 2^{-c}$: this is rare.

For a given $g$, we can find good enough halfspace with mb $O((\frac{R+D}{g})^{2})$. \chk Perhaps the winnow algorithm, which uses multiplicative update is more efficient.

Has connection to halfspace learning with noise. \why

Assumes that the data is perfectly separable. So, often less preferable than soft margin SVM's. But, a soft margin variant of perceptron algorithm is known.

\paragraph*{With panel of experts algorithm}
Compare the perceptron algorithm $B$ with the algorithm $A$ described in the learning theory survey, which for any given sequence of inputs, using a panel of experts achieves a mistake bound comparable with the mistake bound achieved by the best expert. In the case of halfspaces, every input bit $x_i$ can be viewed as an 'expert'.

Upon making a mistake, $A$ updates the weight of only the experts which made a mistake, whereas $B$ updates weight assigned to every expert.

The weights used in $A$ were all positive, whereas weights used in $B$ can be negative: but this distinction is minor, as it can perhaps be accounted for in the 'experts algorithm' by introducing experts corresponding to $-x_i$.

\section{Maximum margin classifier}
\subsection{The problem}
A discriminative, parametric approach. Number of examples: N. Samples: $(x_{1}, .. x_{N})$, label function $c:X \to \set{\pm 1}$.

Suppose $y(x) = w^{T} \ftr(x) + w_0$ with $y(x)c(x) > 0 \forall x$, for some $w, w_0, \ftr$. So finding a separating hyperplane (see halfspaces in boolean function ref) in some feature space.

\subsection{Hard margin}
\subsubsection{Primal}
To maximize margin, solve: $\max_{w,w_0}[\frac{\min_{n}[y(x_{n})c(x_{n})]}{\norm{w}}]$. Scale w, $w_0$ so that $\min_{n}[y(x_{n})c(x_{n})] = 1$; thence get $\equiv$ problem $\min_{w,w_0}\frac{\norm{w}^{2}}{2}$: $y(x_{n})c(x_{n}) \geq 1$. Prediction: sgn(y(x)).

Can solve using Quadratic programming (QP).

\subsubsection{Dual}
Get Lagrangian $L(w, w_0, a) = \frac{\norm{w}^{2}}{2} + \sum a_{n}[1-(w^{T} \ftr(x_n) + w_0)c(x_{n})]$; $a_{n} \geq 0$. Get dual: $g(a) = \inf_{w, w_0} L(w, w_0, a)$; Set $\gradient_{w, w_0} L(w, w_0, a) = 0$: $w = \sum_{n}a_{n}c(x_{n}) \ftr(x_{n}); 0 = \sum a_{n}c(x_{n})$. So, dual problem: $\max_a g(a) = \max \sum a_{n} - 2^{-1}\sum_{n}\sum_{m} a_{n}a_{m}c(x_{n})c(x_{m})k(x_{n}, x_{m})$: $a_{n} \geq 0; \sum a_{n}c(x_{n}) = 0$.

Can solve using QP too. This form is useful where $dim(\ftr(x)) >>N$.

\paragraph*{KKT conditions}
Primal feasible: $y(x_{n})c(x_{n}) \geq 1$. Dual feasible: $a_{n} \geq 0; \sum a_{n}c(x_{n}) = 0$. Complementary slackness: $a_{n}[1 - c(x_{n}) y(x_{n})] = 0$.

So, $\forall n: a_{n} = 0 \lor c(x_{n})y(x_{n})= 1$: in latter case, you have support vectors. So, aka Support Vector Machine (SVM). Take S: number of support vectors.

\paragraph*{Predictor}
Substituting for w, get: $y(x) = \sum_{n}a_{n}c(x_{n})k(x_{n}, x) + w_0$: only S terms actually appear with $a_{n} \neq 0$: so SVM is fast to evaluate. So, $w_0 = \frac{\sum_{m} [c(x_{m})y(x_m) - \sum_n a_{n}k(x_{n}, x_{m})]}{N}$.

\subsection{Soft margins}
Allow some points to be misclassified or to be below the margin, but linearly penalize such outliers.

\subsubsection{Primal}
So, use slack variables: $\gx_{n} \geq 0$; Instead of $y(x_{n})c(x_{n}) \geq 1$, use constraint $y(x_{n})c(x_{n}) + \gx_{n} \geq 1$.

$\min C\sum_{n=1}^{N} \gx_{n} + \frac{\norm{w}^{2}}{2}$: $C$ is tradeoff between penalty on $\gx$ and margin; saying $\sum \gx_i \leq G$; so controls model complexity. As $C \to \infty$, get hard margin SVM.

\subsubsection{Dual}
Lagrangian: $L(w, w_0, \gx, a, m) = \frac{\norm{w}^{2}}{2} + C\sum_{n=1}^{N} \gx_{n} + \sum a_{n}(1 - c(x_{n})y(x_{n}) - \gx_{n}) + \sum \gm_{n}\gx_{n}$: $a \geq 0, \gm \geq 0$. Set $\gradient_{w, w_0, \gx} $L$ = 0$: $w = \sum a_{n}c(x_{n})\ftr(x_{n}), \sum a_{n}c(x_{n}) = 0, a_{n} = $C$ - \gm_{n}$. Thence get dual g(a), with objective function same as hard-margin case with constraints: $0 \leq a_{n} \leq C$: as $\gm_{n} \geq 0; \sum a_{n}c(x_{n}) = 0$.

\paragraph*{KKT conditions}
Primal feasible: $1- c(x_{n})y(x_{n}) - \gx_{n} \leq 0$. Dual feasible: $a_{n} \geq 0, \gm_{n} \geq 0$. Complimentary slackness: $a_{n}(1 - c(x_{n})y(x_{n}) - \gx_{n}) = 0, \gm_{n}\gx_{n} = 0$.

So, support vectors now are points on or within certain margin from hyperplane. Predictor same as hard margin case.

\chapter{Sparse signal detection}
\section{Problem}
\subsection{Generating process}
Suppose that the $p$ dimensional 'observation vector' $Y$ is generated from $Y_i \distr N(\gth_i, \stddev^2) = \gth_i + N(0, \stddev^2)$. $\stddev$ is known.

In addition suppose that $\gth$ is sparse. The set $K = \set{i: \gth_i \neq 0}$ is called the signal set.

$n$ observations $\set{Y_i = y_i}$ are made. Usually $n << p$.

\subsection{Decision rule sought}
The problem for the decision rule, given $Y_i$ (the observation), is to estimate $\gth_i$. More simply, one might seek decision rules to estimate the indicator variable $I[\gth_i \neq 0]$ given $\gth_i$.

\subsection{As a classification problem}
This is essentially a classification problem with some peculiarities. It is an abduction problem (the test points are known beforehand), and no labeled training set is provided.

Framing it as a classification problem is a good way to state the final goal, but one can not apply solution ideas typical of classifiers naturally. So, this view is not very informative.

\subsection{Peculiarities}
If the number of signals, $|K|$ were known beforehand, the problem would be trivial: one would just select the top $|K|$ elements of $\hat{E}[Y]$.

\section{Risk}
Identifying non-signals as signals often carries an especially high penalty. Eg: In case of gene-expression data, in response to certain conditions, the expression (ie, signalness) of each gene identified as being a signal is verified using laborious wet-lab experiments.

So, it is often hard to express a formula for evaluating the actual risk of a decision procedure, yet one can make qualitative statements about it. Yet, one can define a simpler risk function and show that a decision procedure chosen using a certain process will be low risk. \tbc

\section{Hypothesis classes}
\subsection{Desired qualities}
\subsubsection{Sparsity}
The main point in modeling $\gth_i$ is to ensure that the model results in sparse $\gth$: that is $\gth_i$ should often be close to 0.

\subsubsection{Adaptability to different sparsity levels}
The hypothesis class should include $\gth$ of different sparsity levels.

\subsubsection{Robustness to large signals}
The hypothesis class should include $\gth$ with arbitrarily large components.

\subsection{Probabilistic models}
\tbc

\subsubsection{Scale mixture models}
Scale mixture models for $\theta_i$ say: $\gth_i|\gl_i = N(0, \gl_i^2)$. 

\chapter{Sequential data points}
\section{Trajectory prediction}
\subsection{Problem}
Consider sequential data: observations $S$ consisting of labels $L_1 .. L_n$ observed at different positions $X_1.. X_n$ (perhaps times). We want to predict $L_{n+1}$ corresponding to $X_{n+1}$.

The underlying process is such that the distribution of $L_{n+1}$ depends on both $S$ and $X_{n+1}$. 

\subsection{Simplifications}
Even predicting a lower bound or upper bound (whp) of $L_{n+1}$ may be useful.

\subsection{Applications}
Predicting trajectory of a missile, market price of a security tomorrow.

\part{Applications}
Also see AI survey.

\chapter{Search results}
\section{Ranking search query results}
Queries $q \in Q$: bag of words, Set of documents D: a seq of words. For query $q_{i}$, retrieved relevant documents $D_{i} \subseteq D$; Got full or partial orderings $L_{i}$. Let $Q = \set{q_{i}}$. Maybe want to complete Q vs D relevence level matrix. Or, given docs D' relevant for unseen query q, want to rank D'.

\subsection{Feature extraction}
$\ftr: Q \times D \to X \in R^{D}$; D is usually $\approx 10$. $\ftr_{i}(q,d)$ could be TF/IDF, common word count etc.. Let $f:X \to R$ be scoring function; it induces a relevance order $\set{z_{i}}$ on D for every query $q_{i}$; let $F = \set{f}$.

\subsection{Objectives to optimize}
Let $y_{i}$ be ground truth relevance-ordering of D corresponding to query $q_{i}$; let y be induced by $g:X \to R$.

Loss function: $L(f, q_{i})$ measures deviation of $z_{i}$ from $y_{i}$.\\
Risk: $R = \int_{Q}L(f,q) f_Q(q)dq$. Want to find f which minimizes L. But Pr(q) usually unknown; so minimize empirical risk: $\hat{R}=n^{-1} \sum_{i} L(f,q_{i})$. Assume that $\abs{R - \hat{R}} \to 0$.

\subsection{Various Loss functions L}
\subsubsection{Pointwise}
\tbc

\subsubsection{Pairwise}
Checks if this is violated: $y_{i}(d)>y_{i}(d') \implies f(d)>f(d')$; so $(y_{i}(d) - y_{i}(d'))(f(d) - f(d')) \geq 0$. Widely used. Eg: SVM ordinal regression; ranknet (best pairwise function as of 2009); lambda-rank: $f(d) = w^{T}d$.

\subsubsection{Listwise}
Measure badness of a bigger part of the ranking, not just correctness of ranking of pairs of documents. $q_{i}$ uniquely identified by $(y_{i}, D)$; so can consider $L(f, y_{i},D)$ or $L(z_{i}, y_{i})$.

\paragraph*{Using a probability distribution on permutations}
Define prob distribution over permutations $Pr_{f}(\pi)$ with good properties: favors permutations rated highly according to f to permutations rated lower, best permutation according to f has max probability. So find f whose $Pr_{f}$ is closest to $Pr_{g}$. Can minimize cross entropy: $L(f, g) = - \sum_{\pi} Pr_{g}(\pi) \log Pr_{f}(\pi)$ (List net); or $L(f, g) = - \sum \log Pr(y_{i}|D, f)$: thereby maximizing likelihood (List-MLE). These Listwise loss calculation is hard: may need to consider n! permutations: so instead consider only ranking of top $k$ docs induced by f. List-MLE known to be best Listwise loss function based ranking method (2009), also better than all pairsise loss function based methods.

\chapter{Document clustering}
\section{Document classification and clustering}
Aka text mining. Corpus of documents. Usually position of document in corpus ignored in modeling, just like bag of words assumption.

\subsection{Feature extraction}
Document is a string of words; but want a simpler representation.

Maybe just the set of words which appear is most important: use word-counts. Maybe number of occurances is also important.

\paragraph*{Burstiness}
First time a word appears, it is very informative; but its second occurance is much less surprising/ informative. Rarer the word, the more this is true.

\subsubsection{Bag of words assumption}
Aka word exchangability. Each document modeled as just a bunch of words. Ignore stopwords, words with low I(word; document).

\subsection{Dimensionality reduction}
Results in noise reduction: good way of tackling synonyms. Considers the entire corpus.

\subsubsection{Approaches}
Can look at the feature vs item matrix, and try to find a low rank approximation for it. This yields the latent factors behind features and items.

Or, can describe a parametrized process which creates a vector in the feature space (an item), and find the parameters which generated each item. This yields the latent factors behind items.

\subsection{Model class distribution using word counts}
Can take word counts, then use multinomial distribution to model class distr. But this ignores burstiness; so it is appropriate only for common words.

\paragraph*{DCM distr}
Better model to utilize avg rare, very rare words. Draw document specific multinomial distribution.

\subsection{Classification}
Documents naturally belong to many classes.

\subsection{Clustering}
Coclustering useful. Clustering words also useful in query enhancement.

Coclustering : Can view as a bipartite graph clustering problem (Dhillon).

\chapter{Web portal related}
\section{Pick content}
Match content with user intent.

Long term objective: Maximize user experience. Short term, meterable, objective: Maximize click-through rates (CTR) subject to constraints (Eg: don't show porn).

\section{Pick content-layout}

\section{Maximize ad revenue}
\subsection{Match advertisements with content}
\tbc

\section{Data}
Inventory often expires: Eg: news articles; cause: user fatigue.

Data is huge.

\subsection{User responses}
Responses are multivariate: click rates, ratings etc.. Negative responses very noisy.

\section{Online experimentation}
Comparing models tough without experiments.

Usually allowed on a fraction of visits. A generalization of multi-armed bandits: See Game theory ref.

\chapter{Spoken dialog system (domain-specific)}
\section{Problem}
The problem is for the user to communicate a task to a machine through a verbal dialog. A dialog consists of several 'turns', with the user and the machine speaking alternately within each turn.

Since the machine's 'belief' (aka dialog state) about the user's task changes as the dialog proceeds, this problem is also called belief-tracking.

\subsection{Examples}
Voice mail system. Bus route information system. Route system.

\subsection{Domain ontology}
The set of concepts the interaction involves is often limited. For example, in the voice-mail system, the concepts may be limited to Name, Message, Action (having values 'record message', 'save message', 'delete message'). This naturally limits the language internally used by the machine, though to interact with the user a richer language may be used.

\section{Interaction graph}
The chain of events involved in dialog-processing is as follows. First the user has a goal $S_u$ he wants to communicate. Also, the dialog itself has a current state $S_d$, based on what has already been uttered. $S_u, S_d$ cannot be observed by the machine.

The user then speaks something - ie acts on an intention $A_u$. This is transformed into speech/ percieved action $Y_u$.

This is processed by the Automatic Speech Recognition (ASR) module, which provides a scored list of possible phrases the user has uttered.

The output of ASR is passed to a (usually domain specific) parser or natural language understanding (NLU) module. This outputs a scored list of utterences: $\set{(A'_u, c)}$ in a certain well-defined language. $A'_u$ is usually in a language specific to the domain, eg: (Bus = 40, start = StopA, end = StopB).

The output of the NLU is then passed to the belief tracking system, which then produces a scored list of beliefs, or a belief state $S_m$ which is meant to approximate $(S_u, S_d, A_u)$.

A decision-making module (aka decision manager) considers $S_m$ and chooses what action $A_m$ to perform (including what to say to the user). $A_m$ in-turn affects the dialog model/ belief state.

$A_m$ is then used by the language generation modules to communicate with the user. Thence, speech/ action $Y_m$ is produced, which is then decoded as representing $A'_m$ by the user.

The above can be visualized as a directed graph with various components being represented by nodes and messages labelling arrows.

The ASR and NLU modules use Finite State Transducers for their functioning.

\chapter{Others}
\section{Predicting movie ratings}
Netflix problem: rating(movie, user, time) for some triples available; make prediction for unknown triples. 


\end{document}
