<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>&#43;Distribution structure learning on Vishvas&#39;s notes</title>
    <link>https://vishvAsa.github.io/notes/math/probability/statistics/distribution_structure_learning/</link>
    <description>Recent content in &#43;Distribution structure learning on Vishvas&#39;s notes</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://vishvAsa.github.io/notes/math/probability/statistics/distribution_structure_learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>0 Problems</title>
      <link>https://vishvAsa.github.io/notes/math/probability/statistics/distribution_structure_learning/0_problems/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://vishvAsa.github.io/notes/math/probability/statistics/distribution_structure_learning/0_problems/</guid>
      <description>&lt;h2 id=&#34;conditional-distributions-and-notation&#34;&gt;Conditional distributions and notation&lt;/h2&gt;&#xA;&lt;p&gt;Got observations of events: RV \(X\) took values \(\set{x_{i}}\), deduce/ model the process causing those events. In general, we want to model the conditional distributions \(f_{X_r|X_{\lnot r}}\). Often, we use the alternate notation \(Y = X_r\) and \(X = X_{\lnot r}\).&lt;/p&gt;&#xA;&lt;h2 id=&#34;connection-to-modeling-marginal-density&#34;&gt;Connection to modeling marginal density&lt;/h2&gt;&#xA;&lt;p&gt;Note that \(X_{\lnot r}\) may be empty, so that marginal/ unconditional distribution modeling - which is estimating \(f_X(X)\) - is a special case of conditional distribution modeling.&lt;/p&gt;</description>
    </item>
    <item>
      <title>1 Parameter estimation</title>
      <link>https://vishvAsa.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://vishvAsa.github.io/notes/math/probability/statistics/distribution_structure_learning/1_parameter_estimation/</guid>
      <description>&lt;h2 id=&#34;estimate-parameters-using-statistics&#34;&gt;Estimate parameters using statistics&lt;/h2&gt;&#xA;&lt;p&gt;The distinction between choosing parametric and non-parametric approaches are considered in the decision theory section.&lt;/p&gt;&#xA;&lt;h3 id=&#34;statistic-estimator&#34;&gt;Statistic, estimator&lt;/h3&gt;&#xA;&lt;p&gt;A statistic \(\hat{t} = \hat{g}(X)\) is a function of the sample \(X\); an observable random variable. When it is used to estimate some parameter, it is called an estimator. t can be estimated by estimating \(\gth\).&lt;/p&gt;&#xA;&lt;h4 id=&#34;point-estimation-of-the-parameter&#34;&gt;Point estimation of the parameter&lt;/h4&gt;&#xA;&lt;p&gt;If \(\hat{t}\) tries to approximate \(t\), it is an estimator.&lt;/p&gt;</description>
    </item>
    <item>
      <title>2 Mean and variance</title>
      <link>https://vishvAsa.github.io/notes/math/probability/statistics/distribution_structure_learning/2_mean_variance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://vishvAsa.github.io/notes/math/probability/statistics/distribution_structure_learning/2_mean_variance/</guid>
      <description>&lt;h2 id=&#34;mean-estimation&#34;&gt;Mean: estimation&lt;/h2&gt;&#xA;&lt;h3 id=&#34;consistency&#34;&gt;Consistency&lt;/h3&gt;&#xA;&lt;p&gt;Aka Law of large numbers&lt;/p&gt;&#xA;&lt;p&gt;Let \(\set{X_{i}}\) iid. \(\hat{X}&lt;em&gt;{n} = n^{-1}\sum^{n} X&lt;/em&gt;{i}\). As \(var[\hat{X_{n}}] = \stddev^{2}/n \to 0\) as \(n \to \infty\), Weak law: \(\hat{X}_{n}\) is a consistent estimator of \(\mean\).&lt;/p&gt;&#xA;&lt;h3 id=&#34;normalness-of-estimator-distribution&#34;&gt;Normalness of estimator distribution&lt;/h3&gt;&#xA;&lt;p&gt;Aka Central limit theorem (CLT)&lt;/p&gt;&#xA;&lt;p&gt;Take estimator \(U_{n} = \frac{\bar{X} - \mean}{\frac{\stddev}{\sqrt{n}}}\). \(lt_{n\to \infty} Pr(U_{n} \leq u) = \int_{-\infty}^{u} \frac{1}{\sqrt{2\pi}}e^{-t^{2}/2}dt\): so approaches CDF of N(0,1): See convergence of moment generating function (MGF) below. So, as n increases, \(var[\bar{X}]\) becomes smaller: visualize pdfs of \(X, \bar{X}&lt;em&gt;{30}, \bar{X}&lt;/em&gt;{50}\); see how curve becomes more normal and gets thinner and taller. Generally, can use CLT when \(n&amp;gt;30\).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Conditional independence</title>
      <link>https://vishvAsa.github.io/notes/math/probability/statistics/distribution_structure_learning/conditional_independence_structure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://vishvAsa.github.io/notes/math/probability/statistics/distribution_structure_learning/conditional_independence_structure/</guid>
      <description>&lt;h2 id=&#34;problems&#34;&gt;Problems&lt;/h2&gt;&#xA;&lt;p&gt;In all of the following, we suppose that \(f_X(x)\) is given by a graphical model.&lt;/p&gt;&#xA;&lt;h3 id=&#34;model-estimation&#34;&gt;Model Estimation&lt;/h3&gt;&#xA;&lt;p&gt;The most ambitious goal is to estimate the parameters associated with the distribution. This can often be accomplished by minimizing the log loss associated with the conditional distribution \(f_{X_i|X_{\nbd(i)}}\), once the structure of the underlying graphical model has been estimated (ie feature selection is done).&lt;/p&gt;&#xA;&lt;h3 id=&#34;edge-recovery&#34;&gt;Edge recovery&lt;/h3&gt;&#xA;&lt;p&gt;Deduce the graph encoding the conditional independence relationships among features.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Density estimation</title>
      <link>https://vishvAsa.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://vishvAsa.github.io/notes/math/probability/statistics/distribution_structure_learning/density_estimation/</guid>
      <description>&lt;h2 id=&#34;importance&#34;&gt;Importance&lt;/h2&gt;&#xA;&lt;p&gt;Fitting a model to observations, ie picking a probability distribution from a family of distributions, is an important component of many statistics tasks where one reasons about uncertainty by explicitly using probability theory. Such tasks are labeled &amp;lsquo;Bayesian inference&amp;rsquo;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;choosing-the-distribution-family&#34;&gt;Choosing the distribution family&lt;/h2&gt;&#xA;&lt;h3 id=&#34;observe-empirical-distribution&#34;&gt;Observe empirical distribution&lt;/h3&gt;&#xA;&lt;p&gt;Draw a bar graph, see what the curve looks like.&lt;/p&gt;&#xA;&lt;h3 id=&#34;given-expected-values-of-fns--seteftr_ix--mean_i---and-a-base-measure-h&#34;&gt;Given expected values of fns \( \set{E[\ftr_{i(X)}] = \mean_{i} } \) and a base measure h&lt;/h3&gt;&#xA;&lt;p&gt;Suppose we want to modify h as little as possible, under KL divergence, so that it has \(E[\ftr(X)] = \mean\). If h is U, then this is same as finding a maximum entropy distribution with \(E[\ftr(X)] = \mean\). Then, the solution belongs to the exponential family generated by h and \(\ftr()\): see probabilistic models ref.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Parametric density estimation</title>
      <link>https://vishvAsa.github.io/notes/math/probability/statistics/distribution_structure_learning/parametric_density_estimation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://vishvAsa.github.io/notes/math/probability/statistics/distribution_structure_learning/parametric_density_estimation/</guid>
      <description>&lt;h2 id=&#34;problem-and-solution-ideals&#34;&gt;Problem and solution ideals&lt;/h2&gt;&#xA;&lt;h3 id=&#34;density-estimation-using-a-distribution-class&#34;&gt;Density estimation using a distribution class&lt;/h3&gt;&#xA;&lt;p&gt;Suppose that parameter \(t\) specifies the distribution \(f_t(x_r|x_{\lnot r})\), where \(x_{\lnot r}\) can be empty! Let \(T\) be parameter space spanned by such \(t\); it represents the class of distributions which can be specified in this form.&lt;/p&gt;&#xA;&lt;p&gt;Given finite data set \(\set{x^{(i)}}\), we want to approximate an unknown target distribution \(D(x_r|x_{\lnot r})\) which may not belong to this distribution family using \(T\). The approximation can be a weighted combination of combination of distributions in \(T\).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Support estimation</title>
      <link>https://vishvAsa.github.io/notes/math/probability/statistics/distribution_structure_learning/support_estimation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://vishvAsa.github.io/notes/math/probability/statistics/distribution_structure_learning/support_estimation/</guid>
      <description>&lt;h2 id=&#34;estimate-support-of-a-distribution-d&#34;&gt;Estimate support of a distribution D&lt;/h2&gt;&#xA;&lt;p&gt;Find set \(S&amp;rsquo;\) such that \(Pr(x \notin S&amp;rsquo;)&amp;lt;p \in (0,1]\), given sample \(S\). Can be solved by probability density estimation techniques, but actually simpler.&lt;/p&gt;&#xA;&lt;p&gt;Visualization: take the input space; draw solid ovals around sampled points; the algorithm will draw a dotted oval around these, which will represent the support of the distribution.&lt;/p&gt;&#xA;&lt;h3 id=&#34;with-soft-margin-kernel-hyperplane&#34;&gt;With soft margin kernel hyperplane&lt;/h3&gt;&#xA;&lt;p&gt;Aka One Class SVM or OSVM.&lt;/p&gt;&#xA;&lt;p&gt;Given \(N\) examples \(\set{x_{i}}\); project to some feature space associated with kernel \(k(x,y) = \ftr(x)^{T}\ftr(y)\); want to find hyperplane \(w^{T}\ftr(x) - \gr\) such that all points in the support fall on one side of the hyperplane, outliers fall on the other side: support identifier \(f = sgn(w^{T}x - \gr)\); so, allowing a soft margin, want to solve \(\max_{\gr, w} \frac{\gr}{\norm{w}} + C\sum \gx_{i}\) such that \(w^{T}\ftr(x_{i}) + \gx_{i} \geq \gr, \gx_{i} \geq 0\); \(\equiv\) obj function: \(\min_{w, \gx, \gr} \norm{w}^{2}/2 + \frac{1}{\gn N} \sum \gx_{i} - \gr\), for some coefficient \(0 \leq \gn \leq 1\).&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
