<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Vishvas&#39;s notes  | Clustering</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.72.0" />
    
    
      <META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
    

    <link href="/storage/emulated/0/notesData/notes/math/probability/statistics/clustering/" rel="alternate" type="application/rss+xml" title="Vishvas&#39;s notes" />
    <link href="/storage/emulated/0/notesData/notes/math/probability/statistics/clustering/" rel="feed" type="application/rss+xml" title="Vishvas&#39;s notes" />

    <meta property="og:title" content="Clustering" />
<meta property="og:description" content="The clustering problem Given \(N\) points, want \(k\) clusters. Often, \(k\) is not known.
Use Summarizing data is important for generalization, understanding future data points.
Supplying labels to unlabeled data; thence classifying new data-points. Eg: Face recognition using Eigenfaces. Use to estimate distribution support and thence in novelty detection.
Criteria: Continuity vs compactness Consider a starfish: The continuity critierion will identify the 5 arms as 5 clusters, but the comactness criterion will fail." />
<meta property="og:type" content="article" />
<meta property="og:url" content="file:///storage/emulated/0/notesData/notes/math/probability/statistics/clustering/" />

<meta itemprop="name" content="Clustering">
<meta itemprop="description" content="The clustering problem Given \(N\) points, want \(k\) clusters. Often, \(k\) is not known.
Use Summarizing data is important for generalization, understanding future data points.
Supplying labels to unlabeled data; thence classifying new data-points. Eg: Face recognition using Eigenfaces. Use to estimate distribution support and thence in novelty detection.
Criteria: Continuity vs compactness Consider a starfish: The continuity critierion will identify the 5 arms as 5 clusters, but the comactness criterion will fail.">

<meta itemprop="wordCount" content="1279">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Clustering"/>
<meta name="twitter:description" content="The clustering problem Given \(N\) points, want \(k\) clusters. Often, \(k\) is not known.
Use Summarizing data is important for generalization, understanding future data points.
Supplying labels to unlabeled data; thence classifying new data-points. Eg: Face recognition using Eigenfaces. Use to estimate distribution support and thence in novelty detection.
Criteria: Continuity vs compactness Consider a starfish: The continuity critierion will identify the 5 arms as 5 clusters, but the comactness criterion will fail."/>

      
    

    <script src="/storage/emulated/0/notesData/notes/webpack_dist/dir_tree-bundle.js"></script>
    <script type="text/javascript">
    
    let baseURL = "file:\/\/\/storage\/emulated\/0\/notesData\/notes";
    let basePath = "/" + baseURL.split("/").slice(3).join("/");
    let siteParams = JSON.parse("{\u0022contactlink\u0022:\u0022https:\/\/github.com\/vvasuki\/notes\/issues\/new\u0022,\u0022disqusshortcode\u0022:\u0022vvasuki-site\u0022,\u0022env\u0022:\u0022production\u0022,\u0022githubeditmepathbase\u0022:\u0022https:\/\/github.com\/vvasuki\/notes\/edit\/master\/content\/\u0022,\u0022mainSections\u0022:[\u0022math\u0022],\u0022mainsections\u0022:[\u0022math\u0022]}");
    let pageDefaultsList = JSON.parse("[{\u0022scope\u0022:{\u0022pathPrefix\u0022:\u0022\u0022},\u0022values\u0022:{\u0022comments\u0022:true,\u0022layout\u0022:\u0022page\u0022,\u0022search\u0022:true,\u0022sidebar\u0022:\u0022home_sidebar\u0022}}]");
    let sidebarsData = JSON.parse("{\u0022home_sidebar\u0022:{\u0022contents\u0022:[{\u0022url\u0022:\u0022recdir:\/\/\u0022},{\u0022contents\u0022:[{\u0022title\u0022:\u0022‡§ú‡•ç‡§Ø‡•å‡§§‡§ø‡§∑‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/jyotiSham\/\u0022},{\u0022title\u0022:\u0022‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/sanskrit\/\u0022},{\u0022title\u0022:\u0022‡§Æ‡•Ä‡§Æ‡§æ‡§Ç‡§∏‡§æ\u0022,\u0022url\u0022:\u0022..\/mImAMsA\/\u0022},{\u0022title\u0022:\u0022Notes Home\u0022,\u0022url\u0022:\u0022..\/notes\/\u0022},{\u0022title\u0022:\u0022‡§ï‡§æ‡§µ‡•ç‡§Ø‡§Æ‡•ç\u0022,\u0022url\u0022:\u0022..\/kAvya\/\u0022},{\u0022title\u0022:\u0022‡§∏‡§Ç‡§∏‡•ç‡§ï‡§æ‡§∞‡§æ‡§É\u0022,\u0022url\u0022:\u0022\/..\/saMskAra\/\u0022},{\u0022title\u0022:\u0022Vishvas\u0027s home page\u0022,\u0022url\u0022:\u0022\/..\/\u0022}],\u0022title\u0022:\u0022‡§∏‡§ô‡•ç‡§ó‡•ç‡§∞‡§π‡§æ‡§®‡•ç‡§§‡§∞‡§Æ‡•ç\u0022},{\u0022title\u0022:\u0022\\u003ci class=\\\u0022fas fa-hand-holding-heart\\\u0022\\u003e\\u003c\/i\\u003eDonate Via Vishvas\u0022,\u0022url\u0022:\u0022https:\/\/vvasuki.github.io\/interests\/dharma-via-vishvas\/\u0022}],\u0022title\u0022:\u0022Parts\u0022}}");
    let autocompletePageUrl = "\/storage\/emulated\/0\/notesData\/notes\/data\/pages.tsv";
    
    var pageRelUrlTree = {};
</script>

    <script>
    
    let pageVars = {};
    pageVars.pageUrlMinusBasePath = "\/storage\/emulated\/0\/notesData\/notes\/math\/probability\/statistics\/clustering\/".replace(basePath, "/");
    pageVars.pageParams = {};
    pageVars.pageSource = "math\/probability\/statistics\/clustering.md";
    console.log(pageVars.pageSource);
    var pageDefaults;
    for (let possiblePageDefaults of pageDefaultsList) {
      if (pageVars.pageSource.startsWith(possiblePageDefaults.scope.pathPrefix)) {
        pageDefaults = possiblePageDefaults.values
      }
    }
    
    </script>
    <script src="/storage/emulated/0/notesData/notes/webpack_dist/main-bundle.js"></script>
    <script src="/storage/emulated/0/notesData/notes/webpack_dist/transliteration-bundle.js"></script>
    <script src="/storage/emulated/0/notesData/notes/non_webpack_js/disqus.js"></script>
    <script src="/storage/emulated/0/notesData/notes/webpack_dist/ui_lib-bundle.js"></script>
    <link rel="stylesheet" href="/storage/emulated/0/notesData/notes/css/fonts.css">
    <link rel="stylesheet" href="/storage/emulated/0/notesData/notes/css/@fortawesome/fontawesome-free/css/solid.min.css">
    

    <link rel="stylesheet" href="/storage/emulated/0/notesData/notes/css/@fortawesome/fontawesome-free/css/fontawesome.min.css">

    
    <link rel="alternate" hreflang="sa-Deva" href="#ZgotmplZ" />
    <link rel="alternate" hreflang="sa-Deva" href="#ZgotmplZ?transliteration_target=devanagari" />
    <link rel="alternate" hreflang="sa-Knda" href="#ZgotmplZ?transliteration_target=kannada" />
    <link rel="alternate" hreflang="sa-Mlym" href="#ZgotmplZ?transliteration_target=malayalam" />
    <link rel="alternate" hreflang="sa-Telu" href="#ZgotmplZ?transliteration_target=telugu" />
    <link rel="alternate" hreflang="sa-Taml-t-sa-Taml-m0-superscript" href="#ZgotmplZ?transliteration_target=tamil_superscripted" />
    <link rel="alternate" hreflang="sa-Taml" href="#ZgotmplZ?transliteration_target=tamil" />
    <link rel="alternate" hreflang="sa-Gran" href="#ZgotmplZ?transliteration_target=grantha" />
    <link rel="alternate" hreflang="sa-Gujr" href="#ZgotmplZ?transliteration_target=gujarati" />
    <link rel="alternate" hreflang="sa-Orya" href="#ZgotmplZ?transliteration_target=oriya" />
    <link rel="alternate" hreflang="sa-Beng-t-sa-Beng-m0-assamese" href="#ZgotmplZ?transliteration_target=assamese" />
    <link rel="alternate" hreflang="sa-Beng" href="#ZgotmplZ?transliteration_target=bengali" />
    <link rel="alternate" hreflang="sa-Guru" href="#ZgotmplZ?transliteration_target=gurmukhi" />
    <link rel="alternate" hreflang="sa-Cyrl" href="#ZgotmplZ?transliteration_target=cyrillic" />
    <link rel="alternate" hreflang="sa-Sinh" href="#ZgotmplZ?transliteration_target=sinhala" />
    <link rel="alternate" hreflang="sa-Shar" href="#ZgotmplZ?transliteration_target=sharada" />
    <link rel="alternate" hreflang="sa-Brah" href="#ZgotmplZ?transliteration_target=brahmi" />
    <link rel="alternate" hreflang="sa-Modi" href="#ZgotmplZ?transliteration_target=modi" />
    <link rel="alternate" hreflang="sa-Tirh" href="#ZgotmplZ?transliteration_target=tirhuta_maithili" />
    <link rel="alternate" hreflang="sa-Latn-t-sa-Zyyy-m0-iast" href="#ZgotmplZ?transliteration_target=iast" />
  </head>

  <body class="ma0 bg-near-white">
    


    
<header class="p-1 bg-yellow">
    <nav role="navigation">
      <div id="div_top_bar" class="row">
        <div class="col-md-3 justify-content-start">
          <a  href="/storage/emulated/0/notesData/notes/" class="btn col border">
            <i class="fas fa-gopuram ">Vishvas&#39;s notes <br/> Clustering</i>
          </a>
        </div>
        <div id="div_top_bar_right" class="col-md-auto d-flex justify-content-center">
          <form action="/storage/emulated/0/notesData/notes/search">
            <input id="titleSearchInputBox" placeholder="‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ø‡§ï‡§æ‡§®‡•ç‡§µ‡§ø‡§∑‡•ç‡§Ø‡§§‡§æ‡§Æ‡•ç" name="s"><i class="btn fas fa-search "></i>
          </form>
          <a href="/storage/emulated/0/notesData/notes/custom_site_search/"  class="btn btn-default">Full search</a>
          <select name="transliterationDropdown" size="1" onchange="module_transliteration.updateTransliteration()">
            <option value="devanagari" selected="">‡§∏</option>
            <option value="iast">ƒÅ</option>
            <option value="kannada">‡≤Ö</option>
            <option value="malayalam">‡¥Ö</option>
            <option value="telugu">‡∞ï</option>
            <option value="tamil_superscripted">‡Æï¬≤</option>
            <option value="tamil_extended">‡Æï</option>
            <option value="grantha">ëåÖ</option>
            <option value="gujarati">‡™Ö</option>
            <option value="oriya">‡¨Ö</option>
            <option value="assamese">‡¶Ö‡¶∏</option>
            <option value="bengali">‡¶Ö</option>
            <option value="gurmukhi">‡®Ö</option>
            <option value="cyrillic">–ø—É</option>
            <option value="sinhala">‡∂Ö</option>
            <option value="sharada">ëÜëëáÄëÜ∞</option>
            <option value="brahmi">ëÄÖ</option>
            <option value="modi">ëò¶ëòªëòöëò≤</option>
            <option value="tirhuta_maithili">ëíÅ</option>
          </select>
        </div>
        <div class="row col  d-flex justify-content-center">
          <div><a  name="previousPage" href="" class="btn btn-secondary">###<i class="fas fa-caret-left"></i></a></div>
          <div ><a name="nextPage" href="" class="btn btn-secondary"><i class="fas fa-caret-right"></i> ### </a></div>
        </div>
        <ul id="top-bar-right-custom" class="list-group list-group-horizontal">
        </ul>
      </div>
    </nav>
</header>

    
    <div class="row" name="contentRow">
      
      <aside class="col-md-3 card border " id="sidebar">
        <div id="sidebarTitle" class="card-title bg-light-gray border d-flex justify-content-between" >
          <a name="sidebarToggleLink" data-toggle="collapse" href="#sidebar_body" role="button" aria-expanded="true" aria-controls="sidebar_body" onclick="module_main.default.sidebarToggleHandler()">
            Menu <i class="fas fa-caret-down"></i></a>
        </div>
        <nav class="card-body p-0 collapse show" id="sidebar_body">
          <ul id="displayed_sidebar" class="list pl2 p-2 bg-yellow">
        </ul>
        </nav>
      </aside>
      <main class="col p-3" role="main">
        
<header class='border d-flex justify-content-between'>
    <h1 id="Clustering">Clustering</h1>
    
    <a id="editLink" class="btn btn-primary"  href="https://github.com/vvasuki/notes/edit/master/content/math/probability/statistics/clustering.md"><i class="fas fa-edit"></i></a>
    
</header>
<article>
  <aside id="toc_card" class="card border ">
    <div id="toc_header" class="card-title border d-flex justify-content-between">
        <a data-toggle="collapse" href="#toc_body" role="button" aria-expanded="true" aria-controls="toc_body">
          What's in this page? <i class="fas fa-caret-down"></i> </a>
    </div>
    <div id="toc_body" class="card-body collapse p-0">
      
      <ul id="toc_ul" class="list p-0">
      </ul>
    </div>
  </aside>
  <div id="post_content">
  <h2 id="the-clustering-problem">The clustering problem</h2>
<p>Given \(N\) points, want \(k\) clusters. Often, \(k\) is not known.</p>
<h3 id="use">Use</h3>
<p>Summarizing data is important for generalization, understanding future data points.</p>
<p>Supplying labels to unlabeled data; thence classifying new data-points. Eg: Face recognition using Eigenfaces. Use to estimate distribution support and thence in novelty detection.</p>
<h3 id="criteria-continuity-vs-compactness">Criteria: Continuity vs compactness</h3>
<p>Consider a starfish: The continuity critierion will identify the 5 arms as 5 clusters, but the comactness criterion will fail. Consider points produced from two gaussians with distinct centers: the compactness criterion appears better.</p>
<h3 id="extensions">Extensions</h3>
<p>Coclustering data-points along with features.</p>
<h4 id="find-non-redundant-clusterings">Find Non-redundant clusterings</h4>
<p>Aka Disparate clusters. Want clusterings based on unrelated criteria. Eg: can classify people based on sex, race etc.. \(k\) Disparate clusters can be thought of as lying in \(k\) orthogonal subspaces.</p>
<h4 id="with-background-clutter">With background clutter</h4>
<p>Consider clustering stars in the night sky to find galaxies. Important for clustering algorithm to ignore clutter.</p>
<h3 id="evaluation-of-clustering">Evaluation of clustering</h3>
<p>In case the \(k\) true labels are known: Just use ways of evaluating classification. Can always do this by generating artificial data.</p>
<h3 id="challenges">Challenges</h3>
<h4 id="curse-of-dimensionality">Curse of dimensionality</h4>
<p>As dimensions/ features grow, more difficult to group similar things together; there could be irrelevant or noisy features.</p>
<p>Use dimentionality reduction techniques.</p>
<h4 id="number-of-clusters-k">Number of clusters k</h4>
<p>The actual number depends on the data. Choice of \(k\) is mostly based on heuristics.</p>
<p>Some methods take \(k\) as input, others discover \(k\) themselves.</p>
<h4 id="identify-important-clusters">Identify important clusters</h4>
<p>Clusters which matter remain distinct at different levels of coarsity.</p>
<h5 id="cluster-visualization">Cluster visualization</h5>
<p>Look at clusters in 2D or 3D at varying coarsity levels to identify important clusters.</p>
<h3 id="approaches">Approaches</h3>
<h4 id="views-of-the-data">Views of the data</h4>
<p>Visualize data points as points in feature space.</p>
<p>Or as &lsquo;data points (X) vs features (Y)&rsquo; matrix A. In case features are binary (eg: word present or absent), get contingency table P(X, Y), an estimate of the joint probability matrix.</p>
<h4 id="density-estimation">Density estimation</h4>
<h5 id="parametric">Parametric</h5>
<p>Try to find distribution of data in the input space. Usually fit some multimodal distribution: the different modes define different cluster representatives.</p>
<p>Eg: Fit a mixture of \(k\) Gaussians centered at various spots in the input space.</p>
<p>Advantage of having a parametric model: can extrapolate what the data will look like with more sampling.</p>
<h5 id="non-parametric">Non-Parametric</h5>
<p>Can do non-parametric density inference, then do density shaving: ignore points with low density, identify the modes.</p>
<h5 id="relative-performance">Relative performance</h5>
<p>Non parametric methods usually perform better, given enough data-points.</p>
<h4 id="centroid-based-vs-agglomerative-clustering">Centroid based vs agglomerative clustering</h4>
<p>Described fully in another section.</p>
<p>Centroid based clustering methods are usually fast: the costliest step is often assignment of points to clusters: O(kn). Agglomerative methods usually involve comparison between all cluster-pairs for the purpose of agglomeration; so are slower: \(O(n^{2})\).</p>
<p>Centroid based methods require \(k\) to be known before hand, they need initial points. Agglomerative methods find varying number of clusters: from \(N\) to 1; it is up to the user to know where to stop - this can be difficult.</p>
<h2 id="agglomerative-clustering">Agglomerative clustering</h2>
<p>Bottom up approach. Start off with \(N\) clusters: 1 for each point; pick nearest pair of clusters; merge them; repeat till you have \(k\) clusters.</p>
<h3 id="intercluster-metrics">Intercluster metrics</h3>
<p>Distance between means. Or between closest pair of points: tends to produce elongated clusters: clustering by continuity criterion. Or between farthest pair of points: clustering by compactness criterion. These correspond to the 2 clustering criteria.</p>
<h2 id="centroid-based-clustering">Centroid based clustering</h2>
<p>Use centroids or cluster representatives to cluster.</p>
<h3 id="mean-best-cluster-representative-wrt-bregman-div">Mean: Best cluster representative wrt Bregman div</h3>
<p>Given Bregman div \(d_f\) based on convex function f. Show by easy algebra that \(n^{-1}\sum_i d_\gf(X_i, z) - n^{-1}\sum_i d_\gf(X_i, \mean) = d_f(z, \mean) \geq 0\). So, mean is best cluster representative wrt 2 norm: 2-norm is also a bregman divergence.</p>
<h3 id="k-means-clustering">k means clustering</h3>
<h4 id="objective">Objective</h4>
<h5 id="as-minimizing-within-cluster-scatter">As minimizing within cluster scatter</h5>
<p>Find \(k\) centroids, make \(k\) partitions (Vorinoi tesselations) in the input space: \<br>
\(S&rsquo; = (S_{i}') = argmin_{S} \sum_{i=1}^{k} \sum_{x_{j} \in S_{i}} d(x_{j}, \mean_{i})\).</p>
<h5 id="as-maximizing-inter-cluster-scatter">As maximizing inter-cluster scatter</h5>
<p>Use scatter matrices/ scalars \<br>
\(S_B, S_W, S_T\) as in LDA. For any bregman divergence: \(S_T = S_B + S_W\). Implicitly tries to maximimze \(S_B\) : Between cluster scatter.</p>
<h4 id="algorithm">Algorithm</h4>
<p>Start of with \(k\) vectors \((m_{i}^{0})\) as means of \((S_{i})\); At time t, you have: \((m_{i}^{t})\). Reassign all points to the \(S_{i}^{t}\) corresponding to the closest \(m_{i}^{t}\); calculate new means \((m_{i}^{t+1})\) as the centers of these \((S_{i}^{t})\); repeat.</p>
<p>If \(d\) is any Bregman div, \(k\) means minimizes this at each iteration: Alg finds better clustering, Mean is best cluster representative.</p>
<p>Time: O(kndt): very fast.</p>
<h4 id="as-low-rank-factorization-with-alternating-minimization">As low rank factorization with alternating minimization</h4>
<p>Let X be the \(d \times n\) data matrix. Doing \(X \approx MW\), where M is the \(d \times k\) means matrix, and \(W \in \set{0, 1}^{k\times n}\) denotes membership. For strict partitioning, there is the constraint \(w_i \in I_k\).</p>
<p>So, k-means is equivalent to solving \(\min_{M, W} \norm{X - MW}_F\) by alternatively minimizing M with W fixed, and W with M fixed subject to constraints on W.</p>
<h4 id="drawbacks-and-extensions">Drawbacks and extensions</h4>
<p>This is a greedy algorithm, does local minimization of the objective function. Highly sensitive to initial conditions. Can end up with empty clusters, with bad initialization. So, have varied initialization strategies.</p>
<p>Fails to cluster data points arranged in concentric rings. So use the kernel trick here: get kernel \(k\) means.</p>
<h3 id="with-gmm">With GMM</h3>
<p>You model the observed data with \(k\) normal distributions \((D_i)\) specified by means \((\mean_i)\), covariances \((\covmatrix_i)\), prior probabilities of a point being generated by \((D_i)\), aka mixture weights, \((p_i)\). If you find the best parameters for this model, you can assign points to the cluster associated with the \(D_i\) most likely to have generated it.</p>
<p>Start with some guessed parameters. Repeat the following steps iteratively: a] Assign each point to the \(D_i\) most likely to have generated it: do \(\min_i (\log p_i) (x-m_i)^{T}\covmatrix_i^{-1}(x-m_i)\): same as minimizing a weighted &lsquo;Mahalonobis distance&rsquo;; \((\log p_i)\) can be seen as shrinking \(D_i\) appropriately by acting on \(\covmatrix_i^{-1}\). Let \(n_i\) count the points assigned to cluster i. Update \(p_i = n_i/n\). b] Update parameters: \(\mean_i = n_i^{-1}\sum_{x_j \in i} x_j; \covmatrix_{i, (j, k)} = \frac{1}{n_i-1}(x_i,j - \mean_j)(x_i,k - \mean_j)\): note unbiased estimater used in estimating covariances.</p>
<h4 id="generalizing-k-means">Generalizing k-means</h4>
<p>k-means corresponds to GMM clustering with each Normal Distribution in the model constrained to being spherical: at each step you assign the point to the cluster with \(\mean = \argmin_{m_i} (x-m_i)^{T}(x-m_i)\).</p>
<h3 id="with-non-negative-matrix-factorization">With non-negative matrix factorization</h3>
<p>Let X be the \(d \times n\) data matrix. Doing \(X \approx MW\), where \(M \in R_+^{d \times k}\) means matrix, and \(W \in R_+^{k\times n}\) denotes membership strength.</p>
<h3 id="finding-the-initialization-points">Finding the initialization points</h3>
<p>Bad initialization points can lead to bad clusters, good ones lead to good clusters. Density estimation useful here. \tbc</p>
<h2 id="co-clustering">Co-clustering</h2>
<p>Cluster both the rows and columns of P simultaneously. Thus dealing with duplicate/ synonymous/ irrelevant features simultaneously while clustering.</p>
<h3 id="objective-information-loss-minimizing">Objective: Information loss minimizing</h3>
<p>Find maps
$$C_{X}: \set{x_{1}, .. x_{m}} \to \set{\hat{x_{1}}, .. , \hat{x_{k}}};\<br>
C_{Y}: \set{y_{1}, .. y_{m}} \to \set{\hat{y_{1}}, .. , \hat{y_{l}}}\(. \)(C_{X}, C_{Y})\( is a coclustering; yields corresponding joint distribution matrix \)p(\hat{X}, \hat{Y})\(. Best co clustering has minimum mutual information loss: \)\min I(X;Y) - I(\hat{X},\hat{Y}) = K(p(X,Y)||q(X,Y))\( where \)q(X, Y) = p(\hat{X}, \hat{Y})p(X|\hat{X})p(Y|\hat{Y})$$.</p>
<h4 id="the-monotonic-optimizer">The monotonic optimizer</h4>
<p>$$K(p(X,Y)||q(X,Y)) = \<br>
K(p(X,Y,\hat{X},\hat{Y})||q(X,Y,\hat{X},\hat{Y}))$$.\<br>
For any clustering, q preserves marginals and conditionals: \<br>
\(q(\hat{x}, \hat{y}) = p(\hat{x}, \hat{y}), q(x, \hat{x}) = p(x, \hat{x}), q(y, \hat{y}) = p(y, \hat{y}), p(x) = q(x), p(x|\hat{x}) = q(x|\hat{x})\) etc..</p>
<p>So,
$$K(p(X,Y,\hat{X},\hat{Y})||q(X,Y,\hat{X},\hat{Y})) =\<br>
\sum_{\hat{x}} \sum_{x : C_{X}(x) = \<br>
\hat{x}} K(p(Y|x)||q(Y|\hat{x}))$$; \<br>
similar form in terms of \(K(p(X|y)||q(X|\hat{y}))\).</p>
<p>Thence information theoretic coclustering alg: Start with \(C_{X}^{(0)}, C_{Y}^{0}\); at step t, for each row \(x\),  set \(C_{X}^{(t+1)}(x)= argmin_{\hat{x}} K(p(Y|x)||q^{(t)}(Y|\hat{x}))\); recompute distributions \(q^{(t+1)}\); at step t+2 similarly recluster columns finding local minima; repeat. This minimizes objective function monotonically. Experiments on document clustering tasks show better clustering than 1D clustering.</p>
<h2 id="using-graph-clustering">Using Graph clustering</h2>
<p>For graph clustering methods, see graph theory ref.</p>

  </div>
</article>







<aside class="card border" id="section-tree-item-">
    <div class="card-title bg-light-gray border d-flex justify-content-between">
        <a href="#ZgotmplZ">Clustering </a>
        <a data-toggle="collapse" href="#section-tree-item-body-" role="button" aria-expanded="false" aria-controls="section-tree-item-body-" >‚Ä¶<i class="fas fa-caret-down" class="collapsed"></i> </a>
    </div>
    <nav id="section-tree-item-body-" class="card-body p-0 collapse">
        
        <li>draft: false</li>
        
        <li>iscjklanguage: false</li>
        
        <li>title: Clustering</li>
        
    </nav>
</aside>


      </main>
    </div>
    <footer class="bg-yellow  p-1" role="contentinfo">
  <div id="disqus_thread"></div>
  <div id="div_footer_bar" class="container-fluid d-flex justify-content-between">
    <a class="btn btn-secondary" href="https://github.com/vvasuki/notes/issues/new" >
      ‡§™‡•ç‡§∞‡§§‡§ø‡§∏‡•ç‡§™‡§®‡•ç‡§¶‡§É
    </a>
    <a class="btn btn-secondary" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" /></a>
    <div class="btn small border">
      Built on 2020 Nov 22 10:54:55 UTC. (<a href="http://google.com/search?q=10%3a54%3a55%20UTC to IST">IST</a>)
    </div>
  <ul id="footer-bar-right-custom" class="list-group list-group-horizontal">
  </ul>
  </div>
</footer>

  </body>
  <script type='text/javascript'>
    
    
    
    module_main.default.onDocumentReadyTasks();
  </script>
</html>
