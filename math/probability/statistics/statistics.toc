\contentsline {chapter}{Contents}{1}{section*.1}
\contentsline {part}{\partnumberline {I}Introduction}{13}{part.1}
\contentsline {chapter}{\chapternumberline {1}Themes}{13}{chapter.1}
\contentsline {section}{\numberline {1.1}Statistical inference}{13}{section.1.1}
\contentsline {subsection}{\numberline {1.1.1}Inference from data subject to randomness}{14}{subsection.1.1.1}
\contentsline {subsection}{\numberline {1.1.2}Modeling problems}{14}{subsection.1.1.2}
\contentsline {subsection}{\numberline {1.1.3}Solving Modeled problems}{14}{subsection.1.1.3}
\contentsline {subsubsection}{\numberline {1.1.3.1}Degrees of abstractness}{14}{subsubsection.1.1.3.1}
\contentsline {section}{\numberline {1.2}Research effort}{15}{section.1.2}
\contentsline {subsection}{\numberline {1.2.1}Analysis of efficiency and complexity}{15}{subsection.1.2.1}
\contentsline {subsection}{\numberline {1.2.2}Modeling and Experimentation}{15}{subsection.1.2.2}
\contentsline {subsubsection}{\numberline {1.2.2.1}Purpose}{15}{subsubsection.1.2.2.1}
\contentsline {subsubsection}{\numberline {1.2.2.2}Avoiding unnecessary work}{15}{subsubsection.1.2.2.2}
\contentsline {subsubsection}{\numberline {1.2.2.3}Degree of experimentation}{15}{subsubsection.1.2.2.3}
\contentsline {subsubsection}{\numberline {1.2.2.4}Data preparation}{15}{subsubsection.1.2.2.4}
\contentsline {subsection}{\numberline {1.2.3}Following research}{15}{subsection.1.2.3}
\contentsline {section}{\numberline {1.3}Software}{15}{section.1.3}
\contentsline {chapter}{\chapternumberline {2}Decision theory}{16}{chapter.2}
\contentsline {section}{\numberline {2.1}Agents: Actions, policies}{16}{section.2.1}
\contentsline {subsection}{\numberline {2.1.1}State and parameters}{16}{subsection.2.1.1}
\contentsline {subsubsection}{\numberline {2.1.1.1}State space}{16}{subsubsection.2.1.1.1}
\contentsline {subsubsection}{\numberline {2.1.1.2}Parameter space T}{16}{subsubsection.2.1.1.2}
\contentsline {subsubsection}{\numberline {2.1.1.3}State transitions}{16}{subsubsection.2.1.1.3}
\contentsline {subsection}{\numberline {2.1.2}Action space A}{16}{subsection.2.1.2}
\contentsline {subsubsection}{\numberline {2.1.2.1}Common examples}{16}{subsubsection.2.1.2.1}
\contentsline {subsection}{\numberline {2.1.3}Goodness of actions}{17}{subsection.2.1.3}
\contentsline {subsubsection}{\numberline {2.1.3.1}Loss function L}{17}{subsubsection.2.1.3.1}
\contentsline {subsubsection}{\numberline {2.1.3.2}Examples}{17}{subsubsection.2.1.3.2}
\contentsline {subsection}{\numberline {2.1.4}Decision procedure d}{17}{subsection.2.1.4}
\contentsline {subsubsection}{\numberline {2.1.4.1}Mapping observation D to actions}{17}{subsubsection.2.1.4.1}
\contentsline {subsubsection}{\numberline {2.1.4.2}Deterministic procedures}{17}{subsubsection.2.1.4.2}
\contentsline {subsubsection}{\numberline {2.1.4.3}Randomized procedures}{17}{subsubsection.2.1.4.3}
\contentsline {paragraph}{Combining decision procedures}{17}{section*.5}
\contentsline {subsubsection}{\numberline {2.1.4.4}Examples}{18}{subsubsection.2.1.4.4}
\contentsline {section}{\numberline {2.2}Risk R of decision procedure d}{18}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Motivation and setting}{18}{subsection.2.2.1}
\contentsline {subsection}{\numberline {2.2.2}Risk}{18}{subsection.2.2.2}
\contentsline {subsection}{\numberline {2.2.3}Uncertain ground truth case}{18}{subsection.2.2.3}
\contentsline {subsubsection}{\numberline {2.2.3.1}The need}{18}{subsubsection.2.2.3.1}
\contentsline {subsubsection}{\numberline {2.2.3.2}Frequentist and epistemological approaches}{18}{subsubsection.2.2.3.2}
\contentsline {subsubsection}{\numberline {2.2.3.3}Prior beliefs about ground truth}{19}{subsubsection.2.2.3.3}
\contentsline {subsubsection}{\numberline {2.2.3.4}Prior beliefs about best d}{19}{subsubsection.2.2.3.4}
\contentsline {subsubsection}{\numberline {2.2.3.5}Additive form}{19}{subsubsection.2.2.3.5}
\contentsline {paragraph}{Strict restrictions}{19}{section*.6}
\contentsline {subsection}{\numberline {2.2.4}Geometry of R}{19}{subsection.2.2.4}
\contentsline {subsection}{\numberline {2.2.5}Empirical risk}{20}{subsection.2.2.5}
\contentsline {subsection}{\numberline {2.2.6}Minimal risk}{20}{subsection.2.2.6}
\contentsline {subsubsection}{\numberline {2.2.6.1}Definition}{20}{subsubsection.2.2.6.1}
\contentsline {subsubsection}{\numberline {2.2.6.2}Risk Consistency of d}{20}{subsubsection.2.2.6.2}
\contentsline {subsubsection}{\numberline {2.2.6.3}Risk persistence}{20}{subsubsection.2.2.6.3}
\contentsline {subsubsection}{\numberline {2.2.6.4}In High dimensional setting}{20}{subsubsection.2.2.6.4}
\contentsline {section}{\numberline {2.3}As a POMDP}{20}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}State and observation}{21}{subsection.2.3.1}
\contentsline {subsection}{\numberline {2.3.2}Transitions}{21}{subsection.2.3.2}
\contentsline {subsection}{\numberline {2.3.3}Action and Loss}{21}{subsection.2.3.3}
\contentsline {subsection}{\numberline {2.3.4}Policy}{21}{subsection.2.3.4}
\contentsline {subsection}{\numberline {2.3.5}Risk vs value}{21}{subsection.2.3.5}
\contentsline {chapter}{\chapternumberline {3}Procedure for choosing decision procedure d}{21}{chapter.3}
\contentsline {section}{\numberline {3.1}Overview, motivation}{21}{section.3.1}
\contentsline {section}{\numberline {3.2}Picking the right hypothesis space H}{22}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Hypothesis space H}{22}{subsection.3.2.1}
\contentsline {subsection}{\numberline {3.2.2}Motivation, Factors to consider}{22}{subsection.3.2.2}
\contentsline {subsubsection}{\numberline {3.2.2.1}Approximation vs estimation error tradeoff}{22}{subsubsection.3.2.2.1}
\contentsline {subsection}{\numberline {3.2.3}Parameters}{22}{subsection.3.2.3}
\contentsline {subsubsection}{\numberline {3.2.3.1}Common structural assumptions}{23}{subsubsection.3.2.3.1}
\contentsline {section}{\numberline {3.3}Loss function choice}{23}{section.3.3}
\contentsline {subsection}{\numberline {3.3.1}Evaluation}{23}{subsection.3.3.1}
\contentsline {section}{\numberline {3.4}Theoretically find the best d}{23}{section.3.4}
\contentsline {subsection}{\numberline {3.4.1}Best d for fixed $\theta $}{23}{subsection.3.4.1}
\contentsline {subsection}{\numberline {3.4.2}Minimum expected (Bayesian) risk}{23}{subsection.3.4.2}
\contentsline {subsubsection}{\numberline {3.4.2.1}Geometry}{23}{subsubsection.3.4.2.1}
\contentsline {subsection}{\numberline {3.4.3}Choose an admissible procedure}{23}{subsection.3.4.3}
\contentsline {subsubsection}{\numberline {3.4.3.1}Geometry}{24}{subsubsection.3.4.3.1}
\contentsline {subsection}{\numberline {3.4.4}The adversarial setting: minimax procedure}{24}{subsection.3.4.4}
\contentsline {subsubsection}{\numberline {3.4.4.1}Geometry}{24}{subsubsection.3.4.4.1}
\contentsline {subsection}{\numberline {3.4.5}Compare risk profiles of d over range of $\ensuremath {\theta }$}{24}{subsection.3.4.5}
\contentsline {section}{\numberline {3.5}Empirical risk minimization}{24}{section.3.5}
\contentsline {subsection}{\numberline {3.5.1}Fit to D vs generalization ability}{24}{subsection.3.5.1}
\contentsline {subsubsection}{\numberline {3.5.1.1}Training error}{24}{subsubsection.3.5.1.1}
\contentsline {subsubsection}{\numberline {3.5.1.2}Overfitting}{25}{subsubsection.3.5.1.2}
\contentsline {subsubsection}{\numberline {3.5.1.3}Underfitting}{25}{subsubsection.3.5.1.3}
\contentsline {subsection}{\numberline {3.5.2}Overfitting and model complexity}{25}{subsection.3.5.2}
\contentsline {subsubsection}{\numberline {3.5.2.1}Model complexity}{25}{subsubsection.3.5.2.1}
\contentsline {subsubsection}{\numberline {3.5.2.2}Limiting number of parameters}{25}{subsubsection.3.5.2.2}
\contentsline {subsubsection}{\numberline {3.5.2.3}Polynomial regression example}{25}{subsubsection.3.5.2.3}
\contentsline {subsection}{\numberline {3.5.3}Avoiding overfitting}{25}{subsection.3.5.3}
\contentsline {subsubsection}{\numberline {3.5.3.1}Altered risk function}{25}{subsubsection.3.5.3.1}
\contentsline {subsubsection}{\numberline {3.5.3.2}Altered loss function}{26}{subsubsection.3.5.3.2}
\contentsline {subsubsection}{\numberline {3.5.3.3}Other derivations}{26}{subsubsection.3.5.3.3}
\contentsline {subsubsection}{\numberline {3.5.3.4}Hyper-parameters}{26}{subsubsection.3.5.3.4}
\contentsline {subsection}{\numberline {3.5.4}Statistical efficiency analysis}{26}{subsection.3.5.4}
\contentsline {subsubsection}{\numberline {3.5.4.1}Accuracy of empirical risk estimate}{26}{subsubsection.3.5.4.1}
\contentsline {subsubsection}{\numberline {3.5.4.2}Bound deviation from optimum h: bound empirical risk error}{26}{subsubsection.3.5.4.2}
\contentsline {subsection}{\numberline {3.5.5}Check generalization ability}{27}{subsection.3.5.5}
\contentsline {subsubsection}{\numberline {3.5.5.1}Motivation}{27}{subsubsection.3.5.5.1}
\contentsline {subsubsection}{\numberline {3.5.5.2}General procedure}{27}{subsubsection.3.5.5.2}
\contentsline {subsubsection}{\numberline {3.5.5.3}Multiple rounds for robustness}{27}{subsubsection.3.5.5.3}
\contentsline {subsubsection}{\numberline {3.5.5.4}Cross validation}{27}{subsubsection.3.5.5.4}
\contentsline {subsection}{\numberline {3.5.6}Tuning risk minimization}{28}{subsection.3.5.6}
\contentsline {subsubsection}{\numberline {3.5.6.1}Diagnosis}{28}{subsubsection.3.5.6.1}
\contentsline {subsubsection}{\numberline {3.5.6.2}Picking hyperparameters}{28}{subsubsection.3.5.6.2}
\contentsline {section}{\numberline {3.6}Combining Decision procedures}{28}{section.3.6}
\contentsline {section}{\numberline {3.7}Offline vs online learning}{28}{section.3.7}
\contentsline {section}{\numberline {3.8}Interpreting the decision procedure selected}{28}{section.3.8}
\contentsline {chapter}{\chapternumberline {4}Sample}{28}{chapter.4}
\contentsline {section}{\numberline {4.1}Properties}{29}{section.4.1}
\contentsline {subsection}{\numberline {4.1.1}Sample bias}{29}{subsection.4.1.1}
\contentsline {subsection}{\numberline {4.1.2}Completeness, accuracy of examples}{29}{subsection.4.1.2}
\contentsline {subsection}{\numberline {4.1.3}Independence of data points}{29}{subsection.4.1.3}
\contentsline {subsubsection}{\numberline {4.1.3.1}Sequential}{29}{subsubsection.4.1.3.1}
\contentsline {subsubsection}{\numberline {4.1.3.2}Adversarial}{29}{subsubsection.4.1.3.2}
\contentsline {subsubsection}{\numberline {4.1.3.3}Active choice}{29}{subsubsection.4.1.3.3}
\contentsline {subsection}{\numberline {4.1.4}Labeling of the data.}{29}{subsection.4.1.4}
\contentsline {subsection}{\numberline {4.1.5}In case of small sample}{30}{subsection.4.1.5}
\contentsline {subsubsection}{\numberline {4.1.5.1}Few high dimensional data-points}{30}{subsubsection.4.1.5.1}
\contentsline {subsubsection}{\numberline {4.1.5.2}Examples}{30}{subsubsection.4.1.5.2}
\contentsline {part}{\partnumberline {II}Simplification, exploratory analysis}{30}{part.2}
\contentsline {chapter}{\chapternumberline {5}Data exploration}{30}{chapter.5}
\contentsline {chapter}{\chapternumberline {6}Data preparation}{31}{chapter.6}
\contentsline {section}{\numberline {6.1}Motivation}{31}{section.6.1}
\contentsline {section}{\numberline {6.2}Changing the range}{31}{section.6.2}
\contentsline {section}{\numberline {6.3}Dealing with missing values}{31}{section.6.3}
\contentsline {section}{\numberline {6.4}Saling, centering, allowing bias}{31}{section.6.4}
\contentsline {subsection}{\numberline {6.4.1}Motivation}{31}{subsection.6.4.1}
\contentsline {subsection}{\numberline {6.4.2}Centering to 0}{31}{subsection.6.4.2}
\contentsline {subsection}{\numberline {6.4.3}Scaling}{32}{subsection.6.4.3}
\contentsline {subsection}{\numberline {6.4.4}Constant variable}{32}{subsection.6.4.4}
\contentsline {chapter}{\chapternumberline {7}Finding a simpler, more useful representation of the data}{32}{chapter.7}
\contentsline {section}{\numberline {7.1}Feature extraction}{32}{section.7.1}
\contentsline {subsection}{\numberline {7.1.1}Dimensionality reduction}{32}{subsection.7.1.1}
\contentsline {subsubsection}{\numberline {7.1.1.1}Importance}{32}{subsubsection.7.1.1.1}
\contentsline {subsubsection}{\numberline {7.1.1.2}Extant of dimensionality reduction}{32}{subsubsection.7.1.1.2}
\contentsline {section}{\numberline {7.2}Using Kernel function to implicitly map data to a feature space}{32}{section.7.2}
\contentsline {subsection}{\numberline {7.2.1}The Kernel trick}{33}{subsection.7.2.1}
\contentsline {subsection}{\numberline {7.2.2}Use}{33}{subsection.7.2.2}
\contentsline {section}{\numberline {7.3}Casting data into a graph}{33}{section.7.3}
\contentsline {subsection}{\numberline {7.3.1}$\ensuremath {\epsilon }$ neighborhood graph}{33}{subsection.7.3.1}
\contentsline {subsection}{\numberline {7.3.2}k nearest neighbor directed graph}{33}{subsection.7.3.2}
\contentsline {subsection}{\numberline {7.3.3}Fully connected graph}{33}{subsection.7.3.3}
\contentsline {section}{\numberline {7.4}Find similar features}{33}{section.7.4}
\contentsline {subsection}{\numberline {7.4.1}With Clustering}{33}{subsection.7.4.1}
\contentsline {subsection}{\numberline {7.4.2}Find covariance of various features}{34}{subsection.7.4.2}
\contentsline {subsubsection}{\numberline {7.4.2.1}Sample points and their mean}{34}{subsubsection.7.4.2.1}
\contentsline {subsubsection}{\numberline {7.4.2.2}Sample Covariance matrix C}{34}{subsubsection.7.4.2.2}
\contentsline {section}{\numberline {7.5}Identify a good metric}{34}{section.7.5}
\contentsline {subsection}{\numberline {7.5.1}Generalized interpoint distance}{34}{subsection.7.5.1}
\contentsline {chapter}{\chapternumberline {8}Dimensionality reduction}{34}{chapter.8}
\contentsline {section}{\numberline {8.1}General motivations}{34}{section.8.1}
\contentsline {section}{\numberline {8.2}Latent factor modeling}{34}{section.8.2}
\contentsline {subsection}{\numberline {8.2.1}Problem}{34}{subsection.8.2.1}
\contentsline {subsubsection}{\numberline {8.2.1.1}Matrix view}{34}{subsubsection.8.2.1.1}
\contentsline {subsubsection}{\numberline {8.2.1.2}Linear model}{35}{subsubsection.8.2.1.2}
\contentsline {subsubsection}{\numberline {8.2.1.3}Motivation}{35}{subsubsection.8.2.1.3}
\contentsline {subsection}{\numberline {8.2.2}Matrix factorization by SVD}{35}{subsection.8.2.2}
\contentsline {subsection}{\numberline {8.2.3}Non-negative matrix factorization}{35}{subsection.8.2.3}
\contentsline {subsection}{\numberline {8.2.4}Probabilistic modeling}{35}{subsection.8.2.4}
\contentsline {section}{\numberline {8.3}Linear dimensionality reduction}{35}{section.8.3}
\contentsline {subsection}{\numberline {8.3.1}Most variable subspace identification}{35}{subsection.8.3.1}
\contentsline {subsubsection}{\numberline {8.3.1.1}Problem}{35}{subsubsection.8.3.1.1}
\contentsline {subsubsection}{\numberline {8.3.1.2}Motivation}{36}{subsubsection.8.3.1.2}
\contentsline {subsubsection}{\numberline {8.3.1.3}Preprocessing, problem statement}{36}{subsubsection.8.3.1.3}
\contentsline {subsubsection}{\numberline {8.3.1.4}Solution}{36}{subsubsection.8.3.1.4}
\contentsline {subsubsection}{\numberline {8.3.1.5}Best target dimension k}{36}{subsubsection.8.3.1.5}
\contentsline {subsubsection}{\numberline {8.3.1.6}Comments}{36}{subsubsection.8.3.1.6}
\contentsline {subsection}{\numberline {8.3.2}Factor analysis}{36}{subsection.8.3.2}
\contentsline {section}{\numberline {8.4}Supervised linear dimensionality reduction}{36}{section.8.4}
\contentsline {subsection}{\numberline {8.4.1}Linear discriminant analysis}{37}{subsection.8.4.1}
\contentsline {subsubsection}{\numberline {8.4.1.1}The problem}{37}{subsubsection.8.4.1.1}
\contentsline {subsubsection}{\numberline {8.4.1.2}The solution}{37}{subsubsection.8.4.1.2}
\contentsline {section}{\numberline {8.5}Non-linear dimensionality reduction}{37}{section.8.5}
\contentsline {subsection}{\numberline {8.5.1}Kernel PCA}{37}{subsection.8.5.1}
\contentsline {subsection}{\numberline {8.5.2}Manifold learning}{37}{subsection.8.5.2}
\contentsline {subsection}{\numberline {8.5.3}Measuring goodness}{38}{subsection.8.5.3}
\contentsline {chapter}{\chapternumberline {9}Cluster data points}{38}{chapter.9}
\contentsline {section}{\numberline {9.1}The clustering problem}{38}{section.9.1}
\contentsline {subsection}{\numberline {9.1.1}Use}{38}{subsection.9.1.1}
\contentsline {subsection}{\numberline {9.1.2}Criteria: Continuity vs compactness}{38}{subsection.9.1.2}
\contentsline {subsection}{\numberline {9.1.3}Extensions}{38}{subsection.9.1.3}
\contentsline {subsubsection}{\numberline {9.1.3.1}Find Non-redundant clusterings}{38}{subsubsection.9.1.3.1}
\contentsline {subsubsection}{\numberline {9.1.3.2}With background clutter}{38}{subsubsection.9.1.3.2}
\contentsline {subsection}{\numberline {9.1.4}Evaluation of clustering}{38}{subsection.9.1.4}
\contentsline {subsection}{\numberline {9.1.5}Challenges}{39}{subsection.9.1.5}
\contentsline {subsubsection}{\numberline {9.1.5.1}Curse of dimensionality}{39}{subsubsection.9.1.5.1}
\contentsline {subsubsection}{\numberline {9.1.5.2}Number of clusters k}{39}{subsubsection.9.1.5.2}
\contentsline {subsubsection}{\numberline {9.1.5.3}Identify important clusters}{39}{subsubsection.9.1.5.3}
\contentsline {subsection}{\numberline {9.1.6}Approaches}{39}{subsection.9.1.6}
\contentsline {subsubsection}{\numberline {9.1.6.1}Views of the data}{39}{subsubsection.9.1.6.1}
\contentsline {subsubsection}{\numberline {9.1.6.2}Density estimation}{39}{subsubsection.9.1.6.2}
\contentsline {subsubsection}{\numberline {9.1.6.3}Centroid based vs agglomerative clustering}{40}{subsubsection.9.1.6.3}
\contentsline {section}{\numberline {9.2}Agglomerative clustering}{40}{section.9.2}
\contentsline {subsection}{\numberline {9.2.1}Intercluster metrics}{40}{subsection.9.2.1}
\contentsline {section}{\numberline {9.3}Centroid based clustering}{40}{section.9.3}
\contentsline {subsection}{\numberline {9.3.1}Mean: Best cluster representative wrt Bregman div}{40}{subsection.9.3.1}
\contentsline {subsection}{\numberline {9.3.2}k means clustering}{40}{subsection.9.3.2}
\contentsline {subsubsection}{\numberline {9.3.2.1}Objective}{40}{subsubsection.9.3.2.1}
\contentsline {subsubsection}{\numberline {9.3.2.2}Algorithm}{41}{subsubsection.9.3.2.2}
\contentsline {subsubsection}{\numberline {9.3.2.3}As low rank factorization with alternating minimization}{41}{subsubsection.9.3.2.3}
\contentsline {subsubsection}{\numberline {9.3.2.4}Drawbacks and extensions}{41}{subsubsection.9.3.2.4}
\contentsline {subsection}{\numberline {9.3.3}With GMM}{41}{subsection.9.3.3}
\contentsline {subsubsection}{\numberline {9.3.3.1}Generalizing k-means}{41}{subsubsection.9.3.3.1}
\contentsline {subsection}{\numberline {9.3.4}With non-negative matrix factorization}{42}{subsection.9.3.4}
\contentsline {subsection}{\numberline {9.3.5}Finding the initialization points}{42}{subsection.9.3.5}
\contentsline {section}{\numberline {9.4}Co-clustering}{42}{section.9.4}
\contentsline {subsection}{\numberline {9.4.1}Objective: Information loss minimizing}{42}{subsection.9.4.1}
\contentsline {subsubsection}{\numberline {9.4.1.1}The monotonic optimizer}{42}{subsubsection.9.4.1.1}
\contentsline {section}{\numberline {9.5}Using Graph clustering}{42}{section.9.5}
\contentsline {part}{\partnumberline {III}Distribution structure learning}{42}{part.3}
\contentsline {chapter}{\chapternumberline {10}Problems}{43}{chapter.10}
\contentsline {section}{\numberline {10.1}Conditional distributions and notation}{43}{section.10.1}
\contentsline {section}{\numberline {10.2}Connection to modeling marginal density}{43}{section.10.2}
\contentsline {subsection}{\numberline {10.2.1}Problem structure}{43}{subsection.10.2.1}
\contentsline {chapter}{\chapternumberline {11}Estimating parameters}{43}{chapter.11}
\contentsline {section}{\numberline {11.1}Estimate parameters using statistics}{43}{section.11.1}
\contentsline {subsection}{\numberline {11.1.1}Statistic, estimator}{43}{subsection.11.1.1}
\contentsline {subsubsection}{\numberline {11.1.1.1}Point estimation of the parameter}{43}{subsubsection.11.1.1.1}
\contentsline {subsection}{\numberline {11.1.2}Distribution of a statistic}{44}{subsection.11.1.2}
\contentsline {subsection}{\numberline {11.1.3}Summarize Central tendency}{44}{subsection.11.1.3}
\contentsline {subsubsection}{\numberline {11.1.3.1}AM, GM, HM}{44}{subsubsection.11.1.3.1}
\contentsline {subsubsection}{\numberline {11.1.3.2}Modeling accuracy}{44}{subsubsection.11.1.3.2}
\contentsline {subsubsection}{\numberline {11.1.3.3}Combining arithmetic means of subpopulations}{44}{subsubsection.11.1.3.3}
\contentsline {subsection}{\numberline {11.1.4}Other statistics and parameters}{45}{subsection.11.1.4}
\contentsline {subsubsection}{\numberline {11.1.4.1}Summarize variability or dispersion}{45}{subsubsection.11.1.4.1}
\contentsline {subsubsection}{\numberline {11.1.4.2}Order statistics}{45}{subsubsection.11.1.4.2}
\contentsline {subsubsection}{\numberline {11.1.4.3}Other statistics}{45}{subsubsection.11.1.4.3}
\contentsline {section}{\numberline {11.2}Estimator properties}{45}{section.11.2}
\contentsline {subsection}{\numberline {11.2.1}Bias}{45}{subsection.11.2.1}
\contentsline {subsection}{\numberline {11.2.2}Mean square error: Bias variance decomposition}{45}{subsection.11.2.2}
\contentsline {subsection}{\numberline {11.2.3}Relative efficiency of unbiased estimators}{45}{subsection.11.2.3}
\contentsline {subsection}{\numberline {11.2.4}Consistency of unbiased estimators}{46}{subsection.11.2.4}
\contentsline {subsection}{\numberline {11.2.5}Sufficiency of unbiased estimator}{46}{subsection.11.2.5}
\contentsline {subsubsection}{\numberline {11.2.5.1}Motivation from MLE}{46}{subsubsection.11.2.5.1}
\contentsline {subsubsection}{\numberline {11.2.5.2}To show sufficiency if distribution family known}{46}{subsubsection.11.2.5.2}
\contentsline {subsubsection}{\numberline {11.2.5.3}To find sufficient statistic}{46}{subsubsection.11.2.5.3}
\contentsline {subsection}{\numberline {11.2.6}Statistical efficiency}{46}{subsection.11.2.6}
\contentsline {section}{\numberline {11.3}Find estimator for some parameter}{46}{section.11.3}
\contentsline {subsection}{\numberline {11.3.1}From sufficient statistic}{46}{subsection.11.3.1}
\contentsline {subsection}{\numberline {11.3.2}Minimum variance unbiased estimator (MVUE)}{47}{subsection.11.3.2}
\contentsline {subsection}{\numberline {11.3.3}Method of moments}{47}{subsection.11.3.3}
\contentsline {section}{\numberline {11.4}Confidence Interval}{47}{section.11.4}
\contentsline {subsection}{\numberline {11.4.1}Definition}{47}{subsection.11.4.1}
\contentsline {subsection}{\numberline {11.4.2}General procedure}{47}{subsection.11.4.2}
\contentsline {subsubsection}{\numberline {11.4.2.1}Pivotal quantity for estimate}{47}{subsubsection.11.4.2.1}
\contentsline {subsubsection}{\numberline {11.4.2.2}Procedure}{47}{subsubsection.11.4.2.2}
\contentsline {subsection}{\numberline {11.4.3}Pivotal quantity deviation bounds}{47}{subsection.11.4.3}
\contentsline {subsubsection}{\numberline {11.4.3.1}By repeated sampling}{48}{subsubsection.11.4.3.1}
\contentsline {subsubsection}{\numberline {11.4.3.2}By Bootstrap sampling}{48}{subsubsection.11.4.3.2}
\contentsline {paragraph}{Process}{48}{section*.16}
\contentsline {paragraph}{Properties}{48}{section*.17}
\contentsline {subsection}{\numberline {11.4.4}Pivotal quantity for ratio of variances}{48}{subsection.11.4.4}
\contentsline {chapter}{\chapternumberline {12}Mean, variance of real valued RV}{48}{chapter.12}
\contentsline {section}{\numberline {12.1}Mean: estimation}{48}{section.12.1}
\contentsline {subsection}{\numberline {12.1.1}Consistency}{48}{subsection.12.1.1}
\contentsline {subsection}{\numberline {12.1.2}Normalness of estimator distribution}{48}{subsection.12.1.2}
\contentsline {subsubsection}{\numberline {12.1.2.1}Proof showing MGF $M_{U_{n}}(t) \to $ MGF of N(0, 1)}{48}{subsubsection.12.1.2.1}
\contentsline {subsection}{\numberline {12.1.3}Normal distr: Pivotal quantity to estimate mean}{49}{subsection.12.1.3}
\contentsline {subsection}{\numberline {12.1.4}Goodness of empirical estimate}{49}{subsection.12.1.4}
\contentsline {section}{\numberline {12.2}Variance estimation}{49}{section.12.2}
\contentsline {subsection}{\numberline {12.2.1}The biased and unbiased estimators}{49}{subsection.12.2.1}
\contentsline {subsection}{\numberline {12.2.2}Normal distr: Pivotal quantity to estimate variance}{49}{subsection.12.2.2}
\contentsline {section}{\numberline {12.3}Sequential data Sample statistics}{49}{section.12.3}
\contentsline {subsection}{\numberline {12.3.1}k-step Moving averages}{49}{subsection.12.3.1}
\contentsline {subsubsection}{\numberline {12.3.1.1}Simple moving average}{49}{subsubsection.12.3.1.1}
\contentsline {subsubsection}{\numberline {12.3.1.2}Exponential Weighed}{49}{subsubsection.12.3.1.2}
\contentsline {subsubsection}{\numberline {12.3.1.3}Applications}{50}{subsubsection.12.3.1.3}
\contentsline {chapter}{\chapternumberline {13}Density estimation}{50}{chapter.13}
\contentsline {section}{\numberline {13.1}Importance}{50}{section.13.1}
\contentsline {section}{\numberline {13.2}Choosing the distribution family}{50}{section.13.2}
\contentsline {subsection}{\numberline {13.2.1}Observe empirical distribution}{50}{subsection.13.2.1}
\contentsline {subsection}{\numberline {13.2.2}Given expected values of fns $\ensuremath {\left \{ E[\ensuremath {\phi }_{i}(X)] = \ensuremath {\mu }_{i} \right \}}$ and a base measure h}{50}{subsection.13.2.2}
\contentsline {subsection}{\numberline {13.2.3}Given dependence among features}{50}{subsection.13.2.3}
\contentsline {section}{\numberline {13.3}Parametric density estimation}{50}{section.13.3}
\contentsline {section}{\numberline {13.4}Non parametric Probability Density estimation}{50}{section.13.4}
\contentsline {subsection}{\numberline {13.4.1}Histogram and the Kernel histogram}{50}{subsection.13.4.1}
\contentsline {subsection}{\numberline {13.4.2}Kernel density estimation}{51}{subsection.13.4.2}
\contentsline {subsubsection}{\numberline {13.4.2.1}Kernel function for density estimation}{51}{subsubsection.13.4.2.1}
\contentsline {subsubsection}{\numberline {13.4.2.2}Using Gaussian radial basis functions}{51}{subsubsection.13.4.2.2}
\contentsline {section}{\numberline {13.5}Estimate probability measures}{51}{section.13.5}
\contentsline {subsection}{\numberline {13.5.1}Use empirical measures}{51}{subsection.13.5.1}
\contentsline {subsubsection}{\numberline {13.5.1.1}Empirical measure}{51}{subsubsection.13.5.1.1}
\contentsline {subsubsection}{\numberline {13.5.1.2}Goodness of estimate: single event}{51}{subsubsection.13.5.1.2}
\contentsline {subsubsection}{\numberline {13.5.1.3}Goodness of estimate for a class of events}{51}{subsubsection.13.5.1.3}
\contentsline {subsection}{\numberline {13.5.2}Estimate CDF using empirical CDF}{52}{subsection.13.5.2}
\contentsline {chapter}{\chapternumberline {14}Parametric density estimation}{52}{chapter.14}
\contentsline {section}{\numberline {14.1}Problem and solution ideals}{52}{section.14.1}
\contentsline {subsection}{\numberline {14.1.1}Density estimation using a distribution class}{52}{subsection.14.1.1}
\contentsline {subsubsection}{\numberline {14.1.1.1}Related problems}{53}{subsubsection.14.1.1.1}
\contentsline {subsection}{\numberline {14.1.2}Solution ideas}{53}{subsection.14.1.2}
\contentsline {section}{\numberline {14.2}Approximation with Normal distribution}{53}{section.14.2}
\contentsline {subsection}{\numberline {14.2.1}Algorithm}{53}{subsection.14.2.1}
\contentsline {subsubsection}{\numberline {14.2.1.1}2nd order approximation of log f}{53}{subsubsection.14.2.1.1}
\contentsline {subsection}{\numberline {14.2.2}Properties}{53}{subsection.14.2.2}
\contentsline {subsubsection}{\numberline {14.2.2.1}Estimating Z}{53}{subsubsection.14.2.2.1}
\contentsline {subsubsection}{\numberline {14.2.2.2}Non-uniqueness}{53}{subsubsection.14.2.2.2}
\contentsline {section}{\numberline {14.3}Log loss minimization}{54}{section.14.3}
\contentsline {subsection}{\numberline {14.3.1}Optimization problem, estimate}{54}{subsection.14.3.1}
\contentsline {subsubsection}{\numberline {14.3.1.1}Functional Invariance property}{54}{subsubsection.14.3.1.1}
\contentsline {subsubsection}{\numberline {14.3.1.2}Avg Log likelihood function}{54}{subsubsection.14.3.1.2}
\contentsline {subsection}{\numberline {14.3.2}Other perspectives}{54}{subsection.14.3.2}
\contentsline {subsubsection}{\numberline {14.3.2.1}As log loss risk minimization}{54}{subsubsection.14.3.2.1}
\contentsline {subsubsection}{\numberline {14.3.2.2}Priors as regularizers}{54}{subsubsection.14.3.2.2}
\contentsline {subsubsection}{\numberline {14.3.2.3}As empirical code-length divergence minimization}{54}{subsubsection.14.3.2.3}
\contentsline {subsection}{\numberline {14.3.3}Derivatives of log likelihood}{55}{subsection.14.3.3}
\contentsline {subsubsection}{\numberline {14.3.3.1}Score function : Sensitivity of log Likelihood}{55}{subsubsection.14.3.3.1}
\contentsline {subsubsection}{\numberline {14.3.3.2}Variance wrt X of sensitivity score of likelihood}{55}{subsubsection.14.3.3.2}
\contentsline {subsection}{\numberline {14.3.4}Computational cost}{55}{subsection.14.3.4}
\contentsline {subsubsection}{\numberline {14.3.4.1}Computing partition function}{55}{subsubsection.14.3.4.1}
\contentsline {subsubsection}{\numberline {14.3.4.2}Pseudolikelihood maximization}{55}{subsubsection.14.3.4.2}
\contentsline {section}{\numberline {14.4}Non uniform model for P(t)}{56}{section.14.4}
\contentsline {subsection}{\numberline {14.4.1}Objective, estimate}{56}{subsection.14.4.1}
\contentsline {subsection}{\numberline {14.4.2}Relation to MLE}{56}{subsection.14.4.2}
\contentsline {subsection}{\numberline {14.4.3}Defining prior distributions}{56}{subsection.14.4.3}
\contentsline {subsubsection}{\numberline {14.4.3.1}Hyperparameters for prior distribution of parameters}{56}{subsubsection.14.4.3.1}
\contentsline {subsubsection}{\numberline {14.4.3.2}Conjugate prior for a likelihood}{56}{subsubsection.14.4.3.2}
\contentsline {section}{\numberline {14.5}Model combination}{56}{section.14.5}
\contentsline {section}{\numberline {14.6}Information criteria}{57}{section.14.6}
\contentsline {subsection}{\numberline {14.6.1}Use}{57}{subsection.14.6.1}
\contentsline {chapter}{\chapternumberline {15}Support estimation}{57}{chapter.15}
\contentsline {section}{\numberline {15.1}Estimate support of a distribution D}{57}{section.15.1}
\contentsline {subsection}{\numberline {15.1.1}With soft margin kernel hyperplane}{57}{subsection.15.1.1}
\contentsline {subsubsection}{\numberline {15.1.1.1}Choosing kernel, tuning parameters}{57}{subsubsection.15.1.1.1}
\contentsline {subsubsection}{\numberline {15.1.1.2}Comparison with thresholded Kernel Density estimator}{58}{subsubsection.15.1.1.2}
\contentsline {subsubsection}{\numberline {15.1.1.3}Comparison with using soft margin hyperspheres}{58}{subsubsection.15.1.1.3}
\contentsline {subsubsection}{\numberline {15.1.1.4}Connection to binary classification}{58}{subsubsection.15.1.1.4}
\contentsline {subsection}{\numberline {15.1.2}Using soft margin hyperspheres}{58}{subsection.15.1.2}
\contentsline {subsection}{\numberline {15.1.3}Using Clustering}{58}{subsection.15.1.3}
\contentsline {section}{\numberline {15.2}Novelty detection}{58}{section.15.2}
\contentsline {subsection}{\numberline {15.2.1}Problem}{58}{subsection.15.2.1}
\contentsline {subsubsection}{\numberline {15.2.1.1}As One class classification}{59}{subsubsection.15.2.1.1}
\contentsline {subsubsection}{\numberline {15.2.1.2}Motivation}{59}{subsubsection.15.2.1.2}
\contentsline {subsection}{\numberline {15.2.2}Using density estimation}{59}{subsection.15.2.2}
\contentsline {subsection}{\numberline {15.2.3}Using support of the distribution}{59}{subsection.15.2.3}
\contentsline {subsubsection}{\numberline {15.2.3.1}Ransack}{59}{subsubsection.15.2.3.1}
\contentsline {subsection}{\numberline {15.2.4}Boundary methods}{59}{subsection.15.2.4}
\contentsline {subsubsection}{\numberline {15.2.4.1}K nearest neighbors}{59}{subsubsection.15.2.4.1}
\contentsline {subsubsection}{\numberline {15.2.4.2}Support vector data description}{59}{subsubsection.15.2.4.2}
\contentsline {subsubsection}{\numberline {15.2.4.3}PCA}{59}{subsubsection.15.2.4.3}
\contentsline {chapter}{\chapternumberline {16}Conditional independence structure: discrete case}{59}{chapter.16}
\contentsline {section}{\numberline {16.1}Problems}{59}{section.16.1}
\contentsline {subsection}{\numberline {16.1.1}Model Estimation}{60}{subsection.16.1.1}
\contentsline {subsection}{\numberline {16.1.2}Edge recovery}{60}{subsection.16.1.2}
\contentsline {subsubsection}{\numberline {16.1.2.1}Ising models: Signed edge recovery}{60}{subsubsection.16.1.2.1}
\contentsline {subsection}{\numberline {16.1.3}High dimensional case}{60}{subsection.16.1.3}
\contentsline {subsubsection}{\numberline {16.1.3.1}Measuring performance}{60}{subsubsection.16.1.3.1}
\contentsline {section}{\numberline {16.2}Approaches}{60}{section.16.2}
\contentsline {subsection}{\numberline {16.2.1}Learn closest tree}{60}{subsection.16.2.1}
\contentsline {subsubsection}{\numberline {16.2.1.1}Aim}{60}{subsubsection.16.2.1.1}
\contentsline {subsubsection}{\numberline {16.2.1.2}Algorithm}{60}{subsubsection.16.2.1.2}
\contentsline {section}{\numberline {16.3}Learn neighborhoods}{61}{section.16.3}
\contentsline {subsection}{\numberline {16.3.1}For ising models}{61}{subsection.16.3.1}
\contentsline {subsubsection}{\numberline {16.3.1.1}Results}{61}{subsubsection.16.3.1.1}
\contentsline {subsubsection}{\numberline {16.3.1.2}Caveats}{61}{subsubsection.16.3.1.2}
\contentsline {subsubsection}{\numberline {16.3.1.3}Analysis technique}{61}{subsubsection.16.3.1.3}
\contentsline {subsection}{\numberline {16.3.2}For discrete graphical models}{61}{subsection.16.3.2}
\contentsline {chapter}{\chapternumberline {17}Hypothesis testing}{62}{chapter.17}
\contentsline {section}{\numberline {17.1}Model selection given 2 models}{62}{section.17.1}
\contentsline {section}{\numberline {17.2}Hypotheses}{62}{section.17.2}
\contentsline {subsection}{\numberline {17.2.1}Null hypothesis}{62}{subsection.17.2.1}
\contentsline {subsection}{\numberline {17.2.2}Alternate hypothesis}{62}{subsection.17.2.2}
\contentsline {subsection}{\numberline {17.2.3}The decision}{62}{subsection.17.2.3}
\contentsline {section}{\numberline {17.3}Experiment/ Test}{62}{section.17.3}
\contentsline {subsection}{\numberline {17.3.1}Errors}{62}{subsection.17.3.1}
\contentsline {subsubsection}{\numberline {17.3.1.1}Type 1}{62}{subsubsection.17.3.1.1}
\contentsline {subsubsection}{\numberline {17.3.1.2}Type 2}{62}{subsubsection.17.3.1.2}
\contentsline {subsection}{\numberline {17.3.2}Tradeoff}{63}{subsection.17.3.2}
\contentsline {subsection}{\numberline {17.3.3}p-value of the statistic}{63}{subsection.17.3.3}
\contentsline {subsection}{\numberline {17.3.4}Power of a test}{63}{subsection.17.3.4}
\contentsline {section}{\numberline {17.4}Test design}{63}{section.17.4}
\contentsline {subsection}{\numberline {17.4.1}Best test for given $\ensuremath {\alpha }$}{63}{subsection.17.4.1}
\contentsline {subsection}{\numberline {17.4.2}Difference in differences}{63}{subsection.17.4.2}
\contentsline {part}{\partnumberline {IV}Label prediction/ identification}{63}{part.4}
\contentsline {chapter}{\chapternumberline {18}Problems}{64}{chapter.18}
\contentsline {section}{\numberline {18.1}Core problem}{64}{section.18.1}
\contentsline {subsection}{\numberline {18.1.1}Input and response variables}{64}{subsection.18.1.1}
\contentsline {subsection}{\numberline {18.1.2}Range of X and L}{64}{subsection.18.1.2}
\contentsline {subsection}{\numberline {18.1.3}Labeling rule sought}{64}{subsection.18.1.3}
\contentsline {section}{\numberline {18.2}Action space}{64}{section.18.2}
\contentsline {section}{\numberline {18.3}Actual phenomenon}{64}{section.18.3}
\contentsline {subsection}{\numberline {18.3.1}Randomized function}{64}{subsection.18.3.1}
\contentsline {subsection}{\numberline {18.3.2}Volatility in form}{64}{subsection.18.3.2}
\contentsline {subsection}{\numberline {18.3.3}Deterministic Labeling function}{65}{subsection.18.3.3}
\contentsline {subsubsection}{\numberline {18.3.3.1}Features}{65}{subsubsection.18.3.3.1}
\contentsline {subsection}{\numberline {18.3.4}General noise model}{65}{subsection.18.3.4}
\contentsline {subsubsection}{\numberline {18.3.4.1}Using a randomized noise function}{65}{subsubsection.18.3.4.1}
\contentsline {subsubsection}{\numberline {18.3.4.2}Using a Noise variable}{65}{subsubsection.18.3.4.2}
\contentsline {subsection}{\numberline {18.3.5}Noise in case of vector labels}{65}{subsection.18.3.5}
\contentsline {subsubsection}{\numberline {18.3.5.1}Additive noise}{65}{subsubsection.18.3.5.1}
\contentsline {subsubsection}{\numberline {18.3.5.2}Multiplicative noise}{65}{subsubsection.18.3.5.2}
\contentsline {section}{\numberline {18.4}Example/ training points}{66}{section.18.4}
\contentsline {subsection}{\numberline {18.4.1}Labeled}{66}{subsection.18.4.1}
\contentsline {subsection}{\numberline {18.4.2}Unlabeled}{66}{subsection.18.4.2}
\contentsline {subsection}{\numberline {18.4.3}Alternative labels}{66}{subsection.18.4.3}
\contentsline {section}{\numberline {18.5}Distribution on test points}{66}{section.18.5}
\contentsline {subsection}{\numberline {18.5.1}Transduction vs induction}{66}{subsection.18.5.1}
\contentsline {chapter}{\chapternumberline {19}Risk and evaluation}{67}{chapter.19}
\contentsline {section}{\numberline {19.1}Loss functions: labeling single data points}{67}{section.19.1}
\contentsline {subsection}{\numberline {19.1.1}Loss functions: vector labels}{67}{subsection.19.1.1}
\contentsline {subsection}{\numberline {19.1.2}Loss functions for classification}{67}{subsection.19.1.2}
\contentsline {subsection}{\numberline {19.1.3}0/1 loss}{67}{subsection.19.1.3}
\contentsline {subsubsection}{\numberline {19.1.3.1}Minimal risk: Binary classification}{67}{subsubsection.19.1.3.1}
\contentsline {subsubsection}{\numberline {19.1.3.2}Connection to log loss risk: binary classification}{68}{subsubsection.19.1.3.2}
\contentsline {subsection}{\numberline {19.1.4}Log loss}{68}{subsection.19.1.4}
\contentsline {section}{\numberline {19.2}Loss functions: labeling multiple data points}{68}{section.19.2}
\contentsline {subsection}{\numberline {19.2.1}Confusion matrix}{68}{subsection.19.2.1}
\contentsline {subsection}{\numberline {19.2.2}True and false positives}{68}{subsection.19.2.2}
\contentsline {subsection}{\numberline {19.2.3}Precision, recall, specificity}{68}{subsection.19.2.3}
\contentsline {subsubsection}{\numberline {19.2.3.1}Emphasis on one '+ve' class}{68}{subsubsection.19.2.3.1}
\contentsline {subsubsection}{\numberline {19.2.3.2}Sensitivity - specificity tradeoff}{69}{subsubsection.19.2.3.2}
\contentsline {subsubsection}{\numberline {19.2.3.3}Sensitivity vs 1-specificity curve}{69}{subsubsection.19.2.3.3}
\contentsline {subsubsection}{\numberline {19.2.3.4}Precision/ recall tradeoff}{69}{subsubsection.19.2.3.4}
\contentsline {chapter}{\chapternumberline {20}General Solution properties}{70}{chapter.20}
\contentsline {section}{\numberline {20.1}Empirical risk minimization vs expert systems}{70}{section.20.1}
\contentsline {section}{\numberline {20.2}Hypothesis classes}{70}{section.20.2}
\contentsline {subsection}{\numberline {20.2.1}Probabilistic models}{70}{subsection.20.2.1}
\contentsline {subsection}{\numberline {20.2.2}Mean or Mode models}{70}{subsection.20.2.2}
\contentsline {subsubsection}{\numberline {20.2.2.1}Comparison to probabilistic models}{70}{subsubsection.20.2.2.1}
\contentsline {subsection}{\numberline {20.2.3}Probabilistic models: comparison}{71}{subsection.20.2.3}
\contentsline {subsubsection}{\numberline {20.2.3.1}Discriminative model corresponding to generative model}{71}{subsubsection.20.2.3.1}
\contentsline {subsubsection}{\numberline {20.2.3.2}Ease in using unlabeled points}{71}{subsubsection.20.2.3.2}
\contentsline {section}{\numberline {20.3}Discrete deterministic labeling rules}{71}{section.20.3}
\contentsline {subsection}{\numberline {20.3.1}Decision surfaces}{71}{subsection.20.3.1}
\contentsline {subsection}{\numberline {20.3.2}k-ary classifier from binary classifier}{71}{subsection.20.3.2}
\contentsline {subsection}{\numberline {20.3.3}Curse of dimensionality}{72}{subsection.20.3.3}
\contentsline {chapter}{\chapternumberline {21}With additional unlabeled data-points}{72}{chapter.21}
\contentsline {section}{\numberline {21.1}Generative approaches}{72}{section.21.1}
\contentsline {section}{\numberline {21.2}Label propagation on graphs}{72}{section.21.2}
\contentsline {subsection}{\numberline {21.2.1}Quadratic criterion}{72}{subsection.21.2.1}
\contentsline {subsubsection}{\numberline {21.2.1.1}Binary labels}{72}{subsubsection.21.2.1.1}
\contentsline {subsubsection}{\numberline {21.2.1.2}Discrete labels}{72}{subsubsection.21.2.1.2}
\contentsline {subsection}{\numberline {21.2.2}Rewriting using Graph Laplacians}{73}{subsection.21.2.2}
\contentsline {subsection}{\numberline {21.2.3}Real relaxation: solve linear system of eqns}{73}{subsection.21.2.3}
\contentsline {subsection}{\numberline {21.2.4}Low rank approximation for fast solution}{73}{subsection.21.2.4}
\contentsline {chapter}{\chapternumberline {22}Using labels from other viewpoints}{73}{chapter.22}
\contentsline {section}{\numberline {22.1}Data point Neighborhood approach}{73}{section.22.1}
\contentsline {section}{\numberline {22.2}Collaborative filtering}{73}{section.22.2}
\contentsline {subsection}{\numberline {22.2.1}Latent factor approach}{73}{subsection.22.2.1}
\contentsline {subsubsection}{\numberline {22.2.1.1}Low rank factorization}{74}{subsubsection.22.2.1.1}
\contentsline {subsection}{\numberline {22.2.2}Association rule mining}{74}{subsection.22.2.2}
\contentsline {chapter}{\chapternumberline {23}Vector labels prediction: Regression}{74}{chapter.23}
\contentsline {section}{\numberline {23.1}General problem}{74}{section.23.1}
\contentsline {section}{\numberline {23.2}Linear regression}{74}{section.23.2}
\contentsline {subsection}{\numberline {23.2.1}The problem}{74}{subsection.23.2.1}
\contentsline {subsection}{\numberline {23.2.2}The solution}{74}{subsection.23.2.2}
\contentsline {subsubsection}{\numberline {23.2.2.1}Quadratic loss function}{74}{subsubsection.23.2.2.1}
\contentsline {subsection}{\numberline {23.2.3}Maximum likelihood estimate with Gaussian noise}{75}{subsection.23.2.3}
\contentsline {subsection}{\numberline {23.2.4}Imposing prior distributions on w}{75}{subsection.23.2.4}
\contentsline {subsubsection}{\numberline {23.2.4.1}Quadratic regularizer}{75}{subsubsection.23.2.4.1}
\contentsline {subsubsection}{\numberline {23.2.4.2}Priors which prefer sparse w}{75}{subsubsection.23.2.4.2}
\contentsline {subsection}{\numberline {23.2.5}Solution}{75}{subsection.23.2.5}
\contentsline {chapter}{\chapternumberline {24}Prediction with fully labeled data}{75}{chapter.24}
\contentsline {section}{\numberline {24.1}Binary classification}{75}{section.24.1}
\contentsline {section}{\numberline {24.2}Non parametric methods}{75}{section.24.2}
\contentsline {subsection}{\numberline {24.2.1}k nearest neighbors}{75}{subsection.24.2.1}
\contentsline {section}{\numberline {24.3}Linear models for discrete classification}{76}{section.24.3}
\contentsline {subsection}{\numberline {24.3.1}Arbitrary separator from fully separable training set}{76}{subsection.24.3.1}
\contentsline {subsection}{\numberline {24.3.2}Winnow: multiplicative update}{76}{subsection.24.3.2}
\contentsline {subsection}{\numberline {24.3.3}Perceptron learning alg for halfspaces}{76}{subsection.24.3.3}
\contentsline {subsubsection}{\numberline {24.3.3.1}The problem}{76}{subsubsection.24.3.3.1}
\contentsline {subsubsection}{\numberline {24.3.3.2}The algorithm}{76}{subsubsection.24.3.3.2}
\contentsline {subsubsection}{\numberline {24.3.3.3}Convergence to u}{77}{subsubsection.24.3.3.3}
\contentsline {subsubsection}{\numberline {24.3.3.4}Comparison}{77}{subsubsection.24.3.3.4}
\contentsline {section}{\numberline {24.4}Maximum margin classifier}{77}{section.24.4}
\contentsline {subsection}{\numberline {24.4.1}The problem}{77}{subsection.24.4.1}
\contentsline {subsection}{\numberline {24.4.2}Hard margin}{78}{subsection.24.4.2}
\contentsline {subsubsection}{\numberline {24.4.2.1}Primal}{78}{subsubsection.24.4.2.1}
\contentsline {subsubsection}{\numberline {24.4.2.2}Dual}{78}{subsubsection.24.4.2.2}
\contentsline {subsection}{\numberline {24.4.3}Soft margins}{78}{subsection.24.4.3}
\contentsline {subsubsection}{\numberline {24.4.3.1}Primal}{78}{subsubsection.24.4.3.1}
\contentsline {subsubsection}{\numberline {24.4.3.2}Dual}{78}{subsubsection.24.4.3.2}
\contentsline {chapter}{\chapternumberline {25}Sparse signal detection}{79}{chapter.25}
\contentsline {section}{\numberline {25.1}Problem}{79}{section.25.1}
\contentsline {subsection}{\numberline {25.1.1}Generating process}{79}{subsection.25.1.1}
\contentsline {subsection}{\numberline {25.1.2}Decision rule sought}{79}{subsection.25.1.2}
\contentsline {subsection}{\numberline {25.1.3}As a classification problem}{79}{subsection.25.1.3}
\contentsline {subsection}{\numberline {25.1.4}Peculiarities}{79}{subsection.25.1.4}
\contentsline {section}{\numberline {25.2}Risk}{79}{section.25.2}
\contentsline {section}{\numberline {25.3}Hypothesis classes}{80}{section.25.3}
\contentsline {subsection}{\numberline {25.3.1}Desired qualities}{80}{subsection.25.3.1}
\contentsline {subsubsection}{\numberline {25.3.1.1}Sparsity}{80}{subsubsection.25.3.1.1}
\contentsline {subsubsection}{\numberline {25.3.1.2}Adaptability to different sparsity levels}{80}{subsubsection.25.3.1.2}
\contentsline {subsubsection}{\numberline {25.3.1.3}Robustness to large signals}{80}{subsubsection.25.3.1.3}
\contentsline {subsection}{\numberline {25.3.2}Probabilistic models}{80}{subsection.25.3.2}
\contentsline {subsubsection}{\numberline {25.3.2.1}Scale mixture models}{80}{subsubsection.25.3.2.1}
\contentsline {chapter}{\chapternumberline {26}Sequential data points}{80}{chapter.26}
\contentsline {section}{\numberline {26.1}Trajectory prediction}{80}{section.26.1}
\contentsline {subsection}{\numberline {26.1.1}Problem}{80}{subsection.26.1.1}
\contentsline {subsection}{\numberline {26.1.2}Simplifications}{80}{subsection.26.1.2}
\contentsline {subsection}{\numberline {26.1.3}Applications}{81}{subsection.26.1.3}
\contentsline {part}{\partnumberline {V}Applications}{81}{part.5}
\contentsline {chapter}{\chapternumberline {27}Search results}{81}{chapter.27}
\contentsline {section}{\numberline {27.1}Ranking search query results}{81}{section.27.1}
\contentsline {subsection}{\numberline {27.1.1}Feature extraction}{81}{subsection.27.1.1}
\contentsline {subsection}{\numberline {27.1.2}Objectives to optimize}{81}{subsection.27.1.2}
\contentsline {subsection}{\numberline {27.1.3}Various Loss functions L}{81}{subsection.27.1.3}
\contentsline {subsubsection}{\numberline {27.1.3.1}Pointwise}{81}{subsubsection.27.1.3.1}
\contentsline {subsubsection}{\numberline {27.1.3.2}Pairwise}{81}{subsubsection.27.1.3.2}
\contentsline {subsubsection}{\numberline {27.1.3.3}Listwise}{82}{subsubsection.27.1.3.3}
\contentsline {chapter}{\chapternumberline {28}Document clustering}{82}{chapter.28}
\contentsline {section}{\numberline {28.1}Document classification and clustering}{82}{section.28.1}
\contentsline {subsection}{\numberline {28.1.1}Feature extraction}{82}{subsection.28.1.1}
\contentsline {subsubsection}{\numberline {28.1.1.1}Bag of words assumption}{82}{subsubsection.28.1.1.1}
\contentsline {subsection}{\numberline {28.1.2}Dimensionality reduction}{82}{subsection.28.1.2}
\contentsline {subsubsection}{\numberline {28.1.2.1}Approaches}{83}{subsubsection.28.1.2.1}
\contentsline {subsection}{\numberline {28.1.3}Model class distribution using word counts}{83}{subsection.28.1.3}
\contentsline {subsection}{\numberline {28.1.4}Classification}{83}{subsection.28.1.4}
\contentsline {subsection}{\numberline {28.1.5}Clustering}{83}{subsection.28.1.5}
\contentsline {chapter}{\chapternumberline {29}Web portal related}{83}{chapter.29}
\contentsline {section}{\numberline {29.1}Pick content}{83}{section.29.1}
\contentsline {section}{\numberline {29.2}Pick content-layout}{83}{section.29.2}
\contentsline {section}{\numberline {29.3}Maximize ad revenue}{83}{section.29.3}
\contentsline {subsection}{\numberline {29.3.1}Match advertisements with content}{83}{subsection.29.3.1}
\contentsline {section}{\numberline {29.4}Data}{83}{section.29.4}
\contentsline {subsection}{\numberline {29.4.1}User responses}{84}{subsection.29.4.1}
\contentsline {section}{\numberline {29.5}Online experimentation}{84}{section.29.5}
\contentsline {chapter}{\chapternumberline {30}Spoken dialog system (domain-specific)}{84}{chapter.30}
\contentsline {section}{\numberline {30.1}Problem}{84}{section.30.1}
\contentsline {subsection}{\numberline {30.1.1}Examples}{84}{subsection.30.1.1}
\contentsline {subsection}{\numberline {30.1.2}Domain ontology}{84}{subsection.30.1.2}
\contentsline {section}{\numberline {30.2}Interaction graph}{84}{section.30.2}
\contentsline {chapter}{\chapternumberline {31}Others}{85}{chapter.31}
\contentsline {section}{\numberline {31.1}Predicting movie ratings}{85}{section.31.1}
