\documentclass{article}
\usepackage{caption}
\input{../../../../packages}
\input{../../../../macros}

%opening
\title{Discrete Graphical Model Structure Learning}
\author{Vishvas Vasuki}
% \date{}

\begin{document}
\maketitle
% \tableofcontents

\section{Abstract}
\begin{itemize}
 \item This report examines the discrete graphical model structure learning algorithm specified in a paper by Ravikumar etal.
 \item Purpose is to check if the algorithm works, and gain intuition into the conditions under which it works. This report details early experiments in this direction.
 \item Future work will involve theoretical analysis and more detailed experiments to determine conditions under which the algorithm is guaranteed to recover the graph structure.
\end{itemize}


\section{Introduction}
\begin{itemize}
 \item Outline.
\end{itemize}

\subsection{Undirected graphical models and structure learning}
\begin{itemize}
 \item Introduction to probabilistic graphical models. The problem of learning undirected graphs associated with graphical models.
\end{itemize}

\subsubsection{Pairwise graphical models.}
\begin{itemize}
 \item Definition.
\end{itemize}

\subsubsection{The structure learning problem.}
\begin{itemize}
 \item Motivate the problem using a small example.
\end{itemize}

\subsubsection{Discrete graphical models.}
\begin{itemize}
 \item Definition and probability distribution.
 \item We consider only pairwise discrete graphical models: doing so does not result in any loss of generality. 
 \item Remark about needing many parameters per edge.
 \item Specify the conditional probability distribution.
\end{itemize}

\subsection{Ising models}
\begin{itemize}
 \item Definition and probability distribution.
 \item Remark about needing only one parameter per edge.
 \item Specify the conditional probability distribution.
\end{itemize}

\section{Structure learning algorithm}
\begin{itemize}
 \item Overview.
\end{itemize}

\subsection{Neighborhood learning algorithm}
\begin{itemize}
 \item Describe algorithm structure: you take one node at a time, and learn its neighborhood.
\end{itemize}

\subsubsection{For general discrete models}
\begin{itemize}
 \item l1/l2 regularized parameter estimation.
 \item Formulation as a constrained optimization problem. 
 \item Meaning of 'regularization parameter'.
\end{itemize}

\subsubsection{For Ising models}
\begin{itemize}
 \item l1 regularized parameter estimation.
\end{itemize}

\subsubsection{Resolving inconsistencies}
\begin{itemize}
 \item From the sparsity pattern of the parameters $\gt_{i, :, :, :}$ learned using logistic regression, the algorithm deduces the neighbors of each node in $G$. The algorithm can encounter the following inconsistency when deducing graph structure: The neighborhood set learned for node $u$ may include $v$, but the neighborhood set learned for $v$ may not include $u$.
 \item For the Ising model case, this is usually not a problem, as the analysis by Ravikumar etal guarantees consistent signed edge recovery when certain conditions are met.
 \item In the case of general discrete graphical models, during our early experimetns, we resolve inconsistencies using the OR rule.
\end{itemize}

\subsection{Related work}
\begin{itemize}
 \item Mention some related work in order to contrast with other approaches to structure learning.
 \item Chow-Liu: assume tree structure.
 \item Learning graphical models using contingency tables.
 \item Koller paper which learns graphical models using l1 regularization.
 \item Learning conditional random fields using l1l2 regularization.
\end{itemize}


\section{Experiment Setup}
\begin{itemize}
 \item Overview.
\end{itemize}

\subsection{Test distributions.}
\begin{itemize}
 \item Picking a topology. We have tried star graphs and chain structured graphs.
 \item Range of each random variable.
 \item Selecting the parameters: For a given number of nodes and for a given topology, the parameter array is drawn uniformly at random from [0, k]; k is usually 1 or 3.
 \item Importance of picking k correctly.
\end{itemize}

\subsection{Drawing samples.}
\begin{itemize}
 \item Sampling from the distribution: As we are sampling from tree structured graphical models, data is sampled exactly: MCMC is not used. Cite Mark Schmidt software.
 \item MCMC will be used for non-tree structured graphical models.
\end{itemize}

\subsection{Goals and Evaluation}
\begin{itemize}
 \item One of the goals of our experiments are to understand how many examples are needed for the algorithm to learn the structure of a pairwise discrete graphical model with $p$ nodes with high probability.
 \item Another of the goals of our experiments are to understand the dependency between the number of samples $n$, the number $p$ of nodes in $G$, and the parameter $\gl$ used by the neighborhood learning algorithm.
 \item In order to address the first goal, we draw multiple samples sets of increasing size ($n \in [2^{5}, 2^{14}]$) from the distribution $D$ and do the following for each choice of $n$: We fix a suitable $\gl$ either manually or using validation, and finally we empirically determine the probability of success of the structure learning algorithm.
 \item In order to address the second goal, we plot the $\gl$ used during the previous process either against $n$ or against $\sqrt{\frac{\log p}{n}}$.
 \item Procedure used in solving the logistic regression problem in its contrained optimization form.
\end{itemize}


\subsection{Choosing the regularization parameter}
\begin{itemize}
 \item Picking the parameter $c$ while solving the logistic regression problem in its constrained optimization form.
 \item This is one of the most vexing problems during experimentation.
 \item Dependence of the sparsity in the structure learned on the regularization parameter.
 \item Dependence of the ideal regularization parameter on p and n, according to the Obozinsky paper.
\end{itemize}

\subsubsection{The validation procedure.}
\begin{itemize}
 \item One can manually pick the regularization parameter - by picking the one which yields a model of expected sparsity.
 \item One can also use validation to automatically pick regularization parameters. This process involves assigning scores to regularization parameters based on how good it is. The goodness of a given regularization parameter depends on the goodness of the model parameters learned using that regularization parameter. This raises the question: How good is a given model parameter array $\gt$?
 \item Goodness of the model parameters can either be judged based on its sparsity, or based on its likelihood given some observations.
 \item We will consider the latter first. As determining the likelihood of $\gt$ requires the computation of the log partition function, we often find it better to compute the pseudolikelihood of the parameter. However, experiments revealed that this way of picking parameters is not reliable.
 \item Rate model parameters based on its ability to yield the desired sparsity. For our experiments, we use this method - not only is it more reliable, it is also faster. We use binary search here.\chk Hopefully, analysis will help us identify the parameters even without knowing beforehand the sparsity structure of the graphical model generating the observations.
 \item Describe k-fold validation. Draw k samples-sets. Learn the model parameters corresponding to a given regularization parameter using each one of these sample sets, and determine the average goodness of the model parameters thus learned. This is used to rate the corresponding regularization parameter.
\end{itemize}

\subsection{Solving the optimization problem efficiently}
\begin{itemize}
 \item Van Greer algorithm to solve the problem in its Lagrangian formulation. This formulation is generally more convenient for the purpose of analysis.
 \item Projected Quasi-Newton algorithm to solve the problem in its constrained optimization formulation.
\end{itemize}


\section{Results and discussion}
\begin{itemize}
 \item Figures corresponding to star graph experiments using the constrained optimization problem. They form a sort of \textit{proof of concept} for the structure learning algorithm.
 \item Figures corresponding to star graph experiments using the lagrangian formulation of the problem. They show that using sparsity-based validation to learn the best $\gl$ succeeds, but other experiments show that the $k$ in k-fold validation must be larger. Explain the small number of nodes tested.
 \item Failure of chain graph experiments.
 \item For a given p, charactarize the dependence of the regularization parameter on n.
 \item For a given n, charactarize the dependence of the regularization parameter on p.
\end{itemize}


\section{Conclusion}
\begin{itemize}
 \item Repeat the abstract in past tense.
\end{itemize}

\subsection{Future work}
\begin{itemize}
 \item Analysis.
 \item Application to a problem of practical interest.
\end{itemize}

\section{Acknowledgements}
\begin{enumerate}
 \item Acknowledge Pradeep, Ali Jalali, Sujay.
\end{enumerate}


% \bibliographystyle{plain}
% \bibliography{../references}

\end{document}
