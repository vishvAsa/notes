\documentclass{article}
% \usepackage[left=3cm,top=1.5cm,right=3cm,bottom=3cm]{geometry}
\usepackage{caption, subfigure}
\input{../../../../packages}
\input{../../../../macros}

%opening
\title{Block coordinate descent algorithm for L1/L2 Regularized Logistic Regression}
% \author{vishvAs vAsuki}
\date{}
\begin{document}
\maketitle

\abstract
The strucutre learning algorithm proposed for discrete graphical models described in \citet{ravikumarIsing09} involves solving an L1/L2 Regularized Logistic Regression problem. In this note, we describe the Block coordinate descent algorithm being used for solving this problem. This algorithm is an adaptation of the algorithm proposed in \citet{meierRS2008}. Early results from experiments which use this algorithm are described in a course project report\cite{sparsityStructureAlgorithmsReport}.

\section{The discrete graphical model and its parametrization}
Consider a discrete pairwise graphical model describing a probability distribution over $p$ variables, each of which can take one of $m$ discrete values. In this report, we use a non-minimal parametrization for the graphical model; and we assume that the node potentials are all constant:

\begin{equation}
Pr(x|\gTh) = \prod_{(i, j) \in E}\gf_{i, j}(x_i, x_j) \propto \exp(\sum_{(i, j)\in E} \gTh_{i, j, x_i, x_j}),
\label{eqn:distributionDiscreteGM}
\end{equation}

This can also be written as $$Pr(x|\gTh) = \exp(\sum_{(i, j)\in E, (l, k) \in \set{1, .. m}^{2}} \gTh_{i, j, l, k} I[x_i = l, x_j = k]),$$ but we often resort to the former, more succinct notation.

Thus, a probability distribution is completely specified by\\ $\gTh \in R^{p \times p \times m \times m}$, with $\forall (i, j)\notin E: \gTh_{i, j, :, :} = 0$. Note that, not being a minimal parametrization, there exist several sets of variables which describe exactly the same probability distribution. For example, one can replace the parameter array $\gTh_{i, j} \in R^{m \times m}$ corresponding to edge $(i, j)$ with $\gTh'_{i, j} = \gTh_{i, j} + k 11^{T}$ and still describe the same probability distribution. Minimal representations are described in \citet{pradeepPrivate} and \citet{ravikumarIsing09}.

\section{The neighborhood learning algorithm}
\subsection{The optimization problem}
The corresponding conditional probability distribution for the node $i$ is \begin{equation}
Pr(X_i = x_i|X_{/i} = x_{/i}, \gTh) = \frac{\exp(\sum_{j \in V - \set{i}} \gTh_{i, j, x_i, x_j})}{\sum_l \exp(\sum_{j \in V - \set{i}} \gTh_{i, j, l, x_j})}.
\label{eqn:condProbabilityDiscrete}
\end{equation}
Above, it is assumed that $\forall j \notin \nbd(i), \forall k: \gTh_{i, j, x_i, k} = 0$.

Given $n$ observations $S = \set{x^{(j)}}$, and viewing $x_i$ as a response variable whose value depends on the indicator variables like $I[x_j = k]$, we use this to construct a negative log likelihood function: 
$$nll_i(\gTh_{i, :, :, :}|x^{(k)}) = - \sum_{j\in V - \set{i}} \gTh_{i, j, x_i^{(k)}, x_j^{(k)}} + \log[\sum_l \exp(\sum_{j \in V - \set{i}}\gTh_{i, j, l, x_j^{(k)}})].$$

To determine the neighborhood $\nbd(i)$ of node $i$, we solve the following problem:
\begin{equation}
\argmin_{\gTh_{i, :, :, :}} \set{n^{-1} \sum_{k=1}^{n}nll_i(\gTh_{i, :, :, :} |x^{(k)}) + \gl \sum_{j} \norm{\gTh_{i, j, :, :}}_2}.
\label{eqn:optProblemLagrangianDiscrete}
\end{equation}

\subsection{Gradient and the Hessian}
We now describe the computation of the gradient\\
$\gradient_{\gTh} nll_i(\gTh_{i, :, :, :} |x^{(k)}) = G \in R^{p \times m \times m}$.

\begin{equation*}
\begin{split}
\forall v \in V - \set{i}:\\
G_{v, x_i, x_v} = -1 + (\sum_l \exp(\sum_{j \in V - \set{i}}\gTh_{i, j, l, x_j^{(k)}}))^{-1} exp(\sum_{j \in V - \set{i}}\gTh_{i, j, x_i^{(k)}, x_j^{(k)}}))\\
G_{v, k \neq x_i, x_v} = (\sum_l \exp(\sum_{j \in V - \set{i}}\gTh_{i, j, l, x_j^{(k)}}))^{-1} exp(\sum_{j \in V - \set{i}}\gTh_{i, j, k, x_j^{(k)}}))\\
\end{split}
\end{equation*}
$G_{i, j, k} = 0$ is used for all other $i, j, k$. $n^{-1}\sum_{k=1}^{n}\gradient_{\gTh} nll_i(\gTh_{i, :, :, :} |x^{(k)})$ is then used in the coordinate descent algorithm.

We now compute the diagonal elements of the Hessian. To do this, we describe the matrix $H \in R^{p \times m \times m}$, where $H_{i, j, k} = \frac{\partial^{2} nll_i(x)}{\partial \gTh_{i, j, k}^{2}}$. The second order derivatives of $n^{-1} \sum_{k=1}^{n}nll_i(\gTh_{i, :, :, :} |x^{(k)})$ is then computed using this. This computation is easily done together with the computation of the gradient.

\begin{equation*}
\begin{split}
H_{v, k, x_v} = -(\sum_l \exp(\sum_{j \in V - \set{i}}\gTh_{i, j, l, x_j^{(k)}}))^{-2} exp(2\sum_{j \in V - \set{i}}\gTh_{i, j, k, x_j^{(k)}})) \\
+ (\sum_l \exp(\sum_{j \in V - \set{i}}\gTh_{i, j, l, x_j^{(k)}}))^{-1} exp(\sum_{j \in V - \set{i}}\gTh_{i, j, k, x_j^{(k)}})).
\end{split}
\end{equation*}

Observe from the above that $1 \geq H_{v, x_r, x_v} \geq 0$.

\subsection{The block coordinate descent algorithm}
The algorithm is specified as Algorithm \ref{alg:coordinateDescent}.

\begin{algorithm}[ht]
Input: $\gTh_i^{(0)}$, Sample set $S = \set{x^{(r)}}$ of $n$ points, node $i$ whose neighborhood is to be determined, $\gl$, tol.\\
Output: $\gTh_i$.\\
\begin{algorithmic}
\STATE $\gTh_i \gets \gTh_i^{(0)}$
\LOOP
\FORALL{$v \in V - \set{i}$}
\STATE Compute $\bar{nll}(\gTh_i) = n^{-1}\sum_{k} nll_i(\gTh_{i, :, :, :}|x^{(k)})$.
\STATE Compute $G = \gradient_{\gTh_i} \bar{nll}(\gTh_i)$. [We only need $G_{v, :, :}$ actually.]
\STATE Together with G, compute the array $H'$ with $H'_{i, j, k} = \frac{\partial^{2} \bar{nll}(\gTh_i)}{\partial \gTh_{i, j, k}^{2}}$. [We only need $H'_{v, :, :}$ actually.]
\STATE $h_v \gets -\max \set{H', 10^{-5}}$. [Actually, need to do $h_v \gets -\max H'_{v, :, :}$.]
\STATE $T \gets -G_{v, :, :} - h_v \gTh_{v, :, :}$.

\STATE $D \gets p\times m \times m$ array of zeros.
\IF{$\norm{T}_F \geq \gl$}
\STATE $D_{v, :, :} \gets - \gTh_i$.
\ELSE
\STATE $D_{v, :, :} \gets - h_v^{-1}[-G_v - \gl \frac{T}{\norm{T}_F}]$.
\ENDIF

\IF{$\max \abs{D} \geq tol$}
\STATE Get $\ga$ from line search along $D$ with $\ga_0 = 2, \gd = 0.75, \gs = .01$.
\STATE $\gTh_{i, v} \gets \gTh_{i, v} + \ga D_v$.
\ENDIF

\ENDFOR
\STATE Return if decrease in objective value is less than $tol$.
\ENDLOOP
\end{algorithmic}
\caption{Block coordinate descent algorithm}
\label{alg:coordinateDescent}
\end{algorithm}

\bibliographystyle{plainnat}
\bibliography{../../../probabilisticModels}

\end{document}
