\def\CliqueSet{\mathcal{C}}
\def\clique{C}

\section{Problem Setup and Notation}

\noindent We consider the task of estimating the structure associated with a general discrete Markov random field. Let $X =
(X_1,\hdots,X_p)$ be a random vector, each variable $X_i$ taking
values in a discrete set $\mathcal{X}$ of cardinality $m$, say $\mathcal{X} =
\{1,2, \ldots, \statenum\}$. Let $G = (V,E)$ denote a graph with $p$ nodes corresponding to the $p$ variables
$\{X_1,\hdots,X_p\}$, and let $\CliqueSet$ be a set of cliques (fully-connected subgraphs)
of the graph $G$. Let $\{ \phi_{\clique} : \mathcal{X}^{|\clique|} \mapsto \real, \; \clique \in \CliqueSet \}$
denote a collection of potential functions associated with the cliques of the graph.
These functions can be used to define a Markov random field over $(X_1, \ldots, X_\pdim)$, with
density
\begin{eqnarray}
\label{EqnGenMRFPot}
\mathbb{P}(x) & \propto & \exp \biggr \{\sum_{\clique \in \CliqueSet} \phi_\clique(x_\clique) \biggr \}.
\end{eqnarray}

\noindent Since $\mathcal{X}$ is discrete, each potential function $\phi_{\clique}$
can be parameterized as linear combinations of $\{0,1\}$-valued indicator functions.  
Starting with node-wise indicators, for each $s \in \vertex$ and $j
\in \{1, \ldots, \statenum-1\}$, we define
\begin{eqnarray*}
\I[x_{s} = j] & = & \begin{cases} 1 & \mbox{if $x_s =j$} \\ 
											0 & \mbox{otherwise.}
                    \end{cases}
\end{eqnarray*}
Note that we omit an indicator for $x_s = \statenum$ from the list, since
it is redundant given the indicators for $j = 1, \ldots, \statenum-1$.
In a similar fashion, we define the $|\clique|$-way clique-wise indicator functions 
$\I[x_\clique = v]$, for $v \in \{1,2,\ldots, \statenum - 1\}^{|\clique|}$.

\noindent With this notation, any set of potential functions can then be written as
\begin{eqnarray*}
\phi_{\clique}(x_\clique) & = & \sum_{v \in \{1, \ldots, \statenum-1 \}^{|\clique|}}
	\theta^*_{\clique;v} \; \I[x_\clique = v] \quad \mbox{for $\clique \in \CliqueSet$}
\end{eqnarray*}
Thus, \eqref{EqnGenMRFPot} can be rewritten as,
\begin{align}
\label{EqnGenMRF}
\mathbb{P}(x) & \propto & \exp \biggr \{\sum_{\clique \in \CliqueSet; v \in \{1, \ldots, \statenum-1 \}^{|\clique|}} \theta^*_{\clique;v} \, \I[x_\clique = v] \biggr \}.
\end{align}

Thus, the Markov random field can be parameterized in terms of the collection of tensors $\theta^* := \{\theta^*_{\clique;v} \; \clique \in \CliqueSet; v \in \{1, \ldots, \statenum-1 \}^{|\clique|}\}$. In the sequel, it will be useful to collate these into vectors $\theta^*_{\clique} \in \real^{(\statenum-1)^{|\clique|}}$ associated with the cliques $\clique \in \CliqueSet$. 

{\bf \emph{Pairwise Markov Random Fields.}}

Here the set of cliques consists of the set of nodes $V$ and the setof edges $E$. Thus,
using nodewise and pairwise indicator functions as before, any pairwise MRF over $(X_1, \ldots, X_\pdim)$
can be expressed as
\begin{align}
\label{EqnGenMRFPairwise}
\nonumber \mathbb{P}(x)  &\propto  \exp \biggr \{\sum_{s \in \vertex; j \in \{1, \ldots, \statenum-1 \}} \theta_{s;j}\I[x_s = j] 
	\\
& + \sum_{(s,t) \in \edge; j,k \in \{1, \ldots, \statenum-1 \}} \theta_{st;jk} \I[x_s = j,\,x_t = k] \biggr \},
\end{align}
for a set of parameters $\theta^* := \{\theta^*_{s;j}, \theta^*_{st;jk} :\, s,t \in V;\, (s,t) \in E;\, j,k \in \{1, \ldots, \statenum-1 \}\}$.
It will be useful to collate these into vectors $\theta^*_s \in \real^{\statenum-1}$ for each $s \in \vertex$, and the vectors $\theta^*_{st} \in \real^{(\statenum-1)^2}$ associated with each edge.

\subsection{Graphical Model Selection}

Suppose that we are given a collection $\Data \defn \{x^{(1)}, \ldots, x^{(\numobs)} \}$ of $\numobs$ samples, where each
$\pdim$-dimensional vector  $x^{(i)} \in \{1,\ldots,\statenum\}^\pdim$ is drawn i.i.d. from a distribution $\mprob_{\theta^*}$ of the
form~\eqref{EqnGenMRF}, for parameters $\theta^*$ and graph $\graph = (\vertex, \edge^*)$ over the $\pdim$ variables. 
The goal of \emph{graphical model selection} is to infer the edge set $\edge^*$ of the graphical model defining the probability distribution that generates the samples. Note that the true edge set $\edge^*$ can also be expressed as a function of the parameters as
\begin{eqnarray}
\label{EqnEdge}
\edge^* = \{(s,t) \in V \times V :\, \exists \, \clique \in \CliqueSet; \{s,t\} \in \clique;\; \theta^*_{\clique} \neq 0 \}.
\end{eqnarray}

In this paper, we focus largely on the special case of pairwise Markov random fields.

\subsubsection{Pairwise Model Selection}

Recall that a pairwise MRF is parameterized by vectors $\theta^*_s \in \real^{\statenum-1}$ for each $s \in \vertex$, and the vectors $\theta^*_{st} \in \real^{(\statenum-1)^2}$ associated with each edge $(s,t) \in E^*$.  It is convenient to view the parameter vector $\theta^*$ as a
a collection of $\binom{\pdim}{2}$ vectors in  $\real^{\statenum-1}$ indexed by pairs of distinct vertices,  but non-zero if and only if the vertex pair $(s,t)$ belongs to the unknown edge set $\edge^*$ of the underlying graph $\graph$. With this set-up, we now describe a graph selection procedure for the pairwise model that is the natural generalization of the procedures for binary graphical models~\citep{RWLIsing} and Gaussian graphical models~\citep{Meinshausen:06}.  Specifically, we focus on recovering for each vertex $\svert \in V$, its neighborhood set, and then combine the neighborhood sets across vertices to form the graph estimate.

\noindent Let us define the vector $\Theta^*_{\backslash \svert} \in \real^{(\statenum-1)^2(\pdim-1)}$, which is the concatenation of the (short) vectors $\theta^*_{\svert t}$ for all $t\in\vertex\backslash\{\svert\}$. Note that unless vertex $\svert$ is connected to all of its neighbors, many of vectors $\theta^*_{\svert t}$ are zero.  In particular, the
problem of neighborhood estimation for vertex $\svert$ corresponds to the recovery of the set
\begin{eqnarray*}
\N(\svert) & = & \biggr \{ u \in \vertex \backslash \{\svert\} \, \mid
\, \|\theta^*_{\svert u}\|_0 \neq 0 \biggr \}.
\end{eqnarray*}

This is precisely the structure captured by \emph{group-sparsity}. 

\myparagraph{Group-sparsity}
Consider vectors in $\real^p$, and suppose that the index set $\{1,\hdots,p\}$ can be partitioned into a set of $T$ disjoint groups $\mathcal{G} = \{G_1,\hdots,G_T\}$. With this setup, a vector $u \in \real^p$ is said to be group-sparse with respect to $\mathcal{G}$ if the set $S = \{j \in  \{1,\hdots,T\}: u_{G_j} \neq 0\}$ has small cardinality. 

Consider the parameter vector $\Theta^*_{\backslash \svert} \in \real^{(\statenum-1)^2(\pdim-1)}$. By construction, it has groups of parameters collated on the edges. Letting $\mathcal{G}_{rt}$ be the index set of parameters on the  $(r,t)$ edge, $\{\theta_{rt;jk} ;\; j,k \in \{1,\hdots,\statenum - 1\}\}$, it then has the partition into groups given by $\mathcal{G} = \{ \mathcal{G}_{rs}; s \in  \vertex \backslash \svert\}$. We can then see that a small neighborhood $\N(\svert)$ for node $r$ entails that $\Theta^*_{\backslash \svert}$ is group-sparse with respect to $\mathcal{G}$.

Let us now set up notation for group-structured norms. For any vector $u \in \real^p$ where $\{1,\hdots,p\}$ is partitioned into a set of $T$ disjoint groups $\mathcal{G}$ as before, we define $\|u\|_{\mathcal{G},a,b} = \|(\|u_{G_1}\|_{a},\hdots, u_{G_T}\|_{a})\|_b$. We will typically suppress the dependence on the partition $\mathcal{G}$ when it is clear from context. The choice $a = 1, b = 2$ yields the group-lasso penalty~[cite], which has been shown to encourage group-sparsity~[cite]. Other choices such as $a = 1, b = \infty$ have also been well studied~[cite]. 

In order to estimate the neighborhood $\N(\svert)$, we thus consider performing a regression of $X_\svert$ on the rest of the variables $X_{\backslash \svert}$, using the group-sparse $\|\cdot\|_{1,2}$ penalty $\norm{\Theta_{\backslash \svert}}{1,2} \, \defn \, \sum_{u \in \vertex \backslash \{\svert\}} \|\theta_{\svert u}\|_2$. The conditional distribution of $X_\svert$ given the other variables $X_{\backslash \{\svert\}} = \{X_{t}\;|\; t \in V \backslash \{\svert\}\}$ takes the form
\begin{align}
\label{EqnMultiClassLogistic}
\nonumber & \mathbb{P}_{\Theta^*} \big[X_\svert = j \, \mid X_{\backslash \svert} = x_{\backslash \svert}\big] = \\
& \;\; \frac{\exp\left(\theta_{\svert;j}^* + \sum_{t \in V\backslash \{\svert\}} \sum_{k} \theta^*_{\svert t;jk} \I[x_t = k]\right)} {1+\sum_{\ell}\exp\left(\theta_{\svert;\ell}^* + \sum_{t \in V\backslash \{\svert\}} \sum_{k} \theta^*_{\svert t;\ell k} \I[x_t = k]\right)}
\end{align}
for all $j \in \{1,\hdots,m-1\}$. Thus, $X_\svert$ can be viewed as the response variable in a multiclass
logistic regression, in which the indicator functions associated with
the other variables
\begin{equation*}
\Big\{\I[x_t = k] ,\,t \in V \backslash \{\svert\}, \; k\in \{1, 2, \ldots,
\statenum-1\}\Big\},
\end{equation*}
play the role of the covariates.

Thus, we consider the following convex program,
\begin{equation}
\label{EqnPairwiseGroup}
\hat{\Theta}_{\backslash \svert} \in \min_{\Theta_{\backslash \svert} \in \real^{(\statenum-1)^2(\pdim-1)}} \biggr\{ \ell(\Theta_{\backslash \svert}; \Data) +
	\regpar_\numobs \norm{\Theta_{\backslash \svert}}{1,2}
	\biggr\},
\end{equation}
where $\ell(\Theta_{\backslash \svert}; \Data) = \frac{1}{\numobs} \sum_{i=1}^\numobs \ell^{(i)}(\Theta_{\backslash \svert}; \Data) \defn
\frac{1}{\numobs} \sum_{i=1}^\numobs \log \mathbb{P}_{\Theta}
\left[X_\svert = x^{(i)}_{\svert} \, \mid \, X_{\backslash\svert} = x^{(i)}_{\backslash \svert} \right]$
is the rescaled multiclass logistic likelihood defined by the conditional distribution~\eqref{EqnMultiClassLogistic}, and
$\regpar_\numobs > 0$ is a regularization parameter. The convex program~\eqref{EqnPairwiseGroup}, an $\ell_1/\ell_2$-regularized multiclass logistic regression problem, is thus the multiclass logistic analog of the group Lasso~[cite]. 

The solution of this program yields an estimate $\hat{\N}(\svert)$ of the neighborhood of node $r$ by 
\begin{align*}
	\hat{\N}(\svert) = \{t \in \vertex : t \neq r ; \| \hat{\theta}_{rt} \neq 0\}. 
\end{align*}

We are interested in the event that all the node neighboorhoods are estimated exactly, $\{\hat{\N}(\svert) = \N(\svert); \svert \in \vertex\}$,
which we also write as $\{\hat{E} = E^*\}$ since it entails that the the full graph is estimated exactly.

\myparagraph{Sparsistency}

Our main result is a high-dimensional analysis of the estimator~\eqref{EqnPairwiseGroup}, where allow the problems dimensions such as the number of nodes $p$,
the maximum node degree $d$, the size of the state space $m$ (and in the case of higher-order MRFs, the maximum clique size $c$) to vary with the number of observations $n$  Our goal is to establish sufficient conditions on the scaling of $(\numobs, \pdim, \degmax, \statenum, c)$ such that our proposed estimator is consistent in the sense that
\begin{eqnarray*}
\mprob \left[\widehat{\edge}_\numobs = \edge^* \right] & \rightarrow &
1 \qquad \mbox{as $\numobs \rightarrow +\infty$}.
\end{eqnarray*}
We sometimes call this property sparsistency, as a shorthand for consistency of the sparsity pattern of the parameters.

\subsubsection{Higher-order Model Selection}

Let us first see what this model selection recipe of node-wise regression with group-sparse regularization, would entail when extended to the general higher-order Markov random fields~\eqref{EqnGenMRF} case.  Recall that such a higher-order MRF  is parameterized by vectors $\theta^*_{C} \in \real^{(\statenum - 1)^{|C|}}$ for $C \in \mathcal{C}$. Let $\maxcliquesize$ be the maximum clique size. It would be convenient to view the parameters as a collection of $\sum_{j=1}^{c} \binom{p}{j}$ vectors indexed by a cliques $C$ of size less than or equal to $\maxcliquesize$, but non-zero if and only if the clique $C \in \mathcal{C}$. 

Again, we fix a node $r$, and define the long vector $\Theta*_{\backslash \svert} \in \real^{\sum_{j=1}^{c-1} \binom{p-1}{j} (m-1)^{j+1}}$ as the concatenation of the parameter vectors $\theta^*_{rC}$ for all $C \subseteq \vertex \backslash \svert;\, |C| < c$. Note that recovery of the neighborhood of a vertex $\svert$ corresponds to the recovery of the set
\begin{eqnarray*}
\N(\svert) & = & \biggr \{ u \in \vertex \backslash \{\svert\} \, \mid
\, \exists \, C \subseteq \vertex \backslash \{\svert,u\} ;\; \|\theta^*_{\svert u C}\|_0 \neq 0 \biggr \}.
\end{eqnarray*}
Thus, we could again make use of group sparsity. Consider the parameter vector $\Theta^*_{\backslash \svert}$. Letting $\mathcal{G}_{rC}$ be the index set of parameters on the  ${r} \cup C$ clique, $\{\theta_{rC;jv} ;\; j \in \{1,\hdots,\statenum - 1\}; v \in \{1,\hdots,\statenum - 1\}^{|C|}\}$, $\Theta^*_{\backslash \svert}$ thus has the partition into groups given by $\mathcal{G} = \{ \mathcal{G}_{rC}; C \subseteq  \vertex \backslash \svert\, |C| < c\}$. We can then see that a small neighborhood $\N(\svert)$ for node $r$ entails that $\Theta^*_{\backslash \svert}$ is group-sparse with respect to $\mathcal{G}$. The group-structured penalty would then take the form $\|\Theta^*_{\backslash \svert}\|_{1,2} = \sum_\{C \subseteq  \vertex \backslash \svert\, |C| < c\} \|\theta^*_{rC}\|_{2}$.

Thus we would solve:
\begin{equation}
\label{EqnGroupHigherOrder}
\min_{\Theta_{\backslash \svert} \in \real^{\sum_{j=1}^{c-1} \binom{p-1}{j} (m-1)^{j+1}}} \biggr\{ \ell(\Theta_{\backslash \svert}; \Data) +
	\regpar_\numobs \norm{\Theta_{\backslash \svert}}{1,2} \biggr\},
\end{equation}

where $\ell(\Theta_{\backslash \svert}; \Data)$ is the likelihood of the data as before. \citet{Dahinden07,Dahinden10} studied the related program of $\ell_1/\ell_2$ regularized maximum likelihood over the complete graph (instead of node-wise regressions) but showed good empirical performance of discrete graphical model structure recovery. The caveat with this approach is the prohibitive computational complexity of this procedure. Note that the number of parameters is $\sum_{j=1}^{c-1} \binom{p-1}{j} (m-1)^{j+1}$ which scales prohibitively even for moderate $c$. Indeed, even the computations in the pairwise case are expensive.

\myparagraph{A Simpler Estimate}
But as we show in Section~\ref{SecHigherOrder}, even when the underlying model is a higher order MRF, surprisingly just solving the pairwise program~\eqref{EqnPairwiseGroup} is \emph{sufficient} to recover the true edges, under certain conditions. Thus, in our second result,
we again analyze the sparsistency of the estimator in \eqref{EqnPairwiseGroup}, but for the case where the underlying graph is a higher-order MRF.







