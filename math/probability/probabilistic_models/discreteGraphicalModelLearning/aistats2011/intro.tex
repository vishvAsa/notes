\section{Introduction}

\myparagraph{Markov Random Fields and Structure Learning} Undirected graphical models, also known as Markov random fields, are used in a variety of domains, including statistical
physics~\citep{Ising25}, natural language processing~\citep{Manning:99}, image analysis~\citep{Woods78,Hassner80,Cross83}, and spatial
statistics~\citep{Ripley81}, among others. A Markov random field (MRF) over a $\pdim$-dimensional discrete random vector $X = (X_1, X_2,\ldots,X_p)$ is specified by an undirected graph $\graph = (\vertex, \edge)$, with vertex set $\vertex = \{1, 2, \ldots, \pdim \}$ -- one for each variable -- and edge set $\edge \subset \vertex \times \vertex$.  The structure of this graph encodes certain conditional independence assumptions among subsets of the variables. In this paper, we consider the task of structure learning, i.e.  estimating the underlying graph structure associated with a general discrete Markov random field from $\numobs$ independent and identically distributed samples $\{ x^{(1)}, x^{(2)}, \ldots, x^{(\numobs)} \}$.

\myparagraph{High-dimensional setting and Group sparsity} We are interested in structure learning in the setting where the dimensionality $p$ of the data is larger than the number of samples $n$. While classical procedures typically break down under such high-dimensional scaling, an active line of recent research has shown it is still possible to obtain practical consistent procedures by leveraging low-dimensional structure. The most popular example is that of leveraging sparsity using $\ell_1$-regularization (e.g.,~\cite{CandesTao06,Donoho:Elad:03,Meinshausen:06,ng:04,Tropp:06,Wainwright06_new,ZhaoYu06}). For MRF structure learning, such $\ell_1$-regularization has been successfully used for Gaussian \cite{Meinshausen:06} and discrete binary pairwise (i.e. Ising) models \citep{RWLIsing,LeeKoller07}. In these instances, there is effectively only one parameter per edge, so that a sparse graph corresponds to a sparse set of parameters. In this paper, we are interested in more general discrete graphical models -- where each variable can take $m$ possible values, and factors can be of order higher than two. We now have multiple parameters per edge, and thus the relevant low-dimensional structure is that of {\em group sparsity}: all parameters of an edge form a group, and a sparse graph now corresponds to certain \emph{groups of parameters} being non-zero. The counterpart of $\ell_1$ regularization for such group-sparse structure is $\ell_1/\ell_q$ regularization for $q > 1$, where we collate the $\ell_q$ norms of the groups, and compute their overall $\ell_1$ norm. Recent work on group and block-sparse linear regression \citep{TurlachVW, ZH08,NWJoint,Lounici09,Obozinski10,RLLWSPAM,BachMKL} show that under such group-sparse settings, group-sparse regularization outperforms the use of $\ell_1$ penalization.

\myparagraph{Our Results: Pairwise $m$-ary models} In this paper, we provide a quantitative consistency analysis of group-sparse regularized structure recovery for general discrete graphical models. We first consider the case of pairwise but otherwise $m$-ary discrete graphical models, and analyze a group-sparse variant of the procedures in ~\citep{RWLIsing,Meinshausen:06}: for each vertex $\svert \in V$, we estimate its neighborhood set using $\ell_1/\ell_2$-regularized maximum conditional likelihood. This reduces to multi-class logistic regression, for which we characterize the number of samples needed for \emph{sparsistency} $i.e.$ consistent recovery of the group-support-set with high probability. This analysis extends recent high-dimensional analyses for linear models to logistic models, and is of independent interest even outside the context of graphical models. We then combine the neighborhood sets across vertices to form the graph estimate. There has been a strong line of work on developing fast algorithms to solve these sparse multiclass logistic regression programs including ~\citet{MGB08,KCFH05}. Indeed, \citep{Dahinden07,Dahinden10} show good empirical performance using such $\ell_1/\ell_q$ regularization even with the joint likelihood over all variables.

\myparagraph{Our Results: General $m$-ary models} One (natural, but expensive)  extension to graphical models with higher-order factors is to again use group-sparse regularization but with higher order factors as groups. However, this leads to prohibitive computational complexity -- e.g. there are $\order(\pdim^{c})$ possible factors of order $c$. Indeed, in their empirical study of such regularizations, \citet{Dahinden07,Dahinden10} could scale up to small graph sizes, even while using some intelligent heuristics. This motivates our second main result. Suppose we solve the pairwise graphical model estimation problem, even when the true model has higher order factors. What is the relationship of this estimate with the true underlying graph? We investigate this for {\em hierarchical} graphical models where the absence of any lower-order factor also implies the absence of factors over supersets of the lower-order factor variables. Higher-order factors could, in principle, cause our pairwise estimator to include spurious edges. Surprisingly, we obtain the result that under slightly more stringent assumptions on the scaling of the sample size (dependent on the size of the higher-order factors) the pairwise estimator excludes the irrelevant edges, and includes all ``dominant'' pairwise edges whose parameters are larger than a certain threshold that depends on the size of the parameters values of higher-order factors. As a consequence, if all pairwise effects are dominant enough, we recover the graph exactly even while using a simple pairwise estimator. But even otherwise, the guaranteed false edge exclusion could be used for further greedy procedures, though we defer further discussion in the sequel.

\myparagraph{Existing approaches} Methods for estimating such graph structure include those based on constraint and hypothesis testing~\citep{spirtes:00}, and those that estimate restricted classes of graph structures such as trees~\citep{chowliu:68}, polytrees~\cite{dasgupta:99}, and hypertrees~\citep{srebro:03}. Another class of approaches estimate the local neighborhood of each node via exhaustive search for the special case of bounded degree graphs. Abbeel et al.\cite{AbbKolNg06} propose a method for learning factor graphs based on local conditional entropies and thresholding, but the computational complexity grows at least as quickly as $\order(\pdim^{\degmax+1})$, where $\degmax$ is the maximum neighborhood size in the graphical model. Bresler et al.~\cite{Bresler08} describe a related local search-based method, and prove under relatively mild assumptions that it can recover the graph structure with $\Theta(\log \pdim)$ samples. However, in the absence of additional restrictions, the computational complexity of the method is $\order(\pdim^{\degmax+1})$. Csisz\'{a}r and Talata~\cite{Csiszar:06} show consistency of a method that uses pseudo-likelihood and a modification of the BIC criterion, but this also involves a prohibitively expensive search.

% \myparagraph{Existing approaches} Methods for estimating such graph structure include those based on constraint and hypothesis testing~\citep{spirtes:00}: these estimate conditional
% independencies from the data, and then determine a graph that most closely represents those independencies. Another class of score-based approaches 
% search through a set of candidate graph structures, but unfortunately these are typically NP-hard~\cite{chickering:95}. Indeed, for general undirected graphical models with discrete random variables, calculation of the partition function or cumulant function associated with the Markov random field function is computationally intractable~\cite{Welsh93}. This motivated 
% estimating restricted classes of graph structures such as trees~\citep{chowliu:68}, polytrees~\cite{dasgupta:99}, and hypertrees~\citep{srebro:03}. 
% Another class of approaches is based on estimating the local neighborhood of each node via exhaustive search for the special case of bounded degree graphs. Abbeel et al.\cite{AbbKolNg06} 
% propose a method for learning factor graphs based on local conditional entropies and thresholding, and analyze its behavior in terms of Kullback-Leibler divergence between the fitted and true models. 
% They obtain a sample complexity that grows logarithmically in the number of vertices $\pdim$, but the computational complexity grows at least as quickly as $\order(\pdim^{\degmax+1})$, where $\degmax$ is the maximum neighborhood size in the graphical model. This order of complexity arises from the fact that for each node, there are $\binom{\pdim}{\degmax} = \order(\pdim^\degmax)$
% possible neighborhoods of size $\degmax$ for a graph with $\pdim$ vertices. Bresler et al.~\cite{Bresler08} describe a related simple search-based method, and prove under relatively mild assumptions that it can recover the graph structure with $\Theta(\log \pdim)$ samples. However, in the absence of additional restrictions, the computational complexity of the method is $\order(\pdim^{\degmax+1})$. Csisz\'{a}r and Talata~\cite{Csiszar:06} show consistency of a method that uses pseudo-likelihood and a modification of the BIC criterion, but this also involves a prohibitively expensive search. 

%\citet{BachHMKL08} propose and analyze a greedy active-set algorithm for solving group-sparse regularized estimators, for the case where the features (sufficient statistics of the graphical model in our case) are drawn from a RKHS corresponding to an amenable hierarchical set of kernels.

%% Shrug off structured group-sparse variants
%It also remains to investigate the consequences and extensions of our analysis on structured variants of group-sparse regularizations such as the hierarchical Composite Absolute Penalties (CAP) family~\citep{ZRYCAP}, structured sparsity~\citep{HZM09} and hierarchical multiple kernel learning~\citep{BachHMKL08}.
