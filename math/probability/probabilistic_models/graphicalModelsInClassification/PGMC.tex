\documentclass{article}
\input{packages}
\input{macros}

\usepackage{amsmath,amssymb,amsthm,amsfonts}

\usepackage{fullpage}
\usepackage{graphicx,subfigure}
\usepackage{epic,eepic,eepicemu}
\usepackage{epsf}
\usepackage{epsfig}
\usepackage{fancyhdr}
\usepackage{graphics}
\usepackage{psfrag,latexsym}
\synctex=0
\usepackage[T1]{fontenc}
%\usepackage{avant}
\renewcommand*\familydefault{\sfdefault}


\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}

\newcommand\myparagraph[1]{\noindent {\it #1}.}
\def\real{\mathbb{R}}
\def\statesp{\mathcal{X}}
\def\vertex{V}
\def\edge{\ensuremath{E}}
\def\graph{G}
\def\edgeEst{\hat{E}}
\def\thetaEst{\hat{\theta}}
\def\thetaEstFinal{\bar{\theta}}
\def\mprob{\mathbb{P}}
\def\I{\mathbb{I}}

\title{Classification using PGMs}
\date{}

\begin{document}
\maketitle

\section{Setup and Notation}

We are concerned with the classification task of predicting  a discrete response $Y \in \mathcal{Y}$ given a set of 
discrete predictors $X = (X_1,\hdots,X_p) \in \mathcal{X}^p$. For simplicity, we will consider the case where $\mathcal{Y} = \mathcal{X} = \{0,1\}$,
and consider the general discrete case in the sequel. We are given $n$ samples $D = \{(X^{(i)}, Y^{(i)})\}_{i=1}^{n}$. 

The goal is to contrast \emph{generative} and \emph{discriminative} approaches for classification. In a generative model, we learn the joint distribution $P(X,Y)$, and predict
the response using the induced conditional distribution $P(Y|X)$. In a discriminative model, we directly learn the conditional distribution $P(Y|X)$.

We are interested in the case where the distribution over the predictors $X$ could be represented using graphical models.


\section{Generative Models}
\label{Sec:GenerativeModel}
We assume that the conditional distribution of $X$ given $Y$ for $Y \in \{0,1\}$ is distributed as a discrete graphical model. So, there is a separate discrete graphical model associated with $\mprob(X|Y=0)$ and with $\mprob(X|Y=1)$. In the case of either graphical model, the notation and setting described below will be used.

\subsection{Graphical model notation and assumptions.}
Let $X = (X_1, X_2, \ldots, X_p)$ denote 
a random vector, with each variable $X_s$ taking values in a corresponding set $\statesp_s$. Say we are given an
undirected graph $\graph$ with vertex set $\vertex = \{1,\ldots, p \}$ and edge set $\edge$, so that each random variable $X_s$ is associated with a vertex $s \in
\vertex$. The pairwise Markov random field associated with the graph $\graph$ over the random vector $X$ is the family of distributions of $X$ which factorize as $\mprob(x) \propto \exp \left \{
\sum_{(s,t) \in \edge} \phi_{st}(x_s, x_t) \right\}$, where for each edge $(s,t) \in \edge$, $\phi_{st}$ is a mapping from pairs $(x_s, x_t) \in \statesp_s \times
\statesp_t$ to the real line.\footnote{Note that, for $(s, t) \notin \edge$, it is convenient to define $\phi_{st}(x_s, x_t) = 0$.}  For models involving discrete random variables, the pairwise assumption involves no loss of generality,
since any Markov random field with higher-order interactions can be converted (by introducing additional variables) to an equivalent
Markov random field with purely pairwise interactions (see Wainwright
and Jordan~\cite{WaiJor03Monster} for details of this procedure).\\

\myparagraph{Ising Model} In this paper, we focus on the special case of the Ising model in which $X_s \in \{-1,1\}$ for each vertex $s \in
\vertex$, and $\phi_{st}(x_s, x_t) = \theta^*_{st} x_s x_t$ for some parameter $\theta^*_{st} \in \real$, so that the distribution takes
the form
\begin{equation}
\label{EqnIsing}
\mprob_{\theta^*}(x) = \frac{1}{Z(\theta^*)} \exp \left \{\sum_{(s,t)\in \edge} \theta^*_{st} x_s x_t \right \}.
\end{equation}
The partition function $Z(\theta^*)$ ensures that the distribution
sums to one. As we are assuming that there are many conditional independence properties characterizing $\mprob_{\theta^*}(x)$ $\theta^*$ can be considered to be sparse.

\subsection{Parameters and their estimation.}
The graphical models associated with $\mprob(X|Y=0)$ and with $\mprob(X|Y=1)$ are distinct. Let $\mprob(X|Y=0)$ and $\mprob(X|Y=1)$ be parametrized by different parameter vectors $\theta^{*(0)}$ and $\theta^{*(1)}$ respectively, and let the associated graphs be $\graph_0 = (\vertex_0, \edge_0)$ and $\graph_1 = (\vertex_1, \edge_1)$. Let $D_i=\set{(x, i)| (x, i) \in D}$.

\subsubsection{Model Estimation}
\myparagraph{Scheme 1}
To estimate $\theta^{*(0)}$ and $\theta^{*(1)}$, one could follow the scheme proposed by Ravikumar et. al. \cite{RWLIsing} and estimate the $p$ parameters $\theta_{\backslash r}^{*(i)} = \set{\theta_{rt}^{*(i)} | t\in V \setdiff \set{r}}$ associated with each vertex $r$ by solving:
\begin{equation}
\label{EqnFtrSelectionGenerative}
\thetaEst_{\backslash r}^{(i)} = \argmin_{\theta_{\backslash r}} l(\theta_{\backslash r}|D_i) + \lambda_i \norm{\theta_{\backslash r}}_1,\\
\end{equation}
where $l(\theta_{\backslash r}|D_i) = - \sum_{x \in D_i} \log \mprob_{\theta_{\backslash r}}(x_r|x_{V \setdiff \set{r}})$.

\myparagraph{Scheme 2}
Suppose that $\theta^{*(0)}$ and $\theta^{*(1)}$ share sparsity structure. Then, one can do feature selection by solving:
\begin{equation}
\label{EqnFtrSelectionGenerativeJoint}
(\thetaEst^{(0)}, \thetaEst^{(1)}) = \argmin_{\theta^{0}, \theta^{1}} l(\theta^{1}|D_1) + l(\theta^{0}|D_0) + \lambda \sum_{j} \norm{[\theta^{1}_{j}\  \theta^{0}_{j}]}_2.\\
\end{equation}

\subsubsection{Using the classifier}
Once we have estimated the generative model parameters, we can utilize the discriminative classifier described in Equation \ref{eqnClassifier}, which results from the computation $\mprob(Y=1|x) \propto \mprob(x|Y=1)\mprob(Y=1)$.

\section{Discriminative Model}
Consider $\mprob(y|x)$ induced by $(\theta^{*0}, \theta^{*1})$, and let $\mprob(Y=1) = q$. Let $E = \edge_0 \union \edge_1$. $\mprob(y|x)$ is specified by
\begin{equation}
\label{eqnClassifier}
\mprob(Y=1|X=x) = \frac{\exp(\theta_0^* + \sum_{(s, t) \in E}\theta_{st}^* x_s x_t)}{1 + \exp(\theta_0^* + \sum_{(s, t) \in E}\theta_{st}^* x_s x_t)},
\end{equation}
where $\theta_0^* = \log(\frac{q}{Z(\theta^{*1})}) - \log(\frac{1-q}{Z(\theta^{*0})})$, and $\theta_{st}^* = \theta_{st}^{*1} - \theta_{st}^{*0}$.

Given $D$, we could estimate this discriminative model directly, without first estimating $\theta^{*1}$ and $\theta^{*0}$, by solving:
\begin{equation}
\label{EqnFtrSelectionDiscriminative}
\thetaEst = \argmin_{\theta} l(\theta|D) + \lambda \sum_{j>0}|\theta_j|, \\
\end{equation}
where $l(\theta|D) = -n^{-1}\sum_{(x, y) \in D} \log \mprob_{\theta}(y|x)$. By looking at the sparsity pattern in $\theta^*$, we estimate the edge-set $\edgeEst$.

Note that the discriminative classifier described by Equation \ref{eqnClassifier} corresponds to using the halfspace
\begin{equation}
\label{eqnDiscriminant}
f(x, \theta^{*i}) = \theta^{*i}_0 + \sum_{(s, t) \in E}\theta^{*i}_{st} x_s x_t \geq 0
\end{equation}
for classification.

\section{Bounds on Classification Error}
We wish to analyze the expected risk of the classifiers above as a function of $n$ and $p$. In doing this, we would be following the footsteps of Ng and Jordan \cite{ng2001}.

The hypothesis class considered while estimating $\theta^*$ directly by solving Problem \ref{EqnFtrSelectionDiscriminative} is at least as large as the hypothesis class considered while estimating $\theta^*$ by first estimating the generative model parameters. Hence, we note that as $n \to \infty$ the expected risk of the classifier in the former case is at least as low as the estimated risk of the classifier learned by estimating $\theta^{*1}$ and $\theta^{*2}$ first. We now analyze the $n$ required to learn a classifier which is almost as the classifiers mentioned for the $n \to \infty$ case.

\subsection{Discriminative case}
\subsubsection{Loose bounds on sample complexity}
As, in the case of logistic regression, the 0/1 misclassification error $\eps^m(\theta)$ is related to the logloss $\eps^l(\theta)$ by $\eps^l(\theta) \geq (\log 2)\eps^m(\theta)$, following the analysis in \cite{ng:04}, we note that $n = \Omega(\log p)poly(|E|)$ suffices. \footnote{It is possible that actually $n = \Omega(|E|\log p)$ suffices when we separate the feature selection step from the final estimation step and use the VC dimension for linear classifiers: \chk. This could improve known sample complexity for $\ell_1$ regularized logistic regression.}

\subsubsection{Tighter bounds}
\begin{theorem}
\label{thmMarginAnalysisDiscriminative}
 Suppose that $\theta^*$ is estimated as $\thetaEst$ as in Equation \ref{EqnFtrSelectionDiscriminative}, with the exception that $|\theta_0^*|$ is included in the regularization. \footnote{Note that using \cite{BachSelfConcordant} seems to require including $\theta_0$ in the regularization.}


Let $f(x, \theta^*)$ be the linear discriminant corresponding to Equation \ref{eqnDiscriminant}, and let $sgn(f(x, \theta^*))$ be the corresponding classifier. Similarly, let $sgn(f(x, \theta))$ be the classifier corresponding to $\thetaEst$.

Suppose that $\theta^*$ also satisfies the conditions specified for the application of Theorem 5 of \cite{BachSelfConcordant} in the below proof. In addition, suppose that $Pr_x(f(x, \theta^*) \leq \mu) \leq p_2$ for $\mu \geq k |\edge|\sqrt{\log p}/n$ for some constant $k$ specified in the proof.

Then, $Pr_y(sgn(f(x, \theta^*)) \neq sgn(f(x, \theta))) \leq p_1 + p_2$ for the failure probability $p_1$ described in the statement of Theorem 5 in \cite{BachSelfConcordant} \footnote{The proof of this theorem is yet to be verified and fixed to suit our purposes.}.
\end{theorem}
\begin{proof}
First, we rewrite the optimization problem so that Theorem 5 from \cite{BachSelfConcordant} can be applied.

\paragraph*{Feature map.} Any data point $x$ is mapped to $z$, a vector whose components are defined by: $z_0 = 1$ and $\set{z_{ij} = x_i x_j | \forall i \neq j}$. The length of $z$ is given by $p' = \binom{p}{2} + 1 = \Theta(p^2)$. Using this feature map, the set of points $D$ is mapped to the set of points $D'$. Once this is done, all $z$ is normalized, so that $\forall i: |z_i| = 1/\sqrt{n}$.

\paragraph*{Equivalent problem.} Next, the define parameters $\theta' = \sqrt{n} \theta^*$. Thus, equivalent to the model described in Equation \ref{eqnClassifier}, we have the model where $\mprob(Y = i) = \sigma(i \dprod{\theta', z})$, which is estimated by solving the optimization problem:
\begin{equation}
\thetaEst' = \argmin_{\theta} l(\theta|D') + \lambda \sum_{j \geq 0}|\theta_j|, \\
\end{equation}
where $l()$ is defined as in Equation \ref{EqnFtrSelectionDiscriminative}. Solving this optimization problem is equivalent to solving the optimization problem specified in the theorem statement.

Applying Theorem 5 from \cite{BachSelfConcordant}, using $\lambda = k \sqrt{\frac{\log p}{n}}$ for some constant $k$, we then conclude that, with probability at least $1 - p_1$:
$$\norm{\thetaEst' - \theta'}_1  = \sqrt{n}\norm{\thetaEst - \theta^*}_1 \leq k_1 |E| \sqrt{\frac{\log p}{n}},$$
where $k_1$ is another constant.

As $\norm{z}_{\infty} = 1$, we can conclude that
$$|f(x, \theta^*) - f(x, \thetaEst)| \leq k_1 |E| \sqrt{\log p}/n.$$

Applying Claim \ref{lemDisagreement}, we have the result.
\end{proof}

\begin{remark}
Note that in Theorem \ref{thmMarginAnalysisDiscriminative}, we considered an optimization problem which was slightly different from Equation \ref{EqnFtrSelectionDiscriminative}.

To achieve misclassification rate which is at most $p_1 + p_2$ greater than the best achievable misclassification rate, number of samples required is given by:
$$n = O(|E|\frac{\sqrt{\log p}}{\mu}).$$

Note that $p_1$ decreases exponentially with $n$, so it suffices to focus on controlling $p_2$ and $\mu$. In particular, if we impose conditions on $\theta^{*i}$ such that $\mu = \Omega(|E|)$, we have:
$$n = O(\sqrt{\log p}).$$\end{remark}
\subsection{Generative case}
\subsubsection{Loose bounds on sample complexity}
Given that feature selection is done and we have an estimate $\edgeEst$ of \edge, using the VC dimension of linear classifiers, we observe that $n = O(|\edgeEst|)$ examples are sufficient to achieve a low classification error rate. Below we consider ways of characterizing the number of samples $n$ required to get estimate $\edgeEst \supset \edge$ which is not too much larger than \edge.

Consider Scheme 1. From \cite{RWLIsing}, we know that if $\theta^{*1}$ and $\theta^{*2}$ satisfy certain strong conditions, $n = O(d^{3}\log p)$ is sufficient to estimate $\edge_i$ accurately. Considering the fact that even $\edgeEst_i \supset \edge_i$ suffices as long as it is not too much bigger than $\edge_i$, we can probably do better: that is, we should be able to use more relaxed conditions on $\theta^{*i}$, and we can make do with smaller $n$.

\subsubsection{Tighter bounds for Scheme 1}
\begin{claim}
\label{lemGenParameterEstimateError}
Suppose that $\theta^{*i}$ are estimated using Scheme 1. Also suppose that $\theta^{*i}$ satisfy the conditions required by Theorem 1 in \cite{RWLIsing}. Then, with probability at least $1 - p_1$ for the failure probability $p_1$ described in the statement of Theorem 1 in \cite{RWLIsing}, $$\norm{\theta^{*i} - \thetaEst^{i}}_2 \leq k_1 \sqrt{min(p, |\edge_i|)}\sqrt{\frac{d \log p}{n}}$$
and
$$\norm{\theta^{*i} - \thetaEst^{i}}_{\infty} \leq k_2 \sqrt{\frac{d \log p}{n}},$$
for some constants $k_1$ and $k_2$.
\end{claim}
\begin{proof}
Considering the proof of Proposition 1 on page 17 of \cite{RWLIsing}, and taking $\lambda_n = k \sqrt{\frac{\log p}{n}}$, we have:
$$\norm{\theta_{\backslash r}^{*i} - \thetaEst_{\backslash r}^{i}}_{\infty} \leq \norm{\theta_{\backslash r}^{*i} - \thetaEst_{\backslash r}^{i}}_2 \leq c \sqrt{\frac{d \log p}{n}},$$ where $c$ some constant independent of $d, p, n$ and $\theta_{\backslash r}$ is the vector of parameters $\set{\theta_{r, j},\ \forall j \in V \setdiff \set{r}}$.

From this, we already have the bound on $\norm{\theta^{*i} - \thetaEst^{i}}_{\infty}$.

We have the bound $\norm{\theta^{*i} - \thetaEst^{i}}_2 \leq c \sqrt{p}\sqrt{\frac{d \log p}{n}}$ by applying the generalized Pythagoras theorem. We also observe from the above that $\norm{\theta_{j, k}^{*i} - \thetaEst_{j, k}^{i}}_2 \leq c \sqrt{\frac{d \log p}{n}} \forall (i, j) \in E$. Again, applying the generalized Pythagoras theorem, we have: $\norm{\theta^{*i} - \thetaEst^{i}}_2 \leq c \sqrt{E}\sqrt{\frac{d \log p}{n}}$. Combining these, we have the bound on $\norm{\theta^{*i} - \thetaEst^{i}}_2$.
\end{proof}

\begin{theorem}
\label{thmMarginAnalysisGenerative}
 Suppose that $\theta^{*i}$ are estimated as $\thetaEst^{(i)}$ using Scheme 1, and let $f(x, \theta^*)$ be the linear discriminant corresponding to Equation \ref{eqnDiscriminant}, and let $sgn(f(x, \theta^*))$ be the corresponding classifier. Similarly, let $sgn(f(x, \theta))$ be the classifier corresponding to $\thetaEst^{(i)}$.

Suppose that $\theta^{*i}$ satisfy the conditions described in Claim \ref{lemGenParameterEstimateError}. In addition, suppose that $Pr_x(f(x, \theta^*) \leq \mu) \leq p_2$ for $\mu \geq k \sqrt{|\edge|} \sqrt{\min(p, |\edge|)}\sqrt{\frac{d \log p}{n}}$ for some constant $k$ specified in the proof.

Then, $Pr_y(sgn(f(x, \theta^*)) \neq sgn(f(x, \theta))) \leq p_1 + p_2 + p_3$ for the failure probability $p_1$ described in the statement of Theorem 1 in \cite{RWLIsing} and $p_3$ is specified in the proof below.
\end{theorem}
\begin{proof}
For some constant $k_1$ from Claim \ref{lemGenParameterEstimateError}, with probability at least $1-p_1$,
$$\norm{\theta - \theta^*}_2 \leq k_1 \sqrt{min(p, |\edge|)}\sqrt{\frac{d \log p}{n}}.$$

Given that this holds, as $|x_s x_t| = 1$, 
$$\sum_{(s, t) \in E}(\theta_{st} - \theta_{st}^*) x_s x_t \leq \norm{\theta - \theta^*}_2 \sqrt{|E|},$$ from applying the Holder inequality.

Hence, using Claim \ref{lemLogSumExpBound} $$\log(\frac{Z(\theta^{*0})}{Z(\thetaEst^{(0)})}) - \log(\frac{Z(\theta^{*1})}{Z(\thetaEst^{(1)})}) = O(\norm{\theta - \theta^*}_2 \sqrt{|E|}).$$

$$\theta_0 - \theta_0^* = \log(\frac{q}{\hat{q}}) - \log(\frac{1-q}{1-\hat{q}}) + \log(\frac{Z(\theta^{*0})}{Z(\thetaEst^{(0)})}) - \log(\frac{Z(\theta^{*1})}{Z(\thetaEst^{(1)})}),$$
where $q = Pr(Y=1)$ and $\hat{q}$ is its estimate using $n$ samples. Using Chernoff bounds, we know that $|q - \hat{q}| = O(\frac{1}{\sqrt{n}})$ with probability at least $1-p_3$. So, we claim that $$|\log(\frac{q}{\hat{q}}) - \log(\frac{1-q}{1-\hat{q}})| = O(\frac{1}{\sqrt{n}}).$$

Hence,  for some constant $k_2$:
$$|\theta_0 - \theta_0^*| \leq k_2 \sqrt{|E|}\sqrt{min(p, |\edge|)}\sqrt{\frac{d \log p}{n}}$$.

Applying these bounds to discriminant functions described by Equation \ref{eqnDiscriminant}, we get 
$$|f(x, \theta^*) - f(x, \theta)| \leq k \sqrt{|\edge|} \sqrt{min(p, |\edge|)}\sqrt{\frac{d \log p}{n}}.$$

Applying Claim \ref{lemDisagreement}, we have the result.
\end{proof}

\begin{remark}
Consider Theorem \ref{thmMarginAnalysisGenerative}. To achieve misclassification rate which is at most $p_1 + p_2 + p_3$ greater than the best achievable misclassification rate, number of samples required is given by:
$$n = O(|E|\min(|E|, p) \frac{d \log p}{\mu^2}).$$

Note that $p_1$ and $p_3$ decrease exponentially with $n$, so it suffices to focus on controlling $p_2$ and $\mu$. In particular, if we impose conditions on $\theta^{*i}$ such that $\mu = \Omega(|E|)$, we have:
$$n = O(d \log p).$$
\end{remark}



\subsection{Technical theorems}
\begin{claim}
\label{lemDisagreement}
Suppose that a family of binary classifiers is defined by $sgn(f(x))$, where $f:R^{p} \to R$ and $sgn(x)$ is the sign function. Consider two classifiers $sgn(f(x))$ and $sgn(g(x))$. 

Then, if $Pr_x(|f(x)| < \mu ) \leq p_1$ and $Pr_x(|g(x) - f(x)| \geq \mu) \leq p_2$. Then, $Pr_x(sgn(f(x)) \neq sgn(g(x))) \leq p_1 + p_2$.
\end{claim}

\begin{claim}
\label{lemLogSumExpBound}
 If $|a_i - b_i| \leq \eps$, then $\log(\frac{\sum_i exp(a_i)}{\sum_i exp(b_i)}) \leq O(\eps)$.
\end{claim}
\begin{proof}
 Suppose that $|a_i - b_i| \leq \eps$. Then, $|exp(a_i) - exp(b_i)| \leq exp(a_i)(1 - exp(\eps)) = exp(a_i)O(\eps)$ using the McLaurin series for $exp(\eps)$.

$\frac{\sum_i exp(a_i)}{\sum_i exp(b_i)} \leq 1 + (\frac{|\sum_i exp(a_i) - \sum_i exp(b_i)|}{\sum_i exp(b_i)}) \leq 1 + (\frac{\sum_i exp(a_i)}{\sum_i exp(b_i)}) O(\eps)$.

So, $\frac{\sum_i exp(a_i)}{\sum_i exp(b_i)} \leq \frac{1}{1 - O(\eps)} \leq 1 + O(\eps)$.

So, $\log(\frac{\sum_i exp(a_i)}{\sum_i exp(b_i)}) \leq \log(1 + O(\eps)) \leq O(\eps)$, using the McLaurin series for $\log(1+x)$.
\end{proof}


\bibliographystyle{plain}
\bibliography{pgmc}
\end{document}