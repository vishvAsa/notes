% This file was created with JabRef 2.5.
% Encoding: UTF8

@ARTICLE{Chow68approximatingdiscrete,
  author = {Chow, C. and Liu, C.},
  title = {Approximating discrete probability distributions with dependence
	trees},
  journal = {Information Theory, IEEE Transactions on},
  year = {1968},
  volume = {14},
  pages = {462--467},
  number = {3},
  abstract = {A method is presented to approximate optimally an<tex>n</tex>-dimensional
	discrete probability distribution by a product of second-order distributions,
	or the distribution of the first-order tree dependence. The problem
	is to find an optimum set of<tex>n - 1</tex>first order dependence
	relationship among the<tex>n</tex>variables. It is shown that the
	procedure derived in this paper yields an approximation of a minimum
	difference in information. It is further shown that when this procedure
	is applied to empirical observations from an unknown distribution
	of tree dependence, the procedure is the maximum-likelihood estimate
	of the distribution.},
  citeulike-article-id = {500644},
  citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1054142},
  keywords = {graphical-models},
  posted-at = {2006-02-10 16:25:10},
  priority = {3},
  url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1054142}
}

@ARTICLE{dahindenContingency09,
  author = {Dahinden, Corinne and Kalisch, Markus and B\"{u}hlmann, Peter},
  title = {Decomposition and Model Selection for Large Contingency Tables},
  journal = {Biometrical Journal},
  year = {2010},
  volume = {52},
  pages = {233--252},
  number = {2},
  month = {March},
  abstract = {Large contingency tables summarizing categorical variables arise in
	many areas. One example is in biology, where large numbers of biomarkers
	are cross-tabulated according to their discrete expression level.
	Interactions of the variables are of great interest and are generally
	studied with log-linear models. The structure of a log-linear model
	can be visually represented by a graph from which the conditional
	independence structure can then be easily read off. However, since
	the number of parameters in a saturated model grows exponentially
	in the number of variables, this generally comes with a heavy computational
	burden. Even if we restrict ourselves to models of lower-order interactions
	or other sparse structures, we are faced with the problem of a large
	number of cells which play the role of sample size. This is in sharp
	contrast to high-dimensional regression or classification procedures
	because, in addition to a high-dimensional parameter, we also have
	to deal with the analogue of a huge sample size. Furthermore, high-dimensional
	tables naturally feature a large number of sampling zeros which often
	leads to the nonexistence of the maximum likelihood estimate. We
	therefore present a decomposition approach, where we first divide
	the problem into several lower-dimensional problems and then combine
	these to form a global solution. Our methodology is computationally
	feasible for log-linear interaction models with many categorical
	variables each or some of them having many levels. We demonstrate
	the proposed method on simulated data and apply it to a bio-medical
	problem in cancer research.},
  address = {Seminar f\"{u}r Statistik, ETH Z\"{u}rich, CH-8092 Z\"{u}rich, Switzerland;
	Competence Center for Systems Physiology and Metabolic Diseases,
	ETH Z\"{u}rich, CH-8093 Z\"{u}rich, Switzerland},
  citeulike-article-id = {7017464},
  citeulike-linkout-0 = {http://dx.doi.org/10.1002/bimj.200900083},
  citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/20213739},
  citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=20213739},
  citeulike-linkout-3 = {http://www3.interscience.wiley.com/cgi-bin/abstract/123314344/ABSTRACT},
  day = {8},
  doi = {10.1002/bimj.200900083},
  issn = {1521-4036},
  keywords = {contingency-table, model-selection},
  owner = {vvasuki},
  posted-at = {2010-04-14 03:35:11},
  priority = {2},
  timestamp = {2010.08.02},
  url = {http://dx.doi.org/10.1002/bimj.200900083}
}

@ARTICLE{meierRS2008,
  author = {Meier, Lukas and van de Geer, Sara and Buhlmann, Peter},
  title = {The group lasso for logistic regression},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year = {2008},
  volume = {70},
  pages = {53--71},
  number = {1},
  month = {February},
  abstract = {Summary. The group lasso is an extension of the lasso to do variable
	selection on (predefined) groups of variables in linear regression
	models. The estimates have the attractive property of being invariant
	under groupwise orthogonal reparameterizations. We extend the group
	lasso to logistic regression models and present an efficient algorithm,
	that is especially suitable for high dimensional problems, which
	can also be applied to generalized linear models to solve the corresponding
	convex optimization problem. The group lasso estimator for logistic
	regression is shown to be statistically consistent even if the number
	of predictors is much larger than sample size but with sparse true
	underlying structure. We further use a two-stage procedure which
	aims for sparser models than the group lasso, leading to improved
	prediction performance for some cases. Moreover, owing to the two-stage
	nature, the estimates can be constructed to be hierarchical. The
	methods are used on simulated and real data sets about splice site
	detection in DNA sequences.},
  citeulike-article-id = {2209456},
  citeulike-linkout-0 = {http://dx.doi.org/10.1111/j.1467-9868.2007.00627.x},
  citeulike-linkout-1 = {http://www.ingentaconnect.com/content/bpl/rssb/2008/00000070/00000001/art00005},
  citeulike-linkout-2 = {http://www3.interscience.wiley.com/cgi-bin/abstract/119418556/ABSTRACT},
  doi = {10.1111/j.1467-9868.2007.00627.x},
  issn = {1369-7412},
  keywords = {group, lasso},
  owner = {vvasuki},
  posted-at = {2010-01-11 17:53:27},
  priority = {2},
  publisher = {Blackwell Publishing},
  timestamp = {2010.08.04},
  url = {http://dx.doi.org/10.1111/j.1467-9868.2007.00627.x}
}

@ARTICLE{ObozinskiWJ08arxiv,
  author = {Obozinski, Guillaume and Wainwright, Martin J. and Jordan, Michael
	I.},
  title = {Union support recovery in high-dimensional multivariate regression},
  year = {2008},
  month = {Aug},
  abstract = {In the problem of multivariate regression, a K-dimensional response
	vector is regressed upon a common set of p covariates, with a p by
	K matrix B* of regression coefficients. We study the behavior of
	the group Lasso using l1/l2 regularization for the union support
	problem, meaning that the set of s rows for which B* is non-zero
	is recovered exactly. Studying this problem under high-dimensional
	scaling, we show that the group Lasso recovers the exact row pattern
	with high probability over the random design and noise for scalings
	of (n,p,s) such that the sample complexity parameter given by theta(n,p,s)
	:= n/[2 psi(B*)log(p-s)] exceeds a critical threshold. Here n is
	the sample size, p is the ambient dimension of the regression model,
	s is the number of non-zero rows, and psi(B*) is a sparsity-overlap
	function that measures a combination of the sparsities and overlaps
	of the K-regression coefficient vectors that constitute the model.
	This sparsity-overlap function reveals that, if the design is uncorrelated
	on the active rows, block l1/l2 regularization for multivariate regression
	never harms performance relative to an ordinary Lasso approach, and
	can yield substantial improvements in sample complexity (up to a
	factor of K) when the regression vectors are suitably orthogonal.
	For more general designs, it is possible for the ordinary Lasso to
	outperform the group Lasso. We complement our analysis with simulations
	that demonstrate the sharpness of our theoretical results, even for
	relatively small problems.},
  archiveprefix = {arXiv},
  citeulike-article-id = {3094370},
  citeulike-linkout-0 = {http://arxiv.org/abs/0808.0711},
  citeulike-linkout-1 = {http://arxiv.org/pdf/0808.0711},
  day = {5},
  eprint = {0808.0711},
  keywords = {arxiv, high\_dimensionality, lasso, regression},
  posted-at = {2008-08-07 09:09:43},
  priority = {2},
  url = {http://arxiv.org/abs/0808.0711}
}

@UNPUBLISHED{pradeepPrivate,
  author = {Pradeep Ravikumar},
  note = {Private communication},
  year = {2010}
}

@UNPUBLISHED{pradeepDropbox,
  author = {Pradeep Ravikumar},
  note = {Learning Discrete Graphical Models [Dropbox note]},
  year = {2010}
}

@ARTICLE{ravikumarIsing09,
  author = {Pradeep Ravikumar and M. J. Wainwright and J. Lafferty.},
  title = {High-dimensional Ising model selection using l1-regularized logistic
	regression},
  journal = {Annals of Statistics},
  year = {2009},
  owner = {vvasuki},
  timestamp = {2010.08.02}
}

@OTHER{schmidtSoftware,
  author = {Mark Schmidt},
  month = jun,
  note = {http://www.cs.ubc.ca/~schmidtm/},
  title = {Graphical Models Software},
  url = {http://www.cs.ubc.ca/~schmidtm/},
  year = {2010}
}

@INPROCEEDINGS{schmidtPQN,
  author = {M. Schmidt and E. van den Berg and M. Friedlander and K. Murphy},
  title = {Optimizing Costly Functions with Simple Constraints: A Limited-Memory
	Projected Quasi-Newton Algorithm},
  year = {2009},
  optyear = {2009}
}

@Unpublished{sparsityStructureAlgorithmsReport,
author = {Vishvas Vasuki},
title = {Learning Discrete Graphical Models using L1/L2 Regularized Logistic Regression},
note = {Project report prepared for the course 'Sparsity, structure and algorithms.'},
}
