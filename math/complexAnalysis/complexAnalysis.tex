\documentclass[oneside, article]{memoir}

\input{../packages}
\input{../packagesMemoir}
\input{../macros}

%opening
\title{Analysis of functions over fields: Quick reference}
\author{vishvAs vAsuki}

\begin{document}
\maketitle
\tableofcontents

See \cite{rudin76ana}.

\part{Introduction}


\chapter{Themes}
Investigating C, functions of complex numbers.

\chapter{Important Fields}
\section{General properties}
For algebraic properties etc.., see algebra ref.

For Open balls, interior points, connectedness etc.. See topology ref.

\section{Numbers}
Natural numbers. Integers.

\subsection{Supremum (GUB) vs infimum (GLB)}
See properties of ordered sets in algebra ref.

\subsection{The field of Rational numbers}
Q; irrationality of $\sqrt{2}$; $\sqrt{2}$ as limit of rationals.

Q is an ordered field. Q is countable, as $Z^{2}$ is countable.

\subsubsection{Absence of least upper bound}
$\set{x \in Q, x^{2}< 2}$ does not have supremum in Q.

\section{The field of Real numbers}
\subsection{Supremum property}
R is a field containing Q, with the supremum property. Every $S \subset R$ has supremum, infemum.

\subsection{As Cuts from Q}
Set elements of R as (Dedekind) cuts: $a \subset Q$ which split Q into 2 parts in any way such that: if $x \in a$, any y: $x > y \in a$, it has some upper bound in Q, it has no max element. Eg: $\set{x \in Q, x^{2}< 2}$. Define $a<b$ by $a \subset b$, $a + b$ as $\set{u + v| u \in a, v \in b}$ etc; map $q \in Q$ to $(-\infty, q)$.

\subsection{Archimedian property of R}
If $0<x<y$, $\exists n \in N : y<nx$. Else, take LUB l of $\set{nx}$, so $\exists n: l-x<nx$, then $l<(n+1)x$: $\contra$.

\subsection{Misc Properties}
Decimal representation; its weakness: $.\bar{9} = 1$. There is a real between any 2 rationals. So: There is a rational between any 2 reals: also by archimedian property.

There is a real number between any two rationals: scale $\sqrt{2}$ as needed.

\subsection{Extended real number system}
$R \union \set{\infty, -\infty}$: not a field.

\subsection{Irrationals}
Irrational numbers. Algebraic numbers: roots of polynomials with rational/ integer coefficients. Transcendental numbers: non algebraic; eg: $e, \pi$. R dense in irrationals: Rationals are countable, but R is not. R dense in transcendentals: algebraic numbers countable.

\subsection{Topological properties k cell in \htext{$R^k$}{..}}
Closed interval in R; Extend to hypercuboid in $R^{k}$. Also see topology ref.

\subsubsection{Nonemptiness of telescoping \htext{$\inters$}{intersection} of cells}
If $\set{I_{n} = [a_{n}, b_{n}]}$ closed intervals in R with $I_{n} \supset I_{n+1}$ (as if diminishing), $\inters_{j} I_{j} \neq \nullSet$: take $E = \set{a_{n}}$; E bounded above by every $b_{n}$; so has sup E = x; this is in $\inters I_{n}$.

\subsubsection{Compactness of k-cells in \htext{$R^k$}{..}}
Take k-cell K definted by k closed intervals. K compact: For $\contra$, take open cover $\set{G_{i}}$ sans finite subcover. Halve these intervals to break it into $2^{k}$ k-cells. If K is not compact: atleast one of these pieces, $K_{1}$ is not compact. Repeat the procedure with $K_{1}$ recursively ad infinitum. But, by theorem about diminishing intervals $I_{n}$, $\exists p \in \inters I_{n}$; as p interior pt of $G_{i}$,  $\exists r, i: N_{r}(p) \supset K_{i}$: so $K_{i}$ has finite cover: $\contra $.

So, Bounded and closed intervals in R are compact.

\subsubsection{Compactness in \htext{$R^k$}{..}}
S closed and bounded $\equiv$ S Compact $\equiv$ C: Every $\infty$ subset E of S has a limit point of S: If S closed and bounded, it is in some k-cell, so compact. If S compact, C is true. If C true: S is closed by defunction. If C true: S is bounded: else $\contra$: $\set{x_{n}: |x_{n} - x_{0}| > n}$ wouldn't have lim pt in S.

So, compact set in $R$ has supremum in it.

\subsection{Uncountability of R}
$\infty$ sequences of $\set{0, 1}$ uncountable by Cantor's diagonalization. So, [0, 1] uncountable. So, looking at decimal representations, R is uncountable.

If $P \subset R^{k}, P \neq \nullSet$, P closed, every p in P is a limit point of P, then P uncountable: By a sort of diagonalization: As P has limit pts, P is $\infty$. For $\contra$ let sequence $\seq{x_{i}} = P$; take $V_{1} = N_{r}(x_{1}) \inters P \neq \nullSet$; take $cl(V_{1})$: is compact; make $V_{2} =  N_{r}(p) \inters V_{1} \neq \nullSet$ with $x_{1} \notin V_{2}$; do recursively; $\inters cl(V_{i}) \neq \nullSet$, but $\inters cl(V_{i}) \inters P = \nullSet : \contra$.

\subsubsection{Cantor set}
Take $E_{0} = [0, 1]$; remove middle third; do this recursively; $P = \inters E_{i}$. P is bounded and closed: so compact; also every pt is a lt pt: so uncountable; has no interval!

P has 0 measure. \why

\subsubsection{Cardinality of \htext{$R^{k}$}{..} is same as R}
Take $p = (a_{1}, .. a_{k}) \in R^{k}$, represent it in binary system; map it to x such that $x_{1:k}$ has first bits of p, $x_{k+1:2k}$ has second bits of p, etc..

\section{The field of Complex numbers C}
Set of ordered pairs over R, with +, * redifined to form a field: more than the complex plane $R^{2}$. An algebraic closure of R due to fundamental theorem of algebra.

\subsection{Spherical form}
$z=a + ib = r (\cos \theta + i \sin \theta)$.

Use Taylor series around 0: $\sin x = x-\frac{x^{3}}{3!}+\frac{x^{5}}{5!} \dots = \frac{e^{xi}-e^{-xi}}{2i}$; $\cos x = 1-\frac{x^{2}}{2!}+\frac{x^{4}}{4!} \dots = \frac{e^{xi}+e^{-xi}}{2}$. So, $(\cos \theta + i \sin \theta) = e^{i\theta}$.

So, $z = re^{i\gth}, y = r'e^{i\ga}$ related by $y = \frac{r'}{r}e^{i(\ga-\gth)}$.

\subsection{Powers of z}
(De Moivre). So, (Also by induction) $[r (cos \theta + i sin \theta)]^{n} = r^{n} e^{ni\theta} = r^{n} (\cos n\theta + i \sin n\theta)$. So, $\bar{z^{n}} = \bar{z}^{n}$.

\subsection{Other Properties}
Magnitude $|z| = \sqrt{a^{2} + b^{2}}$. $\sqrt{a+ib} = e+ih$. C has multiplicative inverse: $\frac{1}{a+ib}=\frac{a-ib}{a^{2}+b^{2}}$ (Realify the denominator).

\subsection{nth roots of 1}
Divide the unit circle into $n$ sectors: Integer powers of $w=e^{2i\pi/n}$. $w^{-1}=w^{n-1}=e^{-2i\pi/n}$. As $1+w+ \dots + w^{n-1} = w(1+w+ \dots + w^{n-1})$, $(1+w+ \dots + w^{n-1}) = 0$: Also for any $w^{x}$. $(a+ib)^{n} = re^{ni\theta}$ and $(a-ib)^{n} = re^{-ni\theta}$ are complex conjugates like $(a+ib)$ and $(a-ib)$.

\section{Quaternions}
Can't turn $R^{n>2}$ into a field \chk. In quarternions, you loose commutativity. In Octanions, you loose associativity.

\part{Real valued Functions and their properties}
\chapter{Properties}
Because of the nature of their range, real valued functions can be characterized using some special features.

Also consider properties of functions over ordered semigroups described elsewhere.

\section{Topological properties}
Limits, continuity, smoothness, steepness. See topology ref.

\subsection{Limits}
See topology ref. Left and right handed limits. $\pm \infty$ as limits.

Limits of sums, products, quotients. Squeeze or pinching theorem: If $f(x) \leq g(x) \leq h(x)$, and if $\lim_{x \to a} f(x) = \lim_{x \to a} h(x) = b$, $\lim_{x \to a} g(x)=b$.

\subsubsection{f:F to F: Find limits}
Try Substitution, factorization, rationalization.

L'Hopital rule: If $lt_{x \to c}f(x) = lt_{x \to c}g(x) = 0, L = \lim_{x\to c}\frac{f(x)}{g(x)} = \frac{f(x)'}{g(x)'}$: from definitions or from generalized mean value thm: define f(c) = g(c) = 0, so derivatives exist around c; as $\exists L, g'(x) \neq 0$.

\subsubsection{Closed functional f}
All sublevel sets of $f$ are closed. Equivalently, the epigraph is closed. \why

Consider/ visualize the epigraph: If $f$ is continuous, dom(f) is closed, then $f$ is closed. Also, if $f$ is continuous, dom(f) is open, $f$ is closed iff $f$ converges to $\infty$ along every sequence converging to bd(dom(f)). \why

\subsection{Continuity}
See topology ref.

If $f, g$ are continuous, then f+g, fg, f/g are continous.

\subsubsection{Absolute continuity of f:Rm to Rn}
More powerful/ specialized than uniform continuity.

$\forall \eps \geq 0, \exists d \geq 0: \forall $ finite sequence of pairwise disjoint sub-intervals $(x_k, y_k)$ : $\sum_{k} | y_k - x_k| < \delta \implies \sum_{k} | f(y_k) - f(x_k) | < \epsilon.$ This can be extended to $f:R^{m} \to X$ for any topological space X.
 

\subsubsection{Simple discontinuity}
Upper and lower limits exist, but different: $\floor{x}$. Non-simple disc: f(x): 1 if $x \in Q$, 0 else.

Fixing discontinuities. $\lim_{x \to 0} \frac{\sin x}{x} = 1$.

\subsubsection{Extreme value existence, boundedness}
(Weierstrass) If real valued $f$ is continuous over compact (closed and bounded in R) $S = [x_{1}, x_{2}]$, it attains maximum and minimum value somewhere in S.

\pf{As S compact, f(S) compact [See topology ref.]. So $f$ closed and bounded. By LUB property of R, $\sup $f$ = M$; take $d_{n}$: $M - n^{-1}\leq f(d_{n}) \leq M$; so $f(d_{n}) \to M$; by Bolzano Weierstrass take convergent subseq $(d_{n_{k}}) \to d$; $d \in S$ as S closed; as $f$ cont, so $f(d) = M$. }

If S not compact, there can be: unbounded but cont f: $S = (0,1), f(x) = x^{-1}$; cont $f$ without max: f(x) = $x$ on (0,1); cont but not uniformly cont: $S = (0,1), f(x) = x^{-1}$.

\subsubsection{Intermediate value theorem}
If continuous f(x):[a, b] $\to$ R , \\
$u \in [f(a), f(b)], \exists c \in [a, b] : f(c) = u$: [a,b] connected, so f([a,b]) connected.

\chapter{Definite-Integral function}
\section{Definitions}
For a fixed $f:R \to R$, the definite integral function is $R^{2} \to R$. In general, it is defined for any $f$ over a measurable space.

Simply see the definition of min-cover integrals defined elsewhere: those form superior notions of integration than what follows.

\subsection{Integral as least Upper sum, greatest lower sum}
Aka Reimann Integral.

\subsubsection{Partitions of [a,b]}
Any such partition $P$ is specified by a set of points $a = x_{0} \leq x_{1} .. x_{n} = b; \gD x_{i} = x_{i}- x_{i-1}$.

\paragraph{Refinement}
$P'$ is a refinement of $P$ if $P \subseteq P'$. Common refinement of $P_{1}, P_{2}: P_{1} \union P_{2}$.

\subsubsection{Upper, lower sums}
Take bounded function \\
$f: [a,b] \to R; M_{i} = \sup \set{f(x)|x \in [x_{i-1}, x_{i}]}; m_{i} = \inf\set{..}$; visualize; upper, lower sums $U(P, f) = \sum_{i=1}^{n}M_{i}\gD x_{i}; L(P, f) = \sum_{i=1}^{n}m_{i}\gD x_{i}; \\
\bar{\int_{a}^{b}}f(x) dx = \inf_{P} \set{U(P, f)}, \underline{\int_{a}^{b}}f(x) dx = \sup_{P} \set{L(P, f)}$: both exist as both $L$ and $U$ sums are bounded and real.

\subsubsection{Integrability}
If $L=U$, $f \in \mathcal{R}$, that is: $f$ is Reimann integrable.

\subsubsection{Limitations}
Not Reimann integrable: $I_Q(x)$ (Indicator function for rational numbers): $U=1$, while $L=0$ as every partition will contain a rational and an irrational number. However, this is box integrable: the corresponding box Integral is 0.

It is but a special case of superior notions such as the Stieltjes integral and the min-cover integral.

\subsection{Min-cover integral of measurable functions}
Aka Lebesgue integral, box integral. Take a real valued function $f$ over measure space $(X, S, m)$. Suppose $f:X \to R$ is a measurable function from $(X, S, m) \to (R, S_r, m_r)$, where $S_r$ is the sigma algebra of open sets in $R$, and $m_r$ is the common box measure on R.

For $f(x) \geq 0 \forall x$,  for any $E \in S$, $\int_E f(x) dm$ is the min-cover measure of portion of the space $E_f$ in $(X \times R)$ bounded by $E$ and $f(x)$. This measure was described for the general case in the product measure portion of the algebra survey. There is the additional restriction that the each $B_i = (B_{iX} \times B_{iR}) \in (S \times S_r)$ used to cover $E_i \subseteq E_f $ is such that for any $(x, f(x)) \in E_i $, its measure along $R$, $m_r(B_{iR}) \geq m_r(f(x))$: that is, they can be visualized as vertical boxes - with $R$ being the vertical axis.

This can be extended to any function $f$. Let $f^+ = 2^{-1}(|f(x)| + f(x))$ and $f^- = 2^{-1}(|f(x)| - f(x))$ be the positive and negative parts of $f$, so that $f = f^+ - f^-$. Then, $\int_E f(x) dm = \int_E f^+(x) dm - \int_E f^-(x) dm$.

\subsubsection{Importance}
It is superior to Reimann integral. The indicator function over rationals $I_Q(x)$ is not reimann integrable, but it is box integrable as the measure $m(Q)=0$.

\subsection{Min-cover integral wrt to non decreasing g}
Aka Lebesgue-Stieltjes integral or Radon integral.

This is defined as the min-cover integral obtained using a measure $m_g$ which corresponds to $g$.

\subsubsection{Upper and lower sums view}
Aka Stieltjes integral. Take $g:[a,b] \to R$ nondecreasing. $\gD g_{i} = g(x_{i}) - g(x_{i-1})$. $f$ bounded. Define $U(P, f, g) = \sum_{i=1}^{n}M_{i}\gD g_{i}$, L(P, f, g), $\bar{\int_{a}^{b}}f(x) dg, \underline{\int}$.

If $L=U$, $f \in R'(g)$: Stieltjes integrable.

For $\int$ to exist, $f$ and $g$ must not share any points of discontinuity: See how g is used to deal with discontinuity in $f$.

\subsubsection{Advantages}
Suppose $f$ is $[x \in Q]$ in a certain interval X, $f \in R(g)$ where g(x) = constant over X.

If $f$ bounded on $[a, b]$, has finite discontinuities, g conts at these points, then $f \in R(g)$: take partition with $\del g_{i}$ small in these pts, where $|m_{i} - M_{i}|$ maybe big but bounded; thence show $U - L < \eps$.

\paragraph{Series as Stieltjes integral}
Derive g from unit step functions: $g(x) = \sum_{n=1}^{\infty} c_{n}I(x - s_{n})$, with $c_{n} \geq 0, \sum c_{n}$ convergent. Then $\int_{a}^{b}f(x) dg = \sum c_{n}f(s_{n})$ : From $\int f d(g_{1} + g_{2}) = \int f d(g_{1})+ \int f d(g_{2})$. $\int_{a}^{b}1 dg = \sum c_{n}$.

\section{Importance}
For a fixed $f, m$, the integral can be viewed as a signed measure over $X$. This measure is significant as it captures and extends the notion of area/ volume etc.; and as described in another chapter, it happens to be closely related to the gradient of the function.

\section{Basic properties}
\subsection{Integral sandwiched between L and U}
If $P \subseteq P'$, $L(P, f, g) \leq L(P', f, g), U(P', f, g) \leq U(P, f, g)$: see geometrically. So, $\bar{\int_{a}^{b}}f(x) dg \leq \underline{\int}$: take common refinement of of $P_{1}, P_{2}: P'$, see $L(P_{1}, f, g) \leq L(P', f, g) \leq U(P', f, g) \leq U(P_{2}, f, g)$.

$f \in R(g)$ $\equiv$ $ \forall \eps: \exists P: U(P, f, g) - L(P,f,g) < \eps$. $\gets$ Pf: $L \leq \underline{\int} \leq \bar{\int} \leq U$. $\to$ Pf: As $\underline{\int} = \bar{\int}$, take $L(P_{1}), U(P_{2})$ close to these, take common refinement.

\subsubsection{Closeness to any sum between L and U}
So, if $s_{i}, t_{i} \in [x_{i-1}, x_{i}], \sum |f(s_{i}) - f(t_{i})| \gD g_{i} \leq \eps$. Also, $|\sum f(t_{i}) \gD g_{i} - \int_{a}^{b}f dg| < \eps$: both are bounded between L and U.

\subsection{Continuity implies integrability}
If $f$ continuous on $[a, b]$, $f \in R(g)$ on $[a, b]$.

\pf{As $[a, b]$ compact, $f$ uniformly cont; so $\forall \eta \exists d: |x-t|\leq d \implies |f(x) - f(t)| < \eta$; take $\eta:  (g(b) - g(a)) \eta \leq \eps$; take any partition P with $\gD x_{i} \leq d$; so $U(P, f, g) - L(P, f, g) = \sum (M_{i} - m_{i}) \gD g_{i} \leq \eps$.}

\subsection{Relationship with Uniform convergence}
Take $f_{n}$ on $[a, b]$, $f_{n} \to f$ uniformly.

If $f_{n} \in R, f \in R$: $lt_{n \to \infty} \int f_{n} dx = \int f dx$ as $\int (f_{n} - \eps_{n}) dx \leq \underline{\int} f dx \leq \bar{\int} f dx \leq \int (f_{n} + \eps_{n})dx$.

So, if $\sum f_{n} \to g$ uniformly, $\sum \int f_{n} dx = \int g dx$.

\subsection{Continuity of semi-definite integral}
If $f \in \mathcal{R}$ on $[a, b]$: $F(x) = \int_{a}^{x}f(t)dt$ is continuous: $f$ is bounded; so $F(y) - F(x) \leq M (y-x) < \eps$ as $y \to x$.

\section{Connection to derivative}
\subsection{Differential of integral over f}
If $f$ is continuous at $c$: : $F(x) = \int_{a}^{x}f(t)dt$ is differentiable at $c$, $F'(c) = f(c)$.

\pf{Continuity of f states that $t-c < d \implies |f(t) - f(c)| < \eps$. So $|\frac{F(t) - F(c)}{t-c} - f(c)| = |\frac{1}{t-c}||\int_{c}^{t}f(u)du - f(c)(t-c)| = |\frac{1}{t-c}||\int_{c}^{t}(f(u)-f(c))du| \leq \eps$.}

\subsection{Definite integral = difference in antiderivative}
\subsubsection{Anti-derivative}
If $f \in R$, and if $\exists F$ differentiable on $[a, b]$ with $F' = f$, then $F$ is called the antiderivative or the indefinite integral of $f$.

For any $f$, if there is an antiderivative $F$, $\set{F(x) + k | k \in R}$ is also an antiderivative. \exclaim{So, there are either 0 or $\infty$ antiderivatives.}

\subsubsection{Fundamental theorem of calculus}
$\int_{a}^{b} f(x)dx = F(b) - F(a)$.

\pf{Pick a partition $P = (x_i)$ with $U - L < \eps$. Apply Mean Value Thm to get: $F(x_{i}) - F(x_{i-1}) = f(t_{i})\gD x_{i}$; add all such terms to get $F(b) - F(a) = \sum f(t_{i})\gD x_{i} = \int_{a}^{b} f(x)dx + \eps$ by a property seen under Stieltjes Integral.}

\subsection{Inter-measure Derivative}
Aka Radon-Nikodym derivative. The min-cover integral may be used to define a derivative which connects two measures on the same measurable space: this is described in the chapter on measures in the Algebra survey.

\section{Integration: evaluating definite integrals}
Methods relevant (eg: transformation to polar coordinates) only for evaluating definite integrals of functions over product spaces are described elsewhere.

Also see section on important integrals for examples of application of these techniques.

\subsection{Integration by parts}
As definite integral equals the difference in the anti-derivative, $\int_{a}^{b}f(x)H(x)dx + \int_{a}^{b}F(x)h(x)dx =\\
 \int_{a}^{b}f(x)h(x)dx = F(b)H(b) - F(a)H(a)$.

Thus, $\int_{[a, b]} u dv = uv]_{[a, b]} - \int_{[a, b]} v du$ is a useful rule when RHS is easier to integrate.

\subsection{Sum, product, decomposition rules}
Due to definite integral being equal to difference in the antiderivative: $\int_{a}^{b} f(x) + h(x) dg = \int_{a}^{b} f dg + \int_{a}^{b} h dg$. $\int_{a}^{b} f dg = \int_{a}^{c} f dg + \int_{c}^{b} f dg$.

$\int f d(g_{1} + g_{2}) = \int f d(g_{1})+ \int f d(g_{2})$.

\subsection{Transform the integral}
Suppose you want to evaluate the definite integral I. Often, evaluating kI or $I^2$ (as in the case of the Gaussian integral) is simpler. Thence one can deduce the value of I.

\subsection{Change of variables}
\subsubsection{Function composition: integrability}
If $f \in R(g), f(x) \in [m, M]$, h continuous over [m, M], then $j(x) = h(f(x)) \in R(g)$. 

\pf{Take $\forall \eta \exists P: U(P, f, g) - L(P, f, g) = \sum (M_{i} - m_{i}) \gD g_{i} = \del (g(b) - g(a)) \leq \eta$. (Supposing $M_i - m_i \leq \gD$.)

$h$ uniformly continuous over $[m, M]$, so $(h(M_{i}) - h(m_{i})) \leq \eps$ for some $\eps$.

Hence, $\forall \eta' \exists P: U(P, j, g) - L(P, j, g) = \sum (M_{i}^{*} - m_{i}^{*}) \gD g_{i} = \eps (g(b) - g(a)) \leq \frac{\eta \eps}{\del} = \eta'$.}

So, if $f, g \in R(g): fg = 4^{-1}((f+g)^{2} - (f-g)^{2}), |f(x)| \in R(g)$. Also, $\int f(x) dg \leq \int |f(x)| dg$.

\subsubsection{Integration}
Take strictly increasing onto $h:[A, B] \to [a, b]$. Suppose that $f \in R$ on $[a, b]$.

Then $g = f \comp h \in R(h)$ on $[A, B]$. Then $\int_{x \in [A, B]} g(x) dh = \int_{t \in [h(A), h(B)]} f(t) dh = \int_{x \in [A, B]} f(h(x)) h'(x)dx $.

\core{Take a partition $P$ with upper and lower sums $L, U$. Use mean value theorem to rewrite in terms of $x$.}

\paragraph{Utility}
Change of variables is in general a extremely useful algebraic trick - so is this particular instance. Suppose $x = h(y)$. With this, one can write: $\int_{[A, B]} f(x) dx = \int_{y \in [h^{-1}(A), h^{-1}(B)]} f(h(y)) \frac{dx}{dy}dy $, as the RHS may be easier to evaluate.

\subsection{Reversing limits}
By convention $\int_a^bf(x)dx = - \int_b^af(x)dx$.


\section{Over product measure spaces}
Consider $f(x) \geq 0 \forall x$. (The integral for the general case may thence be derived as earlier.)

Let $(X, S, m)$ be the product measure space of $\set{(X_i, S_i, m_i) : i \in {1, 2}}$. Let $E \in S = S_1 \times S_2$: Note that this includes only rectangles in $R^{2}$, but not circles etc.. \\$E_1 = \set{x_1 \in X_1: \exists x = (x_1, x_2) \in E}$. Let $E_2(x_1)$ be similarly defined.

Then, from the properties of the product measure: (Fubini) $\int_E f(x) dm = \int_{E_1} g(x_1) dm_1$, where $g(x_1) = \int_{x_2 \in E_2} f(x_1, x_2) d(m_2)$.

\pf{Let $R$ be associated with the usual measure space $(R, S_r, m_r)$. (Core intuition) The min cover measure of the space in $(X \times R)$ bounded by $E$ and $f$ is $inf_{\set{B_i}} (m \times m_r)(\union_i B_i)$, where $B_i = (B_{iX_1}\times B_{iX_2}  \times B_{iR}) \in (S \times S_r)$ cover $E$. Because we are dealing with the product measure, $m_1 m_2 m_r(B_i) = m_1(B_{iX_1})m_2 m_1(B_{iX_2} \times B_{iR})$.}

Thus, integration over the product measure space reduces to integrating over one variable at a time; and the order in which these integrals are taken does not matter in this case. This case is aka multiple integration.

\subsection{Bounds}
If $f \leq h, \int f dg \leq \int h dg$. If $|f(x)| < M \implies |\int_{a}^{b} f dg| \leq M (g(b) - g(a))$.

\subsection{Use polar coordinates}
Consider $\int_{a, b} f(x, y) dx dy$, one can find an equivalent expression in polar coordinates using $x = r \sin \gth, y = r \cos \gth$ and $dxdy = dr (r d\gth)$. This is sometimes simpler to solve.

As in the case of the Gaussian integral, the form $\int_{a, b} f(x, y) dx dy$ may be derived from considering $I^2$.

\section{Important integrals}
\subsection{Normal integral}
Aka Gaussian integral.
$\int_{-\infty}^{\infty}e^{-x^{2}}dx = I = \sqrt{\pi}$: \\
\pf{$I^{2} =  \int_{y=-\infty}^{y=\infty}\int_{x=-\infty}^{x=\infty}e^{-x^{2}-y^{2}}dxdy $; thence transform to polar coordinates and solve.}

\subsubsection{Importance}
The Gaussian integral itself is important due to this nice integrability and the nice properties observed in the Gaussian distribution (very suitable for modeling). Further, it is useful because realated integrals can be used to smooth functions for various purposes (eg: optimization).

In calculating similar integrals, one often uses similar techniques and results about exponential and gamma probability densities.

\part{R to R functions}
\chapter{Superclasses}
Properties of the more general class of real valued functions is described in another part.

Properties of the more general class of functionals over vector spaces are described in the vector spaces survey.

\chapter{Functions over N: Sequences, series, convergence}
\section{Convergent sequences \htext{$(s_{n}), (t_{n})$}{..} in a field}
Also see Topology ref, properties noted in lin alg ref.

Sums, products, quotients (sans 0 denominator) of convergent sequences are convergent.

\subsection{Boundedness}
Upper and lower bounds.

\subsection{Montonicity}
Monotone increasing and decreasing sequences.

A monotone increasing sequence that is bounded above must converge: imagine points on real line; similarly monotone decreasing sequence that is bounded below.

(Bolzano, Weierstrass): Every bounded sequence has a subsequence which converges: let M be the bound; either [-M, 0] or [0, M] has $\infty$ elements; so use this repeatedly to find monotone, bounded subsequence; this converges. Also holds for $R^{n}$.

\subsection{Upper and lower limits}
Take set of subsequential limits E. Upper limit of $s_{n}$: $s^{*} = \limsup_{n \to \infty}s_{n} := \sup E$; similarly lower limit: $s_{*} = \liminf_{n \to \infty}s_{n}$.

If $s_{n} \to s, s^{*} = s_{*} = s$. E is closed, so $s_{*}, s^{*} \in E$.

\section{Generating function of a sequence}
Take sequence $(a_i)$. Get generating function: $G(x; a_n) = \sum_{n=0}^{\infty}a_n x^{n}$.

\section{Series}
$\infty$ series. A sequence of partial sums: $s_{n} = \sum_{i=1}^{n} a_{i}$.

\subsection{Convergence}
Many ideas from convergence of sequences.

\subsection{Convergence tests}
$s_{n}$ converges $\equiv$ it is cauchy. Aka completeness property of R or C. Cauchy criteria for convergence of $s_{n}$: $\forall \eps, \exists N: |s_{N}-s_{N+n}|< \eps$. So, $|a_{n}| \to 0$.

Sum or product of absolutely convergent series is absolutely convergent: limit of new series is sum or product of limits (Cauchy product).

Comparison test: If $|a_{n}| \leq c_{n}$: if $\sum c_{n}$ converges, $\sum a_{n}$ converges; if $\sum |a_{n}|$ diverges, $\sum c_{n}$ diverges: using Cauchy criterion.

If $a_{k} \geq a_{k+1}\geq 0, s_{n} = \sum a_{n}$ converges $\equiv$ $t_{n} = \sum_{k=0}^{n} 2^{k}a_{2^{k}}$ converges: $s_{n}$ monotonically increasing, so check boundedness: $\frac{t_{k}}{2}\geq s_{n}\leq t_{k}$ for $n\leq 2^{k}$.

\subsubsection{Ratio test}
Converges if $\limsup |\frac{a_{n+1}}{a_{n}}| < b< 1$: bound by geometric series $b^{k}a_{n}$; diverges if $>1: |a_{n}| \notto 0$.

\subsubsection{Root test}
Take $x = \limsup |a_{n}|^{1/n}$: So $|a_{n}| \leq x^{n}$. So, convergence if $x<1$, divergence if $x>1$; No info otherwise: $\sum 1/n \diverge$, but $\sum 1/n^{2} \to c$.

More powerful than ratio test: \\
$\liminf |\frac{a_{n+1}}{a_{n}}| \leq \liminf |a_{n}|^{1/n} \leq \limsup |a_{n}|^{1/n} \leq \limsup |\frac{a_{n+1}}{a_{n}}|$. \why

\subsubsection{Alternating series test}
If $|c_{i}| \geq |c_{i+1}|; c_{2k+1} \geq 0, c_{2k} \leq 0, \lim c_{n} = 0$, then $\sum c_{n}$ converges \why. So, $\frac{(-1)^{n}}{n}$ converges.

\subsection{Conditional convergence}
Absolute ($\sum |a_{i}|$) vs conditional ($\sum a_{i}$) convergence. Absolute convergence $\implies$ conditional convergence: ratio test.

For conditional convergent series: There are $\infty$ +ve and -ve numbers which converge to $+\infty$ and $-\infty$: else contradicts absence of absolute convergence or presence of conditional convergence.

\subsection{Absolute convergence}
All $a_{n} \geq 0$. $s_{n}$ monotone: so converges $\equiv$ it is bounded.

\subsection{Rearrangement}
Take $\sum a_{n}$ conditionally but not absolutely convergent: so $|a_{n}| \to 0$, $x \leq y$. $\exists$ rearrangement of terms: $s_{n}' = \sum a_{k}'$, with $\liminf s_{n}' = x, \limsup s_{n}' = y$: take +ve $\set{p_{n}}$, -ve $\set{q_n}$: both diverge, collect enough $p_{n}$ to go just over y; collect enough -ve numbers to go just under x; repeat: visualize as displacement between 2 points in R. So, can converge to any real number r. Eg: Rearrange terms of alternating harmonic series.

If $\sum a_{n}$ converges absolutely, all rearrangements converge to same limit: Any $s_{n}' = \sum a_{k}'$ is in some $s_N$ and vice versa.

\subsection{Series product}
$\sum a_{n}, \sum b_{n}: c_{n} = \sum_{k=0}^{n}a_{k}b_{n-k}$ is the product: put all ordered sequences $(a_{i}, b_{i})$ on a line: the diagonals of $a_{i} vs b_{i}$ matrix are $(c_{n})$.

If $\sum |a_{n}|$ converges, $\sum a_{n} \to A, \sum b_{n} \to B, \sum c_{n} \to AB$: Take $D_n = B_n - B$, $C_{n} = \sum_{k=0}^{n}a_{i}B_{n-i} = \sum_{k=0}^{n}a_{i}(B + D_n) = A_{n}B + \gamma \to AB$.

\section{Special series}
\subsection{Arithmatic series}
Arithmatic series: $S_n = \sum_n Tn: T_n = a + nd$. Reverse the series, add respective terms, get: $2S_n = n(a+nd)$

\subsection{Geometric-like series}
$T_n = ar^{n}$. Multiply by r, subtract terms, get: $(1 - r)S_n = a - ar^{n+1}$. Convergence as $n \to infty$: if $r < 1$. Similar techniques for $a+2ar+3ar^{2} \dots$.

\subsection{Harmonic-like series}
General harmonic series has: $T_n = (a + nd)^{-1}$.

$\sum 1/n $ diverges: $1 + 1/2 + 1/3 + 1/4 ..$: $\sum_{n=3}^{4} 1/n,  \sum_{n=5}^{9} 1/n$ etc exceed 1/2. Harmonic number: $H(n)\approx \ln n = \int x^{-1} dx$.

$\sum 1/n^{p}$ converges if $p>1$, diverges if $p \leq 1$: check convergence of $\sum \frac{2^{k}}{2^{kp}}$.

$\sum_{n=2}^{\infty} \frac{1}{n (\log n)^{p}}$ converges $\equiv$ $p>1$.

\subsection{Power series}
$z\in C: \sum c_{n}z^{n}$. If this converges, got Generating Function!

Take $a = \limsup_{n \to \infty} |c_{n}|^{1/n}, R = 1/a$: convergence if $|z|<R$ (radius of convergence), divergence if $|z|>R$: From Root test, ratio test.

If R = 1, $c_{i} \geq c_{i+1} \geq 0$, then $\sum c_{i}z^{n}$ converges $\forall z$, except maybe z = 1. \why

\subsection{Binomial theorem}
For $(x+y)^{n} : n\in N$: from combinatorics. For $(1+x)^{n} : n\in R$: Solve for coefficients in $(1+x)^{n} = 1 + nx + .. $ by differentiating repeatedly.

\subsection{e}
$e = \lim_{n \to \infty} (1+n^{-1})^{n} = \lim_{n \to \infty} = 1 + 1 + \frac{1}{2!} .. $. $2 < e < 1 + \sum_{n=0}^{\infty} (\frac{1}{2})^{n} = 3$, and e is an increasing sequence: so it converges.

Observe relationship with definition using natural log in another section.

\section{Summation tricks}
\subsection{Summation by parts}
Like integration by parts. Seq $(a_{n}), (b_{n}),  A_{n} = \sum a_{n}$. If $p \in [0, q], a_{n} = A_{n} - A_{n-1}, \sum_{n=p}^{q} a_{n}b_{n} = \sum_{n=p}^{q-1} A_{n}(b_{n}-b_{n+1}) + A_{q}b_{q} - A_{p-1}b_{p}$.

If $(A_{n})$ bounded seq, $b_{0} \geq b_{1} .., \lim_{n \to \infty} b_{n} = 0, \sum b_{n}a_{n}$ converges: Can use summation by parts to show Cauchy criterion.

\subsection{Using binomial summation formula}
$\sum k = \sum \binom{k}{1} = \binom{k+1}{2}$. Similarly, any polynomial $\sum a_{i}k^{i} = \sum b_{i}\binom{k}{i}$.

\subsection{Derivative to estimate limit of partial sums}
$(1-x)^{-1} = \sum_{i=0}^{\infty} x^{i}$; take derivatives to get: $(1-x)^{-2} = \sum_{i=0}^{\infty} ix^{i-1} = \sum_{i=0}^{\infty} (i-1)x^{i-1} - x^{i-1}$. Use similar trick to find: $i^{2}x^{i}$.

\subsection{Integral to estimate limit of partial sums}
Also, use $\sum n \approx \int x dx$.

The series $-\log (1 - x) = \int(\frac{1}{1-x}) = \sum_{i=1}^{\infty} x^{i}/i$; also from McLaurin series.


\chapter{Differential function}
\section{Definition}
\subsection{Rate of change}
The differential function, aka derivative, is \\
$f'(x) = \frac{df}{dx}= \lim_{\change x \to 0} \frac{f(x+\change x) - f(x)}{\change x}$, if the limit exists.

\subsection{Linear approximation view}
Hence, $f(x + \change x) = f(x) + \change(x) f'(x)$, as $\change x \to 0$.
As the primary use of the differential function is to be able to make linear/ polynomial approximations to $f$, one can view the differntial function as a measure of $f(x + \change x) - f(x)$ as $\change x \to 0$.

\subsection{Other views and generalizations}
See the derivatives of general functionals and functions in the vector spaces survey.

\section{Existence: Differentiability}
The the above limit exists at a certain point $x$, $f$ is said to be differentiable at that point.

$f$ can be differentiable on $(a, b)$ but not on endpoints of $[a, b]$.

\subsection{Relationship with continuity}
If $f$ differentiable, $f$ continuous; but not reverse: consider $|x|$.

Also, $f = \sum_{n=0}^{\infty} g(4^{n}x) (3/4)^{n}$, where $g(x) = g(x+2), g(x) = |x| on [-1, 1]$ is continuous but nowhere differentiable: Take $f_{n}= g(4^{n}x) (3/4)^{n}$; by Weierstrass M test, $\sum f_{n}$ converges to f; also converges uniformly; as the partial sums are continuous functions and as $\sum f_{n} \to f$, $f$ is continuous.

\subsection{Smoothness}
If for all $k$, $D^k(x)$ exists at a certain point $x$, $f$ is said to be smooth at that point.

\section{Differential operator}
\subsection{Definition}
Consider the operator $D$, which maps a given function $f$ to its differential function $f'$. Note that the differential function may have a smaller domain than $f$.

\subsubsection{Notation}
So, $D(f)(x) = f'(x)$. Often, this is written as $Df(x)$.

Below, represent the scalar functions $f$ and $D(f)$ as vectors in a vector spaces: see vector spaces and functional analysis surveys for details. Also, $f+g, fg, f/g$ etc.. are naturally defined.

\subsubsection{Higher order differentials}
These are defined by $D^k(x) = D(D^{k-1}(x))$ $: \forall k > 1, k \in N$.

Other notations include: $\frac{d^{k}f(x)}{dx^k}, f^{(k)}(x)$.

\subsection{Inverse}
Directly from the definition, $D(f^{-1}) = \frac{1}{D(f)}$.

\subsection{Linearity}
$D(f + g) = D(f) + D(g)$. This follows from the definition of the differential function.

Hence, the differential operator $D()$ is a linear map between these vector spaces. It is a matrix.

\subsection{Other properties}
If $f, g: R \to R$ differentiable at a certain point $x$, using :
$D(fg) = f D(g) + D(f)g$. This follows from the definition of the differential function. Similarly, $D(f/g) = D(f)g - fD(g))/g^2$.

\subsection{Composition (chain rule)}
Suppose $g, h$ are functions. $D(g(h)) = D(h)D_h(g)$, where $D_h(g)$ is the differential function of $g$ evaluated at $h(x)$. This is proved using linear approximations $f(x + \change x) = f(x) + f'(x)\change x$ and the definition of $D(f)$.

\subsection{Parametrically defined functions}
Suppose $y = f(t)$ and $x = g(t)$. Then, $\frac{dy}{dx} = \frac{f'(t)}{g'(t)}$; as $y = f(g^{-1}(x))$.

\section{Differentiation}
Differentiation is the procedure of evaluating the differential operator for a certain function.

\subsection{Differentiation tricks}
$y=(f(x)^{g(x)})'$: take log on both sides, differentiate. Chain rule.

\subsection{Differentials of important functions}
Ab initio differentiation of sin x, cos x.

\subsubsection{Differentiation of powers and exponents}
$x^{n} :n \in N$ can be derived from the definition. Thus, using the expansion/ definition of $e^{x}$, $D(e^{x})(x) = e^{x}$.

$\forall n\in R: \der x^{n} = \der e^{n\ln x} = nx^{n-1}$.

\section{Other properties}
\subsection{Geometry}
Geometry is described for the case of general functionals in the vector spaces survey.

\subsection{Connection with extrema}
If $f$ differentiable, $f'$ is $0$ here (minimum / maximum singularity). See optimization ref.

\subsection{Effect of uniform convergence}
Take $f_{n}$ on $[a, b]$, $f_{n} \to f$ uniformly.

$f_{n} = \sin nx/\sqrt{n} \to 0$, but $f'(x) \notto 0$.

If $f_{n}'$ continuous, uniformly convergent, and if $\exists c \in (a, b): f_{n}(c)$ converges (This condition ensures ye're not adding constants to $f_{n}$ to keep $f_{n}'$ convergent but $f_{n}$ non convergent); then $\exists f: lt_{n \to \infty} f_{n} = f, f$ is differentiable, $\lim f_{n}' = f'$. Pf: Let $\lim  f_{n}' = g$; From fundamental theorem of calculus: $\int_{c}^{x}f_{n}'(t)dt = f_{n}(x) - f_{n}(c)$; take limits on both sides, get: $\int_{c}^{x} g(x)dt = f(x) -f(c)$; g continuous as $f_{n}'$ continuous, uniformly convergent to g; so use fundamental theorem of calculus again.

\section{Polynomial approximation of f}
\subsection{f(x): Mean change and the gradient}
\subsubsection{Interior extremum existance}
(Rolle) If $f$ is continuous and \\
differentiable in $[x_{1}, x_{2}], f(x_{1}) = f(x_{2}) = 0, \exists c \in (x_{1}, x_{2}): f'(c) =0$. 

Easy to make a visual argument.

\pf{There exists atleast one maximum and one minimum in [a, b]; if it happens to be in the interior set $(a, b)$, $f'(x) = 0$ at this point; otherwise $f$ is a constant function, and there is still an extremum in $(a, b)$.} 

\subsubsection{Mean value theorem}
If $f$ is continuous and differentiable in \\
$[x_{1}, x_{2}], \exists c \in (x_{1}, x_{2}): f'(c) = \frac{f(x_{1})- f(x_{2})}{x_{1} - x_{2}}$. Easy to visualize. For proof, see the generalization to two functions below.

\exclaim{Thence, linear approximation to $f$!}

\subsubsection{Relative to another function}
If $f, g$ continuous and differentiable: $(f(b) - f(a)) g'(x) = (g(b) - g(a)) f'(x)$: Make new function, apply Rolle. Aka Cauchy's mean value theorem.

\pf{Suppose that $f(b) = f(a) + M(g(b)- g(a))$. Now, solve for $M$. Take $F(x) = f(x) - f(a) - M (g(x) - g(a))$. $F(a) = F(b) = 0$; so because of the Interior extremum existance argument, there must exist some $c \in [a, b]$ with $F'(c) = 0$.}

\subsubsection{Definite integral view and the mean}
$\int_{[a, b]} f'(x) dx = f(b) - f(a) = f'(c) (b-a)$ for some $c \in [a, b]$. This can be extended to integration $\int_{[a, b]} f'(x) dg(x)$ wrt another function $g(x)$ using the mean value theorem relative to another function.

As integration can be viewed as an extension of summation, $\frac{\int_{[a, b]} f'(x) dx}{(b-a)} = f'(c)$ can be viewed as the mean of $f'(x)$.

\subsection{Polynomial approximation}
Aka Taylor theorem. $P(a)\dfn \sum_{k=0}^{n-1}f^{(k)}(a) (b-a)^{k}/k!$; then $f(b) = P(a) + R$, where $R = f^{(n)}(c)(b-a)^{n}/n! = \int_{[a, b]} f^{(n)}(y)(b-y)^{n-1}/(n-1)! dy$ for some $c \in [a, b]$.

Can then bound error term by bounding $f^{(n)}(c)$.

\pf{We want to find $f(b) - P(a)$. Note that $P(b) = f(b)$, and $P'(x) = \sum_{k \geq 0} (- f^{(k)}(x)/((k-1)!) + f^{(k+1)}(x)(b-x)^{k}/(k!)) = f^{(n)}(x) (b-x)^{n-1}/(n-1)!$: note how P was cleverly defined around $b$ rather than $a$ to let this happen.

From mean value theorem wrt function $g$, we get $f(b) - P(a) = P(b) - P(a) = \frac{f^{(n)}(c) (b-c)^{n-1}/(n-1)!}{g'(c)}(g(b) - g(a))$. Then, using alternatively $g(x) = (b-x)^{n}$ and $g(x) = \int_{[a, x]} f^{(n)}(y)(b-y)^{n-1}dy$, we get the stated remainders.} 

\subsection{Associated series}
In Pf, note that, in general, $c_i$ getting closer and closer to $a$ as $n$ increases; but we cannot be sure about it: for all we know, all $c_i$ may be very close to $b$.

The polynomial approximation series, aka Taylor series, is the polynomial approximation $P(x)$ as the degree $n \to \inf$.

McLaurin series: Taylor series about 0.

\subsection{Importance}
Polynomial approximation of functions described above is very important in analyzing the solutions to many problems. This is because one can use the nth degree approximation and upper bounds on $|f^{(n)}(x)|$ to get easy to analyze upper and lower bounds on $f(x)$.

For example, this is used to prove that solutions to certain optimization problems which arise in doing maximum likelihood estimation have desirable properties. Also, several optimization algorithms work by minimizing the polynomial (quadratic in the case of Newton's method) approximation to $f(x)$, and this analysis is naturally used there.

\section{Extensions}
Can't easily extend to general metric spaces by using a distance function $d()$: rates of change won't be negative.

\subsection{Extension to functionals}
See vector spaces ref.

\chapter{Important functions}
\section{Classification by dependence on parameters}
See probability ref.

\section{Recurrance equations}
Eg: $a'h_{i+2}+b'h_{i+1}+c'h_{i}=0$ (Linear, homogenous). Get characteristic eqn by supposing $h_{i} = tx^{i}$; Get roots and multiplicity: $(r_{1},2), (r_{2},1)$; Then, $h_{i} = ar_{1}^{i} + bir_{1}^{i} + cr_{2}^{i}$; Solve for a, b, c with boundary conditions. Or, use telescoping sum.

\section{Polynomial P over field K}
Polynomial over field K has coefficients from K. Rational function: ratio of polynomials. For $\perp$ polynomials, see Approx theory ref. p is monic: highest degree of x has coefficient 1.

To show that P has high degree, show it has many roots.

\subsection{Multivariate polynomials}
$Q(x_{1}, .. x_{n}) = Q(x)$, let $x_{-i} = $x sans $x_{i}$: Multivariate polynomial over field F; degree $d = \sum d(x_{i})$. Have $\infty$ roots: $x_{-i} = r_{-i}$, Q(x) = univariate $P(x_{i})$ has roots.

\subsection{Symmetric polynomials}
$Q(x_{1} \dots x_{n}) = Q(\pi(x_{1} \dots x_{n}))$ : no change on permutation. Eg: Parity function P. If Q is symmetric, multilinear $\implies$ Q writable as monomial $Q'(\sum x_{i})$ of same degree: intuitive b'coz vars are indistinct, (Find inductive proof).

\subsubsection{Probability of multivariate polynomials becoming 0}
(Schwartz Zippel). \\
Q(x); $S\subseteq F$; $Pr_{r \in S}(Q(r) = 0) \leq \frac{d}{|S|}$: generalizes univariate case: $P(x_{i})$ has d roots. True for d=0; assume for d=n-1; $Q(x) = \sum_{k} x^{k}Q_{k}(x_{-1})$; $Pr(Q(r) = 0) \leq Pr(Q(r_{-1}) = 0) + Pr(Q(r)=0 | Q(r_{-1}) \neq 0) \leq \frac{d-k}{|S|} + \frac{k}{|S|}$.

\subsection{Roots of polynomial P over R}
From group theory: P over the field Q. For $deg(P) \geq 5, \exists P: P(r)=0, r \in R$, r can't be writ in terms of +, -, *, /, kth root. \why

Fundamental theorem of algebra: P of degree d has d roots in C. \why

If x is root of P, so is $\bar{x}$: $P(x) = \sum a_{i}x^{i} = 0 = \conj{\sum a_{i}x^{i}} = \sum a_{i}\bar{x^{i}} = \sum a_{i}\bar{x}^{i} = P(\bar{x})$. So, if deg(P) is odd, atleast 1 real root exists: also from the fact that $P = \Theta(x^{n})$.

Every polynomial of odd degree (= $\Theta(x^{n})$) has a real root: Consider large +ve and -ve values for x and Intermediate value theorem. Also from the fact that complex roots appear in pairs.

Tricks: The constant gives clues about the root. Use the $\Theta$ notation to consider the roots: To solve $(\frac{e}{x})^{x}=n^{-2}$; get $x \log \frac{x}{e} = 2 \log n$; so $x \leq 2 \log n \leq x^{2}$; so $x = (\log n)^{\theta(1)}$.

Geometry: loci.

\subsubsection{Quadratic eqn}
Reduce to $(ax+b)^{2}=c$ and solve. So find roots of $x^{3}-1$ (Invent i). Similarly, every polynomial can be factorized to (sub)quadratics. Cubics.

\section{Important functions over R and C}
\subsection{Extrema}
$\min(a, b) = 2^{-1}(|a+b| - |a-b|)$.

\subsection{Unit step function I(x)}
$I(x) = [x >0]$. See connection with series and Stieltjes integrals.

Integer functions: $\floor{x}, \ceil{x}$.

\subsection{Logarithm and exponential}
$\ln x \dfn \int_{t=1}^{x} t^{-1}dt$. By fund theorem of calculus, $\der{\ln x} = x^{-1}$. So, $\ln(ab) = \ln a + \ln b$, $\ln x^{k} = k \ln x$.

$e \dfn x: \ln x = 1$. So, $\ln e^{x} = x$, $\der{e^{x}} = e^{x}$. The McLaurin series $e^{x}$.

\subsection{Generalized binomial coefficient}
$\binom{r}{k}$ for any $k\in Z, r\in R$: generalizes $\binom{r}{k}$ from combinatorics (See probability ref).

$\binom{n}{r} = \frac{n}{r}\binom{n-1}{r-1}$. For $r \in N$: $\binom{r}{k} = \binom{r}{r-k} = \frac{r}{r-k}\binom{r-1}{r-k-1} = \frac{r}{r-k}\binom{r-1}{k}: \therefore r\binom{r}{k} = (r-k)\binom{r}{k}$: Both sides are degree k polynomials in r, agree in $\infty$ points: so should be identically equal $\forall r\in R$.

So, $\binom{r}{k} = \binom{r-1}{k} + \binom{r-1}{k-1}$. Applying repeatedly, $n \in N: \sum_{k=0}^{n}\binom{r+k}{k} = \binom{r+n+1}{n}$.

By induction: $n,m \in N:\sum_{k=0}^{n} \binom{k}{m} = \binom{n+1}{m+1}$. Useful in summing series.

$\binom{r}{m}\binom{m}{k} = \binom{r}{k}\binom{r-k}{m-k}$: combinatorial proof for r, m, k integers; thence generalize to $r \in R$: use Polynomial interpolation argument.

Vandermonde convolution: $\sum_{k} \binom{r}{k}\binom{s}{n-k} = \binom{r+s}{n}$: combinatorial proof for integer case; generalize to $r \in R$: use Polynomial interpolation argument.

\subsubsection{Polynomial interpolation argument}
This is very useful in extending relationship between integers to all real numbers: as in proof of $r\binom{r}{k} = (r-k)\binom{r}{k}$.

\subsection{Logistic sigmoid function}
$f(z) = (1+e^{-z})^{-1}$: an $S$ trapped between $0$ and $1$. Used in logistic regression. $f'(z) = (1-f(z))f(z)$.

\subsection{Normal function}
Aka Gaussian function. Generalization of Gaussian distribution. \\
$f(x) = ae^{-\frac{(x-b)^{2}}{2c^{2}}}$: mean at b, max value a, variance/ width $c^{2}$. Using Gaussian integral $\int_{-\infty}^{\infty}e^{-x^{2}}dx = \sqrt{\pi}$.

\subsection{Gamma function}
$\gG(z) = \int_{0}^{\infty} x^{z-1}e^{-x}dx$. $\gG(1) = 1$. $\gG(a) = (a-1)\gG(a-1)$: using integration by parts; so generalizes factorial: $\gG(n) = (n-1)!$. Useful in specifying normalization constants in some distributions.


\part{Bounding techniques}
\chapter{Important limits and bounds}
$p^{1/n} \to 1$: take seq $x_{n} = p^{1/n}-1$. Also, $n^{1/n} \to 1$.

\section{Bounds on combinatorial quantities}
\subsection{Bounds on n!}
(Stirling)$n!\approx \sqrt{2n\pi} (\frac{n}{e})^{n}$. \why

\subsection{Bounds on (a choose b)}
$\binom{a}{b} = \frac{a!}{b!(a-b)!} \approx \frac{a^{a}}{\sqrt{2n\pi} b^{b}(a-b)^{a-b}}$ for large a, b and a-b.

So: $\binom{a}{b} \leq (\frac{ea}{b})^{b}$. $\lceil \frac{a}{b}\rceil \leq \frac{a-1}{b}+1$. $\frac{e^{m}}{\sqrt{em}}< \frac{m^{m}}{m!}\leq \sum \frac{m^{i}}{i!} = e^{m}$. $(\frac{a}{b})^{b} < \binom{a}{b} < a^{b}/b!$.

\section{Bounds on powers and exponents}
\subsection{Bounds on \htext{$(1+b)^{n}$ and $e^{n}$}{exponentials}}
Using Taylor series of $e^{-x}$ wrt 0, stopped at $x^{2}$: $e^{-x} \geq 1-x$.

$(1-x^{-1})^{x} \leq e^{-1} \leq (1-x^{-1})^{x-1}$: first inequality from taylor series argument. Latter ineq similarly proved for $x \in (0, 1)$ by showing $e \geq (1-x^{-1})^{1-x}$.

Also can get bounds using Taylor series; eg: $2^{-1}(e^{ac}+e^{-ac}) \leq e^{a^{2}c^{2}2^{-1}}$.

\subsection{Bounds on log x}
Using log series, or from concavity: $\ln x \leq x-1$; easy to visualize with log x curve under x - 1 line.

\section{Inequalities among means}
\subsection{Bound ab with a convex combination of powers}
Aka Young's inequality. $a, b \geq 0$; $p, q \in R^{+}$ with $p + q = 1$; then $ab \leq pa^{\frac{1}{p}} + qb^{q^{-1}}$. Special case of the inequality of weighted arithmetic and geometric means, but is described separately to illustrate the proof technique.

Pf: Let $a, b >0$. $\ln ab = \ln a + \ln b = p\ln a^{p^{-1}} + q\ln b^{q^{-1}}$. $\ln(x)$ is strictly concave function on $R^{+}$; thence use this to get inequality.

\subsection{AM GM HM inequality}
The various means are described in the statistics survey.

Suppose $a, b \geq 0$. Using the same technique (taking $\log(a^{p}b^{q})$ and taking advantage of concavity of $\log(x)$), we see that weighted GM $\leq$ weighted AM.

Also, weighted GM $\geq$ weighted HM: from applying the AM-GM inequality: $(a^p b^q)^{-1} \leq pa^{-1} + qb^{-1}$.

\chapter{Analysis/ bounding techniques}
Also see functional analysis ref.

Effectively bound one function with another.

\section{General ideas}
Use convexity of functions! See proof of AM/ GM inequalities, for example.

\subsection{Bound f over [a, b] with a constant c}
Show that $f$ is monotone over $[a, b]$, perhaps by taking derivative; take $c$ to be value at $a$ or $b$.

\section{Asymptotics}
Asymptotic notation: o, O, $\Theta, \Omega, \omega$.

\subsection{Hierarchy of growth of functions}
$c, \lg^{*} n, \log n, n^{k}, n^{\log n}, \\
n^{n^{1/3}} = e^{n^{1/3}\ln n}, e^{n}, n! = \theta(\sqrt{n}(\frac{n}{e})^{n}), n^{n}$.

\subsection{Iterative logarithm}
$\lg^{*} n$: 0 if $n\leq 1, 1+\lg^{*}\lg n $ otherwise. Very slow growing.


\bibliographystyle{plain}
\bibliography{complexAnalysis}

\end{document}
