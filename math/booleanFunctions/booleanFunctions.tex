\documentclass[oneside, article]{memoir}

\input{../packages}
\input{../packagesMemoir}
\input{../macros}

%opening
\title{Boolean Functions: Quick reference}
\author{vishvAs vAsuki}

\begin{document}
\maketitle

\tableofcontents

\part{Basic properties, normal forms}
\chapter{Notation and themes}
C: A function class.

\section{Themes}
The structure and power of various classes of boolean functions. The relationships between them.


\chapter{Function Definition}
\section{Boolean function}
$f:\set{0,1}^{n} \to \set{0, 1}$ or $\set{\pm1}^{n} \to \set{\pm 1}$. Ones and zeros: $I^{f} = \set{x: f(x)=1}$; $O^{f}$.

\subsection{Randomized boolean function}
See algebra ref. Maps hypercube points to RV's taking value 1 with some probability.

\section{Variables and their range}
\subsection{Changing input basis}
$x_{i} \to x_{i}'; 0 \to 1; 1 \to -1 : x_{i} = \frac{1- x_{i}'}{2}$.

For fixing polynomial for change in basis: $\set{\pm 1} \to \set{0,1} :x_{i} \to (\frac{1+x_{i}}{2})$.

\subsection{Variables and literals}
Consider $x \in \set{0, 1}^{n}$. Every $x_i$ is a variable, and $x_i, \bar{x_i} .. $ are literals.

\chapter{Properties}
\section{Dependence on input variables}
\subsection{r-junta}
f depends on only $r$ variables. k-junta has at most $2^k$ non 0 coefficients. Each of them is $2^{-k+1}$ heavy: Make two equally high stacks unequal.

Witness to the relevance of variable $x_{i}$: an assignment a which flips f(a) when ith bit is flipped.

\subsection{Monotone fn f}
Flipping $x_{i}$ from 1 to -1 cannot flip f(x) from -1 to 1. $(\forall i, x_{i} \leq y_{i}) \implies f(x_{1}, .. x_{n}) \leq f(y_{1}, .. y_{n})$.

Define $g(x_{i}, x_{-i}) = f(x)$ naturally. Then $g(-1,x_{-i}) = -g(1,x_{-i})$ iff \\
$g(-1,x_{-i}) = -1 \land g(1,x_{-i}) = 1$;\\ $E_{x_{-i}}[g(1,x_{-i}) - g(-1,x_{-i})] = 2 Pr_{x_{-i}}(g(1,x_{-i}) \\
= 1 \land g(-1,x_{-i}) = -1 ) $.

\subsubsection{Influence}
$\hat{f}(\set{i}) = 2^{-1}(E_{x_{-i}}[g(1,x_{-i})] - E_{x_{-i}}[g(-1,x_{-i})]) = I_{i}(f)$.

\subsubsection{Total influence}
As $\norm{x}_{1} \leq \sqrt{n}\norm{x}_{2}$: $I(f) = \sum_{i} \hat{f}(\set{i}) \leq \sqrt{n}\sum_{i} \hat{f}(\set{i})^{2} \leq \sqrt{n}$.

For majority: $I(f) \approx n(\frac{2}{(n-1)\pi})^{0.5}$.

\subsection{Fairness}
f is fair if it is 1 on exactly half the inputs.

\section{Combinatorial properties}
\subsection{Sensitivity to bit i}
Aka Influence of bit i. $x^{(i)}$: x with ith bit flipped. Sensitivity wrt bit i: $I_{i}(f) = Pr_{x}(f(x) \neq f(x^{(i)})$.

Total influence or avg sensitivity or Energy: $I(f) = \sum_{i}I_{i}(f)$.

\subsection{Noise sensitivity}
Aka noise stability. $y = N_{\eps}(x)$: \\
x with $\eps$ noise in each bit. $NS_{\eps}(f) = Pr_{x,y}(f(x) \neq f(y)) = 2 Pr_{x,y}(f(x) = 1 \land f(y) = -1)$ by symmetry of sign changing bit flips.

\section{Expressiveness measures}
Let $C$ be a class of boolean function. Viewing $c$ as the dichotomy it induces on a given set is profitable here.

\subsection{Count the number of f in C}
One can use $|C|$ or packing/ covering numbers. See the functional analysis survey for details.

\subsection{Dichotomy count fn}
Take S with $|S| = m$. $D_{C}(S)$: set of dichotomies on S induced by C; Max dichotomy count: $D_{C}(m) = \max_{|S|=m}|D_{C}(S)|$.

\subsubsection{Shattering and VCD d}
If $D_{C}(S) = 2^{m}$, it is said to be shattered by $C$. VCD(C) is the size of maximal set which is shattered by $C$.

\subsubsection{Bounding using the VCD}
If $m<=d$, $D_{C}(m) = \sum_{i=1}^{m}\binom{n}{i} = 2^{m} = O(m^{d})$. If $m > d$, we use Sauer's lemma. These bounds are important in quantifying the number of samples required to accurately estimate probabilities of a class of events (an example event: a classifier is wrong.). For details, see statistics and computational learning theory surveys.

\paragraph{Sauer's lemma}
(Sauer) If d = vcd(C), by induction and intermediate function, \\
$D_{C}(m) \leq \sum_{i=1}^{d} \binom{m}{i} \leq (\frac{em}{d})^{d} = O(m^{d})$.

Pf: Let $s = |X|$. By induction on s+d. \why Intuition: Can't produce some dichotomy on some d+1 set.


\subsection{Prove VCD(C) = d}
Show some d-sized set shattered by C (Lower bound); show that no $d+1$ sized set is shattered (Upper bound). Use geometric intuition.

\subsubsection{Lower bound}
Try making a matrix of points shattered, whose rows are bit vectors of the shattered points.

\subsubsection{Upper bound}
Find optimal shattering geometry, optimal strategy; Use adversarial labelling. Find $D_{C}(m)$, solve $D_{C}(m)<2^{d}$. If C finite, shatters S, $|C| \geq 2^{|S|}$, so $\log |C| \geq d$.

\subsection{VCD's of some R}
Intervals in R: 2. Halfspaces in $R^{n}$: n+1. Axis aligned rectangles: 4. d-gons in a plane: 2d+1. $VCD(C_{G}) \leq 2ds\log(es)$.

$VCD(C_{r,n}) \geq r(\log (n/r))$ if $C_{r, n}$ embedding closed, contains function f with exactly $r$ relevant vars. Take f. Key idea: A sample point $s_{j}$ is the witness of relevance of var j in f. Key idea: Make a matrix whose rows are points shattered by $C$. Let $k = \log (n/r)$. Build a $rk \times r2^k$ block matrix: 1 row block for every $s_{j}$. In row block j: every column block $i \neq j$ has same entry: $s_{j,i}$; but column block i has all separate entries from ${0,1}^k$. To select any set of points/ rows, select embedding of f into the right set of relevant vars.

So, for decision list of $r$ relevant vars: $r(\log (n/r))\leq d \leq $r$ \log n$.

\chapter{Views and normal forms}
\section{Views}
The truth table. Subset of the hypercube. The concept.

A set system: Can consider c as a set: set of 'selected' points, induces dichotomies on X.

Social (binary) choice or voting scheme of n people.

\section{Normal forms}
\subsection{Write as sign of multinomial of n variables}
$x \in \set{0, 1}^{n}$. Take DNF, change $\bar{x_{i}} \to (1-x_{i})$.

\subsection{k-DNF and k-CNF}
$k \leq n$. DNF is a disjunction of conjunctions. CNF is a conjunction of disjunctions.

\subsubsection{Derivation from truth tables.}
DNF $c$ for boolean function $f$ is obtainable from truth table of $f$: \\$\set{x: f(x) = 0}$.

CNF $d$ for boolean function $f$ is obtainable from truth table of f: get the CNF of $\bar{f}(x)$, negate it to get a $d$.

\subsubsection{Connection between DNF and CNF.}
Any term in $d$ and any clause in $c$ cannot be disjoint \why.

$t$ term CNF can be reduced to $t$-DNF using distributive laws.

t-DNF to t-term CNF is also possible, but this may take exponential time as $RP \neq NP$: See colt ref about hardness of proper learning k-DNF.

\subsubsection{Minimization of number of terms}
Drawing rectangles in Karnaugh / Veitch tables.

\paragraph*{Minterms and maxterms}
Minterms of a DNF are terms such that: Each var appears $\leq 1$ times. Maxterms of a CNF are similarly define. So, a minimal DNF is a sum of minterms, while the minimal CNF is a product of maxterms.

A minterm as $\set{\pm 1}^{n}$ fn: $\prod(\frac{1+x_{i}}{2})$. Constant term is coefficient of $p_{\phi} \dfn 1$.

\subsubsection{DNF norm}
In the uniform distr fn space, Mansour's conjecture: \\
$\exists$ polynomial p: $\norm{DNF}_{1} \leq O(n^{\frac{1}{\eps}}); \norm{f-p}^{2} \leq \eps$.

\subsection{Size s DNF f}
Aka s-term DNF. Some notation: Term-length t, terms $\{T_{i}\}$, s = size(f) = num of terms: maybe poly(n), n variables. 

Actual representation size $\leq ns\log n$.

$|k-DNF| \leq (2n)^{k}$: count the number of possible ways to make a k-DNF.

Number of functions: $O(\binom{3^{n}}{s}) = O(3^{ns})$. So, number of polysize DNF: $`3^{n^{O(\log n)}}$.

t-DNF reducible to disjunction upon feature expansion. With distributive laws, size s DNF reducible to s-CNF and thence to negation of s-DNF. Negation of size s DNF is size s CNF.

Strictly more expressive than decision lists: \why: so more powerful.

\subsubsection{DNF f to l-augmented Decision tree of rank \htext{$\leq \frac{2n}{l}\log s + 1$}{}}
Let p = terms of length $>l$, make most commonly occuring literal $x_{i}$ in the p terms (occurs atleast in $\frac{pl}{2n}$ by pigeon hole), make kids DNF's with $x_{i}$ set to 0 (make many terms disappear) and 1; repeat till all terms have lower term length: rank r(n, p) increases when r(kids): $r(n-1, p-\frac{pl}{2n})$ and r(n-1,p) are same; r(n,0) = r(0, p) = 0; solving recurrance, rank $\leq \frac{2n}{l}\log s + 1$.

\subsubsection{DNF f to PTF}
$sign(T_{1} \dots + T_{s} - 2^{-1})$. So, Any f needs $\leq n$ PTF: Any f is at most n-DNF.

Has PTF of degree $O(\sqrt{t} \log s)$: Let $T_{i} = x_{1}\dots x_{t}, a_{i}(\bar{x}) = \frac{x_{1} + \dots x_{t}}{t}$, Chebyshev pol (see Approx theory sheet) $C_{\sqrt{t}}((1+t^{-1})a_{i}(\bar{x})) = Q_{i}, deg(Q_{i}) = \sqrt{t}$; for $T_{i}(x)=1, |Q_{i}|\geq 2$, else $|Q_{i}| \leq 1$; PTF is $\sum Q_{i}^{\log s} - s - 2^{-1}$.

Has $O(n^{1/3} \log s)$ PTF: \\
Make $n^{2/3}$ augmented Decision tree of rank $2n^{1/3}\log s + 1$; change each $n^{2/3}$ DNF at leaf to $n^{1/3} \log s$ PTF; change Decision tree to $2n^{1/3}\log s + 2$ Decision list (k long, terms: $\{T'_{i}\}$) which output PTF's $\{P_{i}\}$; $\exists$ PTF with polynomial $2^{k}T'_{1}P_{1} + 2^{k-1}T'_{2}P_{2} \dots$ of degree $O(2n^{1/3}\log s + n^{1/3} \log s)$.

\subsubsection{Lower bounds on PTF degree}
f = parity fn: $\{0,1\}^{n} \to \pm 1$, an $O(2^{n})$ size DNF needs n PTF: Let sign(Q(x)) be parity PTF: $x_{i}^{2} = x^{i}$, so all polynomials (even Q(x)) multilinear; $Q(x) = Q(\pi(x))$; let $Q'(x) = \sum_{\pi}Q(\pi(x_{1} \dots x_{i}))$; Q' symmetric, sign(Q') computes parity, degree(Q') = degree(Q); $Q'(x) = Q''(\sum x_{i})$; $Q''(0)<0, Q''(1)>0, Q''(2)<0 \dots$; so Q'' has n roots; so Q', Q has degree n.

Some polysize DNF in n variables needs PTF of degree $n^{1/3}$: DNF f = '1 in a box' fn (Minsky, Papert): $s= m; T_{i} = \bigwedge_{j=1}^{4m^{2}}x_{i,j}$; let $x_{i}$ be variables in box $T_{i}$; let sign(Q(x)) be PTF for f; $Q'(x) = \sum_{\pi_{1,i} \dots \pi_{m,i'}} Q(\pi_{1,i}(x_{1}), \dots \pi_{m,i'}(x_{m}))$; sign(Q') also is f, degree(Q') same; $Q''(\sum x_{1,i}, \dots \sum x_{m, i})$ with same degree $Q''(x)>0$ iff $\sum x_{1,i} \geq 4m^{2}$; let $p(t) = Q''(4m^{2}-(t-1)^{2}, 4m^{2}-(t-3)^{2} \dots 4m^{2}-(t-(2m-1))^{2} )$; $deg(p) = 2 * deg(Q'')$; $p(1)>0, p(2)<0, p(3)>0 \dots$; $degree(p) \geq 2m$, so $deg(Q) \geq m$.

\subsection{Boolean circuits (ckt)}
A DAG with $n$ inputs: $C_{n}$; internal nodes represent logic gates/ operations; directed edges represent inputs and outputs to these nodes. Circuit basis: the gates: usually AND, OR. Bounded vs unbounded fanin. Size and depth of ckt.

Using De Morgan's laws, any ckt can be changed to have all NOT gates at input. Ckt with bounded fan in $k$ can be changed to have bounded fan in 2 with constant blowup in depth. Unbounded fanin ckt convertible to bounded fanin ckt with $\log(size(ckt))$ blowup in depth.

Turing machine halting in time $T(n)$ convertible to ckt of depth $T(n)$ and size $(T(n))^{2}$.

\subsubsection{Boolean formula f}
Aka Boolean expressions, tree like circuits. Subset of Boolean circuits: Only 1 output per gate. Can be rewritten as a 3 CNF.

\subsubsection{Balance tree-like circuits, reduce depth}
(Spira): Take tree O with n nodes, find subtree A with (2/3)n nodes; A could be T or F: create 2 trees BT, BF out of the other n/3 nodes with either A=T or A=F; Now, $O=(A \wedge BT) \vee (\sim A \wedge BF)$; keep repeating this for A, BF, BT to get tree of depth $log_{\frac{3}{2}}n$.

\subsection{Turing machines}
See Computational Complexity ref.

\chapter{Fourier analysis of boolean fn}
Fourier analysis of ANY real valued boolean fn. 

\section{The function space for Uniform distribution}
\subsection{Dimensionality}
Take the real space $R^{2^{n}}$: Not $\infty$ dimensional Hilbert space. Any real valued boolean function can be represented by a point in this space.

Can also be applied to randomized boolean functions: Alter expectations and probabilites to handle extra randomness.

\subsection{Inner product}
$\dprod{f,g} \dfn 2^{-n} \sum_{x} f(x)g(x) = E_{x \in_{U} \set{\pm 1}^{n}}[f(x)g(x)]$: defined thus scaled to make $\norm{p_{S}} = \norm{f} = 1$. If $S \neq T$: $\dprod{p_{S}, p_{T}} = 0$.

\subsubsection{The basis}
These form an alternate basis: Parity functions $p_{S}$, where S is the index vector.

The standard basis is just $\set{e_i}$.

\section{Transforms}
\subsection{Discrete Fourier series}
Discrete Fourier series for any real valued fn over $\set{\pm 1}^{n}$: $f = \sum_{S\subseteq [n]} \hat{f}(S)p_{S}$.

$\norm{f}^{2} = \sum_{S} \hat{f}(S)^{2} = E_{x}[f(x)^{2}] = 1$: [Parseval].

\subsection{Correlation / Discrete Fourier coefficient}
Aka Fourier weights on basis vectors or Fourier spectrum. $\hat{f}(S) = \dprod{f, p_{S}} = E_{x}[f(x)p_{S}(x)]= Pr_{x}(f(x) = p_S(x)) - Pr_{x}(p(x) \neq p_S(x))$: so, a measure of how good an approx of f $p_S$ is. $\dprod{f,g} = \sum_{S} \hat{f}(S) \hat{g}(S)$.

\subsection{Fourier transform of f}
$\hat{f}(S) = 2^{-n} \sum_{x} f(x)p_{S}(x)$. All $\hat{f}(S)$ can be computed in time $O(n2^{n})$ using FFT alg, given all f(x). \why Inverse FFT can be similarly done. Also see Functional analysis ref.

\section{Estimating coefficients}
\subsection{Estimate all Fourier coefficients of f using FFT}
$f_R$: A boolean fn on $\set{0,1}^m$: Take random m*n matrix, define $f_R(y) = f(yR)$. Fourier coefficients of $f_R$ are Fourier coefficients of f restricted to a random subspace.

For random non singular m*n R, with m large enough to give good estimation of $Pr(f(x) p_a(x) = 1)$ whp. Thence find approximations of all the Fourier coefficients of f by finding all coefficients of $f_R$ using FFT.

\subsection{Relationship with total influence}
$I(f) = \sum_{S}|S|\hat{f}(S)^{2}$. \why

\subsection{Relationship with noise sensitivity}
Eg: $NS_{\eps}(\Land_{i} x_{i}) = \frac{2}{2^{n}}(1-(1-\eps)^{n}) \approx 0$; For parity: $NS_{\eps}(p_{1^{n}}(x)) = 2^{-1}-2^{-1}(1-2 \eps)^{n} \approx 2^{-1}$; note correlation with good/ bad Fourier concentration.

$NS_{\eps}(f) = 2^{-1}-2^{-1} \sum_{S}(1-2\eps)^{|S|}\hat{f}(S)^{2}$:\\
$Pr_{x,y}(f(x) \neq f(y)) = 2^{-1} - 2^{-1}E_{x,y}[f(x)f(y)]$; \\
But $E_{x,y}[f(x)f(y)] = E_{x,y}[(\sum_{S}\hat{f}(S)p_{S}(x)) (\sum_{T}\hat{f}(T)p_{T}(x))] \\
= \sum_{S, T}\hat{f}(S)\hat{f}(T)E_{x,y}[p_{S}(x)p_{T}(y)] = \sum_{S}\hat{f}(S)\hat{f}(T)E_{x,y}[p_{S}(x)p_{S}(y)]$; but\\
 $E_{x,y}[p_{S}(x)p_{S}(y)] = \prod_{i} E[p_{\set{i}}(x)p_{\set{i}}(y)] = (1-2\eps)^{|S|}$.

$\sum_{|S| \geq \eps^{-1}} \hat{f}(S)^{2} \leq NS_{\eps}(f)$, k const: $2NS_{\eps}(f) = 1 - \sum_{S}(1-2\eps)^{|S|}\hat{f}(S)^{2} = \sum_{S} \hat{f}(S)^{2}-\sum_{S}(1-2\eps)^{|S|}\hat{f}(S)^{2} = \sum_{S} \hat{f}(S)^{2}(1-(1-2\eps)^{|S|}) \approx \sum_{S} \hat{f}(S)^{2}(1-e^{-2}) \geq k' \sum_{S} \hat{f}(S)^{2}$.

As, $\sum_{|S| \geq \eps^{-1}} \hat{f}(S)^{2} \leq NS_{\eps}(f)$, any f is $(NS_{\eps}(f), \eps^{-1})$ concentrated.

\subsection{Concentration in the spectrum}
Aka Fourier concentration.

\subsubsection{(eps, d) concentrated function f}
$\norm{\sum_{|S| >d} \hat{f}(S)p_{S}}_{2}^{2} \leq \eps$. Visualize with histogram.

If C = polysize DNF formulae, every $c\in C$ is $(\eps, \log |c| \log\frac{1}{\eps})$ concentrated. \why

Depth d ckts of size $|c|$: $(\eps,\log |c| \log\frac{1}{\eps})^{d-1}$ concentrated. \why

\subsection{Best fitting d degree polynomial}
$\norm{f-g}^{2} = \sum_{S} (\hat{f}(S)-\hat{g}(S))^{2} = E_{x}[(f(x)-g(x)))^{2}]$; \\
so $\min_{g} E_{x}[(f(x)-g(x))^{2}] = \sum_{|S|\leq d} \hat{f}(S)p_{S}$.

\subsection{t - sparse f}
Number of non zero coefficients in f $\leq t$.

\subsubsection{Sparse approximator for any f}
$\exists \frac{\norm{f}_{1}^{2}}{\eps}$ \\
sparse g with $\norm{f-g}^{2} \leq \eps$: Remove all $\hat{f}< \frac{\eps}{\norm{f}_{1}}$; then g is $\frac{\norm{f}_{1}^{2}}{\eps}$ sparse: $\norm{f}_{1} = \sum \hat{f}(S)$ and each $\hat{f}(S)$ has min size $\frac{\eps}{\norm{f}_{1}}$. $\norm{f-g}^{2} = \sum_{p_{S} \notin basis(g)} \hat{f}(S)^{2} \leq \\
(\max_{p_{S} \notin basis(g)}) \hat{f}(S) \sum_{p_{S} \notin basis(g)} \hat{f}(S) \leq \eps$.

So, sgn(g) approximates f.

\subsubsection{Weak parity learning problem}
Let f have a t-heavy Fourier component; then find a t/2 heavy Fourier component. Thence find weakly correlated parity.

KM alg solves this; See colt ref.


\part{Boolean function classes}
\chapter{Decision lists and trees}
\section{Decision lists}
Decision list is fully specified by a sequence of $k$ variables $(x_{i})$ and outputs $r(x_{i}), r'(x_k)$. It is like an 'if .. elseif .. elseif .. else' statement. It can be visualised as a chain of variables, each with one outgoing edges representing an output.

In a d-decision list, $d-CNF$'s are used in place of $\set{x_i}$.

\subsection{Generality}
This can be writ as halfspace: $sign(\sum 2^{k-i}x_{i}o(x_{i}))$.

We can write conjunctions, disjunctions as decision lists.

\section{Decision trees}
DAG with internal nodes labelled with variables; leaves are labelled $0$ or $1$ (range of the 'label' variable to be predicted). A decision tree is same as a nested 'if .. else ..' statement.

\subsection{Power}
They include decision lists, but are strictly more powerful: can represent $x_{1} \oplus x_{2}$ as a decision tree.

\subsection{Evaluation and interpretation}
Apart form the overall classification error, one can consider classification errors particular to various decision paths. The leaves of the decision trees, which represent a decision path can be labeled with the error rate observed for that particular path.

\subsection{t augmented Decision tree}
Decision tree with t-DNF's at leaves. t augmented Decision tree of rank $r$ reducible to t + $r$ + 1 decision list.

\subsection{Rank of decision trees}
Rank of a leaf variable is 1. Rank of decision tree (not max depth) = Max (ranks of kids) if kids have diff ranks; else rank of kid + 1. So, $rank \leq \log(nodes)$.

\subsection{Number of decision trees}
Number of Decision trees = $2^{2^{n}}$. Number of poly size Decision trees probably same as number of polysize DNF's: they're interchangeable \chk.

\subsection{Rank r decision tree to r+1 decision list}
Easy decision list for Rank 1 subtree; Take node x with kid subtrees $T_{1}$ and $T_{2}$, append $\bar{x}$ to conjunctions of DL of $T_{1}$ and x to conjunctions of DL of $T_{2}$, join them.

\subsection{Decision tree to polysize DNF f}
Taking $\lor$ of AND's corresponding to paths to leaves with label 1; only one of the terms can be true at a time. So, f = $\sum$ AND terms, with $\norm{AND(..)}_{1} \leq 1$. So, $\norm{f}_{1}\leq t$.

\subsection{Sparsity}
As $\norm{f}_{1}\leq t$, Decision tree with t leaves has approximator with sparsity $O(t)$.

\chapter{Other function classes}
\section{Conjunctions and disjunctions}
View as a set.

\section{Boolean functions from real valued functions}
Take epigraph or subgraph. See complex analysis ref.

\section{Halfspace}
Aka Linear Threshold fn (LTF). $f=sign(\sum a_{i}x_{i} + c) = sgn(a^{T}x + c)$, $a_{i}, c \in Z$. x called the weight vector, c called the bias.

$a^{T}x + c = 0$ is a Hyperplane: take pts x and x' on the hyperplane, use $a^{T}(x-x') = 0$; so a specifies orientation. Distance from origin : $\frac{a^{T}x}{\norm{a}} = \frac{-c}{\norm{a}}$. Distance of any pt x from hyperplane: $\frac{a^{T}x - (-c)}{\norm{a}} = \frac{f(x)}{\norm{a}}$ by geometry.

Weight of halfspace W = $\sum |a_{i}| + |c|$. Low weight LTF has W = poly(n).

\subsection{Intuition}
+1 on surface of one half of the unit sphere; -1 elsewhere; find orientation of halfspace. Like line in $R^{2}$.

\subsection{Noise sensitivity and Fourier concentration}
$NS_{a}(f) \leq \sqrt{a}$. \why So, every halfspace is $(a,a^{-2})$ concentrated. By union Bound, if F is fn of k LTF, $NS_{a}(F) \leq k \sqrt{a}$. So, F is $(a,k^{2}(a)^{-2})$ concentrated.

\subsection{Conversion to PTF}
$\exists$ Rational function $R(x) = \frac{p(x)}{q(x)}$ of degree $O(l\log t)$ like sign function for $|x| \in [1,2^{l}]$: ergo $\exists$ degree $O(l\log t)$ P(x)=p(x)q(x) doing the same. So, get O(log W) PTF.

So, Functions of $\set{f_{i}}$ W weight halfspaces: $\Land^{s} f_{i}$ has O(log W) PTF: Use P(x) on $\sum^{s} f_{i} - s$.

\subsection{Make origin centered halfspace}
Add an extra dimension: Map f to $f' = sgn(\sum_{i=1}^{n+1} a_{i}x_{i}): x_{n+1} = 1, a_{n+1} = c$.

\section{Automata}
See \cite{linz}.

\subsection{Grammar}
\subsubsection{Definition}
A grammar specifies a language by describing structural rules which can be used to accept or reject a sentence. So, each grammar is associated with a language.

It is fully described by (Variables, Terminal chars/ alphabet, Start symbol, Production rules).

Production rules are of the form:
string including variable V $\to$ string with variables and terminal symbols, formed by replacing $V$ with somwething else.

A given string is accepted if, starting with the start symbol, using a finite number of production rules, one can arrive at the string.

\subsubsection{Parsing connection}
The rules involved in generating a given string (sentence or word) is closely connected to the semantic meaning underlying that string. 

\subsubsection{Characteristics}
There are conflicting demands on grammar design/ development.

Due to the parsing connection, unambiguity is desirable.

A good grammar should define a language with which a wide variety of semantics can be expressed. So, expressiveness is desirable.

At the same time, if the resultant language is being used for communicaiton, brevity is desirable. 

\subsubsection{Context free grammars}
(CFG) constrain production rules so that their left hand side (LHS) has exactly one variable/ start symbol.

\subsubsection{Normal forms}
Chomsky normal form for CFG: only $A \rightarrow BC$ or $A \rightarrow \alpha$. Greibach normal form for CFG: only $A\rightarrow \alpha C$.

\subsubsection{Other traits}
Pumping Lemma for CFG (How could a sufficiently long string be produced by a CFG?); use in checking if language is CFG.

Syntax trees and ambiguity. 

\subsection{Regular expression}
This is a symbolic representation of a certain type of languages (defined by its syntax). This language consists of strings which may be produced by the application of production rules specified by the regular expression. THe production rules may also be viewed as producing a set of strings.

\subsubsection{Production rules}
Fundamental production rules include concatenation (cartesian product of string sets), alternation (union of k sets) represented by \verb |  , and the 'kleene star' represented by * (union with empty set of the concatenation-closure of a certain set).

\subsubsection{Grouping}
To clarify operator precedence, expressions are grouped together. Generally, groupings are specified by enclosing them within ().

\subsubsection{Example}
abc(def|hig)*uvw.

Also see programming languages' use of regular expressions for string search and for the associated richer syntax.

\subsection{Finite state automata}
This is a class of boolean functions over strings that can be formed using a finite alphabet.

\subsubsection{States and transitions}
Finite state automaton is a collection of states and transitions between the states. The states include special states such as exactly 1 start-state and $\geq 1$ accept-states.

Every transition between two states is labeled with a (input) character from the alphabet. Depending on the number of transitions corresponding to a given (state, letter) pair, the transition is said to be deterministic or non-deterministic.

\subsubsection{As a directed graph}
One can use a directed graph where nodes and edges correspond to states and transitions. The edges are labelled with appropriate input characters.

\subsubsection{Acceptance}
A given string is accepted or rejected based on whether one can reach a 'final-state' by making transitions appropriate to characters in the string.

\subsubsection{Resources}
Uses constant memory.

\subsubsection{Expressiveness}
Accept regular languages (describable by a regular expression). Pumping lemma for regex: Middle part of strings in regular languages can be pumped.

Strictly more powerful than decision trees. \why

\subsubsection{(Non) determinism}
Determinsitic (DFA) and non deterministic representations.

\subsection{Pushdown automata}
Determinsitic Push-down automata and stack memory. Non deterministic push-down automata; accepts context-free languages (use Greibach).

\section{Polynomial Threshold functions (d-PTF)}
Multivariate poly bool $f(..)=sign(p(..))$: degree $d = \sum degree(x_{i})$. Reduce to halfspace by feature expansion: make $n^{d}$ variables.

\section{Neural network}
G: Directed layered acyclic graph with s internal nodes of indegree r, n inputs, 1 output. $C_{G}$ (in $R^{n}$): G composition of C (in $R^{r}$); each node in G assigned $c \in C$. \textbf{Neural net}: $C_{G}$ with each $c \in C$: $r$ weights $w_{i}$, threshold $\theta$.

\section{Important functions}
\subsection{Parity function}
Important in error checking, Fourier analysis. \\
$p:\set{-1, 1}^{n} \to \set{-1, 1}$ better than $\set{0, 1}^{n}$ defn: $p_{S}(x) \dfn \prod_{x_{i} \in S} x_{i}$, $p_{\phi} \dfn 1$. Length of $p_S = |S|$.

Also writ as $p_{S}(x) : S = \set{0,1}^{n}$, S is indicator vector or index. Or as $p_{S}(x) = (-1)^{S^{T}x}$, where $S.x$ is the GF(2) inner product.

Also $GF_2$ form: $f = \sum_{i \in S} x_{i} \bmod 2$: can be writ as a bit vector. Assignment x are also bitvector; evaluation: $f(x) = \dprod{x, f}$.

$p_{S}p_{T} = p_{S\symdiff T}$. $p_{S}(x \xor y) = p_{S}(x)p_{S}(y)$ \chk.

\subsection{Majority function}
$maj(x) = sgn(\sum_{i} x_{i})$.

\subsubsection{Noise sensitivity}
$NS_{\eps}(maj) = \sqrt{\eps}$: Random walks on Z starting from 0 induced by noise and by x; whp total deviation due to noise is $\leq \sqrt{\eps n}$; after rand walk by x, prob of remaining within $\sqrt{n}$ of 0 is $\sqrt{\eps}$ by looking at lengths of line segments.

\tbc


\bibliographystyle{plain}
\bibliography{../computationalComplexity/computationalComplexity.bib}

\end{document}
