\documentclass[oneside, article]{memoir}
\input{../packages}
\input{../packagesMemoir}
\input{../macros}

%opening
\title{Graphs: Quick reference}
\author{vishvAs vAsuki}

\begin{document}
\maketitle
\tableofcontents

\part{Notation}

\part{Themes}
Discover better, more realistic graph generators. Discover more realistic algorithms for various tasks like finding shortest path.

\chapter{Characterization of research effort}
There is both experimental and theoretical work.

\part{Graphs and their basic properties}
\chapter{Graph G=(V,E)}
E annotated with weights. If G unweighted, all weights are 1.

\section{Vertex properties}
Degree of v: $deg(i) = \sum_{j} e_{i,j}$. Neighbors of v: $\nbd(v)$.

Measure size of $A \set V$: $|A|$ or $vol(A) = \sum_{i \in A} deg(i)$.

\section{Associated objects}
\subsection{Special vertex sets}
Node cover of G=(V,E): subset of V which touches all e in E. 

Proper vertex coloring; chromatic number.

Independent set of verteces: don't share edge: a clique in $\bar{G}$.

\subsection{Walks}
A sequence of edges $(e_i)$ such that $e_i$ shares an end-point with $e_{i+1}$.

A walk may have a cycle. If it does not have a cycle, you have a path.

Also see random walks.

Alternating walks are defined and studied in combinatorial optimization problems over graphs; in these sequences even and odd edges are colored differently.


\subsection{Cut}
k-way cut: $cut((V_{i}))$ is a partitioning of V into k parts. Cutsets is a set of edges which, when removed from G, divides V into k partitions.

\subsubsection{Weight of a cut}
Weight of a cut is the sum of weights of edges in the cutset.

Minimum 2-way cut is a cut with the minimum weight. This is useful in partitioning nodes in a graph.

\subsection{Subgraphs}
A general subgraph: G'=(V', E'). Subgraph induced by $A \set V$. Connected components of a graph. Clique: a complete subgraph.

\subsubsection{Spanning trees}
Spannning tree. MST spanning a certain node set N: Aka Steiner tree.
MST spanning with support over node set N: Aka Group Steiner tree.

\section{Subtypes}
Multigraphs: multiple edges allowed.

\subsection{Tree}
G sans cycle. Forest F: set of trees.

\subsection{Biconnected graph}
2 paths between any node pair.

\subsection{Based on degree}
d-regular G: $\forall v: |N(v)| = d$. Complete graph $K_{n}$.

\subsection{Perfect graph G}
Chromatic number = size of the largest clique in G.

A graph is perfect iff its complement is perfect.

\subsubsection{Bipartite graph}
E = cutset. Complete bipartite graph: All v in A has edge to all u in B. Complete bipartite graph $K_{i,j}$. Can have A vs B adj matrix M. Thence get usual adj matrix for G: $\mat{0& M\\M^{T} & 0}$.

\subsubsection{Chordal graph G}
Aka triangulated graph. Every cycle in G has a chord. G has a junction tree iff it is chordal.

\subsection{Planar graphs}
(Kuratowski) G planar iff $K_{5}$ and $K_{3,3}$ are in G.



\section{Directred graph, networks}
Digraph or directed graph. Networks: digraph with edge weights.

\subsection{Directed acyclic graphs (DAG)}
Very useful in designing algorithms: can do recursion easily.

\subsubsection{Topological numbering t of nodes}
Number nodes so that if there is a path from u to v, then $p(u)< p(v)$. Cyclic graphs don't have a topological numbering.

\subsection{Singly connected directed graph}
Useful in probabilistic graphical models.

A tree underlies the graph: only 1 undirected path between any node pair.

\section{Generalizations}
\subsection{Hypergraphs}
Edges connect k-sets of verices, not just pairs.

\section{Properties}
\subsection{Hop plot}
For h, let g(h) be number of node pairs with path $\leq h$. Hop plot plots this.

\subsection{Diameter}
Diameter of G. q effective diameter: q fraction of $V\times V$ have path length $\leq d$.

\subsection{Single source (s) shortest paths}
Consider a weighted graph with weight $e(a, b)$ between two nodes. Suppose that you want to find the shortest path to every vertex $v \in V$ from $s$.

One can use a bottom-up programming approach. (Dijkstra) Start with current node $c = s$. Initially set $d(s) = 0$ and, $ \forall v \neq s : d(v) = \infty$.

Define $f(c)$: For every $v \in \nbd(c)$, do the update: $d_{t+1}(v) \dfn \min(d_{t}(c) + e(c, v), d_{t}(v))$. Also, simultaneously update the 'backpointer' to point to $c$ (representing  the optimal subsolution) if necessary.

Do $f(c) \forall v \neq s$ .

Time: $O(n^{2})$ in case of a complete graph.

\section{Associated matreces}
Edge incidence matrix J ($m \times n$); for weighted G: edge $e_{i, j}$ terminii (i, j) are marked $ \pm \sqrt{e_{i,j}}$.

Vertex incidence/ adjacency matrix W: presence of edge $e_{i,j}$ indicated by weight at $W_{i,j}, W_{j,i}$.

Connectivity matrix $A^{\infty}$.

Degree matrix: D = diag(deg(i)).

\subsection{Graph Laplacian of no-self-loop undirected graph}
L = (D-W). $L = JJ^{T}$. As L is $|V|*|V|$, it is an operator on the functions with domain V.

\subsubsection{+ve semidefiniteness}
L is symmetric, +ve semidefinite: $x^{T}Lx = x^{T}JJ^{T}x$. So $\ew \geq 0$.

\subsubsection{Smoothness of vectors from quadratic form}
$x^{T}Lx = x^{T}Dx - x^{T}Wx = \sum_{i,j} W_{i,j} (x_i^{2} - x_j x_i) =  2^{-1}\sum_{i,j} W_{i,j} (x_{i} - x_{j})^{2} = \sum_{e_{i,j} \in E} e_{i,j} (x_{i} - x_{j})^{2}$. This is a measure of the degree of oscillations/ smoothness among x, where edges occur.

\subsubsection{Eigenvectors}
$L1 = 01, \therefore \ew_{1}(L)=0$; so L singular.

If G has c connected components: $\ew_{1}= .. \ew_{c} = 0$: construct ev with 1 in the appropriate spots!

\subsubsection{Smoothness of ev}
Take ev x. Then $x^{T}Lx = \sum_{e_{i,j} \in E} e_{i,j} (x_{i} - x_{j})^{2}$ measures smoothness of x where edges occur in the graph. But, ew are stationary points of $R(x) = x^{T}Lx/ (x^{T}x)$. So, ev corresponding to lower ew tend to be smoother.

\paragraph*{Smoothening functions}
Consider the subspace spanned by the p bottom (smooth) ev. Any function on V, ie a $|V|$-dim vector can be projected on to this subspace in order to smoothen it according to the graph structure. Labelling of nodes is an example of such a function.

\paragraph*{Applications}
This property is useful when using ev x for classification of nodes - one doesn't want neighboring nodes to have disparate values in x. This is useful in both spectral clustering and label propogation in semisupervised learning.

\subsection{Normalized graph Laplacian}
Take $N = I-D^{-1/2}WD^{-1/2}$ : this is the normalized version, $D^{-1/2}LD^{-1/2}$ of L, using the normalized adjacency matrix $D^{-1/2}WD^{-1/2}$.

$N \succeq 0$ as $L \succeq 0$, ie $x^{T}Dx-x^{T}Wx \geq 0 \forall x$: taking $D^{1/2}x = y$, see that $\forall y: y^{T}Ny \geq 0$.

\subsubsection{Normalized adjacency matrix has norm 1.}
As $y^{T}y - y^{T}D^{-1/2}WD^{-1/2}y \\
\geq 0$, see that $1 \geq \norm{D^{-1/2}WD^{-1/2}}_2$; Also, using $y = D^{1/2}1$, get \\
$\norm{D^{-1/2}WD^{-1/2}}_2 = 1$.

Another way to see this: $D^{-1/2}WD^{-1/2}$ is obtained by a similarity transformation to $D^{-1}W$, which has ew in the range [-1, 1] due to Gerschgorin thm, and which has $\sw_max = |\ew_max| = 1$ using the ev 1.

\subsubsection{Quadratic form: Normalized smoothness measure}
$y^{T}Ny = y^{T}D^{-1/2}LD^{-1/2}y = \\
\sum_{(i, j)\in E}W_{ij}(\frac{y_i}{\sqrt{d_{ii}}} - \frac{y_j}{\sqrt{d_{jj}}})^2$: from the form $x^{T}Lx$ being a smoothness measure. Punishes deviation between $\set{y_i}$ corresponding to edges emanating from high degree vertices less.

\chapter{Metrics and similarity measures on V}
Similarity measures between u and v are inverses of distance metrics.

\section{Based on paths and random walks}
- (shortest path).

\subsection{Katz measure}
 $sim(u,v) = \sum_{l=1}^{\infty} \gb^{l}|paths_{u,v}(l)|$. Matrix of scores, $K = \sum_{i=1}^{\infty} \gb^{i}A^{i}$.

\subsubsection{The damping parameter}
$K = (I - \gb A)^{-1} - I$ if the sum converges. Use $A = U\EW U^{*}$: EW decomposition, with $\ew_{i}$ ordered in descending order. $\sum_{i=1}^{\infty} \gb^{i}A^{i} = U(\sum_{i=1}^{\infty} \gb^{i}\EW^{i})U^{*}$ does not converge $\forall \gb \geq 0$ and multiplication by $\infty$ is not well defined. Condition for convergence: $\gb \ew_{1} < 1$.

But, for $\gb > 1$ the intuition of weighting longer paths less does not hold.

\subsection{Variants of Katz}
Similarly can use $e^{-\gb A}$.

Truncated Katz usually used: $\sum_{l=1}^{k} \gb^{l}A^{l}$: $O(ln^{2}nz(A))$ op instead of $O(n^{3})$ inverse finding.

\subsection{Hitting time}
$-h_{u,v}$; normed by stationary distribution: $-h_{u,v} \pi_{v}$: to take care of skewing of hitting time due to large $\pi_{v}$. - Commute time: $-h_{u,v} - h_{v, u}$; stationary distr normed: $-h_{u,v}\pi_{v} - h_{v, u}\pi_{u}$.

\subsection{Rooted PageRank}
Random walk can get lost in parts of graph away from u and v; so do random resets and return to u with probability a at each step.

\subsection{SimRank}
A recursive definition: 2 nodes are similar to the extant that they are joined to similar nodes. $sim(x, x) = 1; sim (x, y) = \frac{\sum_{a \in \nbd(x)}\sum_{b \in \nbd(y)} sim(a, b)}{|\nbd(x)||\nbd(y)|}$.

\section{Common neighbor based}
Common neighbors: $|\nbd(u) \inters \nbd(v)|$: same as taking inner product of rows in adj matrix M corresponding to u and v. (Jaccard): $\frac{\nbd(u) \inters \nbd(v)}{\nbd(u) \union \nbd(v)}$: pick a feature at random, see probability that it is a feature of both u and v. (Adamic/ Adar): $\sum_{z \in \nbd(u) \inters \nbd(v)} \frac{1}{\log |\nbd(z)|}$: greater wt to rare features present in both.

\chapter{Random graphs: \htext{$G_{n,p}, G_{n,|E|}$}{..}}
\section{G(n,p)}
A static model: every edge is present or absent independent of other edges. p controls edge density. As $n \to \infty diam(G) \to 2$. Size of the largest cluster is $O(\log n)$.


\chapter{Expanders}
A sparse graph with high connectivity properties. Connectivity quantified as edge expansion or vertex expansion. Let $E'(S)$: edges with exactly one end point in S.

\section{Edge expansion of G}
Aka isoparametric number. \\
$h(G) = \min_{1\leq |S| \leq n/2} \frac{|E'(S)|}{|S|}$.

\section{\htext{$\ga$}{alpha} vertex expansion of G}
$g_{\ga}(G) = \min_{1 \leq |S| \leq n \ga} \frac{|\nbd(S)|}{|S|}$.

\section{Other Examples}
Most graphs are expanders. $K_{n}$ has good expansion properties, but it is not sparse.

\tbc

\chapter{Clique tree of G}
Look at maximal cliques, pick a set of such cliques so that every edge is in some clique, make corresponding clique-nodes, make a tree by adding paths between cliques passing through shared-vertex-set nodes.

Eg: 1-2-3 has clique tree (1,2)-2-(2,3).

G can have multiple clique tree.

\section{Junction tree property}
Take any path in the clique tree between cliques $C_1$ and $C_2$; then $C_1 \inters C_2$ appears in every shared-vertex-set node inbetween them.

If a clique tree has it, it is a junction tree. These are useful in inference using graphical models: see probabilistic modelling ref.

Only chordal graphs have a junction tree; but can always add edges to convert non chordal graphs to chordal graphs.

\subsection{Finding a junction tree}
Take all maximal cliques (this is easy for chordal graphs), connect them with edges with certain weights, find maximal weight spanning tree. \tbc

\subsection{Tree width of G}
Take t = The size of the maximal clique in the junction tree of G with the smallest maximumal clique size. The tree width  = t-1.

\chapter{Relationship with Electric network analysis}
Kirchoff current (y) and voltage (x) laws, Ohm's law: edge current sources f: $Ax=b, A^{T}y=f, y=CAx$. Elimination in A: rows independent if not loop; Formation of spanning tree; so r=n-1. So, left nullspace basis = loops = m-n+1. Implies \textbf{Euler Formula}: n - m + loops = 1.

\chapter{Random walks on G}
\section{Markov process with state graph G}
Take a Markov chain with probability matrix $P_{u, v} = \frac{1}{deg(u)}$. $\pi_{v}=\frac{d(v)}{2|E|}$. \chk

\section{Hitting and cover times}
Hitting time $h_{u,v}$: expected time to get to v from u. Commute time: $C_{u,v} = h_{u,v} + h_{v, u}$. $h_{u,v}<2|E|$ \why.

Cover time C(G) = $\max_{v}$ E[time to hit all nodes starting from v]. $C(G) < 4|V||E|$ \why.

\section{Connection with resistive electric networks}
Let every $e\in E$ have resistance 1, thence get n/w N(G). $R_{u,v}$: effective resistance between u, v. $C_{u, v} = 2m R_{u, v}$ \why.


\part{Determining graph properties}
\chapter{Node search}
\section{Searching for a goal node}
Problem; initial state; successor function; goal test; path cost; step cost.

The search tree vs state graph; its branching factor. NP hardness. Optimality vs completeness of algorithms. Eg: Route finding problem with a map; TSP.

\section{Uninformed Search problem}
Breath first search; exponential memory demands. When pathcost varies: Uniform cost search.

Depth first search: linear space complexity. Iterative deepening DFS: use diameter of state graph. When pathcost varies: Iterative lengthening search.

Bidirectional search.

Avoid repeated states: open and closed lists of states.

\section{Informed search}
At node x, distance to goal node: d(x). Heuristic functions h(x). Admissible heuristics: $h(x) \leq d(x)$; never overestimate. Consistent heuristics and the triangle inequality. Greedy best-first search. A*.

\section{Local search/ optimization}
Eg: Protein structure prediction. Hill climbing. Statespace landscape: Local minima and shoulders. Random restarts. Simulated annealing. Local beam search. Genetic algorithms.

\section{Offline vs online search}
Eg: Search on internet without a prior index; protein design. Learning in online search (exploration); updating the search tree and heuristic values of nodes.


\chapter{Network flow}
Take network G=(V,E) with source s, sink t; \\
$(u,v) \in E$ has capacity c(u,v). Flow $f:V\times V \to R$ with capacity constraints, flow conservation $\sum_{u} f(u, v) = 0$, skew symmetry: f(u, v) = -f(v, u). Residual capacity of an edge: $c_{f}(u, v)$. Thence, can get residual network $G_{f}$. Augmenting path $s, v_{1}, v_{2} .. t$: $\forall i, c_{f}(v_{i}, v_{i+1})>0$.

\section{Max flow problem}
Start with 0 flow. Max flow exists iff $ \nexists $ augmenting path. Check for augmented flow; Keep increasing or decreasing flow by small fractions.

\section{Min cut problem}
Reduce to max flow problem.

\chapter{Tree search}
\section{Find minimum spanning tree (MST)}
(Kruskal) Start with forest F = V; pick $e \in G$ with least weight; if e connects 2 trees in F, add it to F: else discard e; repeat.

(Prim) Grow a tree T starting from a vertex: at each step, add edge with least wt which brings in new vertex to T.

\chapter{As data structures}
See algorithms and data structures survey.

\part{Evolution of real world graphs}
\chapter{Networks in the Real world: Properties}
See \cite{leskovecThesis}.

\section{Examples}
May be directed or undirected. Citation graph can be considered directed. Coauthorship graphs. Query graphs. Online social network websites

\subsection{Affiliation networks}
A bipartite graph G'=(A,C,E) of actors A, communities C and edges E. Can implicitly define social network amongst A: by folding G'/ forming edges between actors depending on shared memberships in $c \in C$.

\section{Static patterns}
\subsection{Small diameter}
Small world phenomenon in social networks: remarkably short paths connect very unrelated people. Eg: A certain developmental psychologist has Erdos number 3. 6 degrees of separation idea.

Two randomly selected web pages are connected is around .35.

So, effective diameter of the graph is small.

\subsection{In and out Degree distribution}
Heavy tailed distributions. Folks look at shape of the graph, and observe a distribution pattern.

May follow power law distribution over a large degree-range; observed in online social networks, citation graphs. So, number of nodes with degree d $\propto d^{-\gb}$. $\gb$ may or mayn't vary over time.

Else DGX distribution.

Else log normal distribution.

In-degree distributuion is more skewed, usually.

\subsection{Scree plot}
Plot of ew vs rank using log vs log scale. Approximately obeys power law: You see a straight line.

\subsection{Clustering coefficient}
In social networks, most new edges form to close triangles. Let degree(v)=d. Clustering coefficient $C_{d}(v) = \frac{\Del(v)}{\nbd(v)(\nbd(v)-1)/2}$, where $\Del(v) = \triangle$ centered at v: the fraction of potential $\triangle$'s centered at v which actually exist.

$C_{d} = avg_{v:deg(v)=d}(C_{d}(v))$. Gets smaller with increasing degree. Follows power law distribution: $C_{d} \propto d^{-1}$.

Global clustering coefficient $C = avg_{v}(C_{d}(v))$. It is a measure of transitivity in the network. C in real networks is significantly higher than for random networks, conditioned on same degree distribution.

Indicates some degree of hierarchial and community structure. The low-degree nodes belong to very dense sub-graphs and those sub-graphs are connected to each other through hubs.

\subsection{Community structure}
\subsubsection{2 meanings}
Communities amongst V observed in real world G, due to homophily. In the internet, many dense bipartitite subgraphs observed (Border).

Communities in the social networking websites often don't correspond to dense subgraphs of the social network.

\subsubsection{Hierarchical structure}
Found in social network, internet: a scale-free self similar structure observed.

\subsubsection{Fractal or onion structure}
Similar structure observed as you keep zooming in on a part of G.

\subsubsection{Communities in the social networking websites}
Group size: Power law distribution. Number of affiliations of nodes of a certain degree: exponential distribution. \cite{zhelevaSocialAffiliationNw} Group membership: a high fraction of singletons: especially in small groups, decreases with group size. With groups size, highest degree of member nodes increases.

\subsection{Core-periphery structure}
found in internet autonomous systems.

\subsection{Network motifs}
Basic building blocks of complex networks. Frequency of occurance compared to random graphs, function then hypothesized. Graph isomporphism checks feasible only for motifs of size $\leq 5$.

Eg: gene regulatory networks.

\subsection{Microscopic properties}
Most (70 to 90\%) edges are bidirectional \cite{leskovecThesis}..

\section{Other properties}
Eigenvector components distribution skewed.

Resilience to random node attacks, but suceptibility to high degree hub node attacks.

Stress.

\section{Temporal evolution patterns}
% See \cite{LeskovecThesis}.

\subsection{Densification}
Polynomial densification: $|E(t)| \propto |V(t)^{\ga}|$ with $\ga >1$. So, avg degree increases with time.

\subsubsection{Relationship with power law degree distribution}
$\gb \in (1,2) \implies \ga = 2/\gb$.

$\gb >2, \ga \implies$ $\gb(n) = \frac{4n^{\ga-1}-1}{2n^{\ga-1}-1}$ changes in a certain way over time.

Both observed in actual graphs \cite{leskovecThesis}.


\subsection{Shrinking diameter}
Diameter shrinks over time, asymptotically levels off.

Densification alone doesn't not result in this (experiment on $G_{n,p}$). Densification with particularly evolving degree sequence result in this. Nothing special about edge attachments: make random graphs with same degree sequences. \cite{leskovecThesis}

\subsection{Node arrival function}
N: time $\to \set{0,1}$. High variation among networks: exponential to sublinear.

\subsection{Edge initiation process}
\subsubsection{Node lifetime L}
After a certain time, node does not form new edges. Spike at L=0, as many people join the network and never visit the networking website again. Best modelled by $Pr(L = l) = ke^{-lk}$: exponential distribution.

\subsubsection{Age}
Edge initiation rate seems to stay constant accross ages.

Graph shows spike at age=0, as many people join the network and never visit the networking website again.

\subsubsection{Time gap G between edges}
G follows power law with exponential cutoff. $p(x) \propto x^{-\ga}e^{-x\gb}$. $\ga$ stays constant with time, but $\gb$ grows linearly.

\subsection{Edge (u, v) destination selection process}
Most (30 to 60\%) new edges close triangles: local. Pr(v is k hops away) $\propto e^{-ck}$; Pr(certain v: k hops away) is doubly inverse exponential as number of verteces k hops away itself grows exponentially. \cite{leskovecThesis}

For the creation of the first edge: $Pr(degree(v) = d) \propto d^{t}$, where t = 1 or 0.9 etc..

\chapter{Generative models for real world graphs}
\section{Applications}
Finding patterns and abnormalities in real world networks.

Making either fine or coarse grained hypotheses of graph formation process.

\subsection{Making Extrapolations}
What will G look like 10 months later?

So, algorithmic benefits of the model important: How easy is it to calculate paths? Eg: A may be queries, C may be retrieved documents, maybe we want to find the closest commercial/ advertisement-carrying queries, so want to find shortest path to commercial query in the network of queries.

\subsection{Graph sampling}
Deduce information about the graph by sampling a small number of nodes: if the temporal properties of the statistics which result from this sampling.

\section{Evaluation of models}
Does the model produced have desired global static and temporal properties? Are the fine grained node arrival and edge formation processes modelled as a time, or do nodes arrive, form edges and stop?

How likely is the generation of a certain real-world network, or of a network with similar properties by the random processes in the model? Is the edge-formation process likely under it?

How useful is it? Is graph sampling easy to determine its properties?

Is it easy to extrapolate its properties analytically? Can you calculate the likelihood of a model given a real world graph G? If so, ye can pick the max likelihood model and extrapolate what G looks like n years later.

\section{Achieving various properties}
\subsection{Strategies}
Look at models which achieve certain properties, even if they don't satisfy other properties; combine their best flavor to produce a good model.

Or observe closely microscopic nw evolution properties: edge initiation, edge destination selection, lifetime etc.., mimic them, see if it produces desired global properties.

\subsection{Power law}
Power laws often appear in combination with self-similar fractal structures.

Naive power law generator: edges come in, pick m from power law, randomly create m edges.

\subsubsection{Hypotheses}
Power law for in-degree is achieved by some flavor of 'preferential attachment' method.

Heavy tails emerge if nodes try to optimize connectivity under resource constraints. So may be humans are engineering things, like the internet, language. Eg: Take a language with n words, cost of word with length j is log j; this leads to power law distribution on word length.

Criticism: But then, monkey typing a keyboard with n letters and a spacebar achieves power-law distribution on word-length.

\subsubsection{Other factors}
A model based on exponential node lifetime distribution and power law with exp cutoff distr for time gap between edges yields power law out-degree distr.

\subsection{Acheiving good community structure}
Copying model acheives good community structure.

\subsection{Edge destination selection}
Most edges (u, w) close triangles. Triangle closing: many choices for w, what yields best improvement over random choice of w among 2-hop neighbors: choose $v=\nbd(u)$ by process A; then choose $w=\nbd(v)$ by process B. Biggest improvement yielded when processes A and B are uniform random choice: bias towards w with more 2-hop paths leading to them.

\section{Models which don't yield heavy tailed distributions}
\subsection{Random graph generators}
(Erdos, Renyi) G(n,p) model. G(n, 0.5) is the uniform distribution over all graphs with $|V|=n$. G(n,N) model. Exhibits phase transition phenomenon. \tbc

Don't produce heavy tailed distributions.

\subsection{Stochastic Adjascency matrix}
Take adj matrix, replace 1 and 0 with probabilities p and q. If p=q, get Erdos, Renyi graph.

\section{Preferential attachment generators}
New node arrives; draws number of edges m from a distribution; probability of connecting with u $\propto degree(u)$.

Yields power law in-degree distribution with degree 3, low diameters. 'Rich get richer'.

\section{Edge Copying model}
Node v arrives, picks no of edges m, either picks node v and does this m times: a] connects to random $u \in \nbd(v)$ or b] with prob $\gb$ connects to random node. Generates power law distribution with exponent $(1-\gb)^{-1}$.

Leads to good community structure.

\section{Random walk model}
New node comes in, does random walk, for each visited node, create edge with some probability. Generates power law distribution.

\section{Community guided attachment}
Represent recursive structure of communities with a tree T of height H. Take perfectly balanced T; leaves will be the nodes; $Pr((u,v) \in E) \propto c^{-\del(u,v)}$, where $\del$ is tree distance.

Leads to densification: $E(t) \propto N(t)^{a}$, as expected. If c near 1, lack community structure; if near 2, get good community struct.

\subsection{Dynamic community guided attachment}
Start with one node. Keep adding levels to a b-ary balanced tree by adding nodes $\set{u}$, out-edges from each u to v with $Pr((u,v) \in E) \propto c^{-\del(u,v)}$.

Leads to densification: $E(t) \propto N(t)^{a}$; also to power law distribution for in-degrees.

\subsection{Defects}
Diameter grows slowly. No heavy tailed out degree distr.

\section{Small world model}
Start with lattice; for each edge with prob p, move edge to random node. Low p values tend to yield good community structure.

(Kleinberg) Yields small diameter, easily discoverable paths if long range neighbors chosen in a certain way.

\section{Forest fire model}
Node v arrives, chooses preexisting ambassador node u, forms link to u, picks random x and y, selects x out-links and y in-links from u to $\set{w_{i}}$, forms out-links to $\set{w_{i}}$, does the same from each $t\in \set{w_{i}}$ recursively: the fire spreads.

\subsection{Good properties}
Heavy tailed in-degree: due to 'rich get richer' flavor: highly connected vertices easily reached, irrespective of initial ambassador node.

Community structure, due to 'copying' flavor.

Heavy tailed out-degree: good chance to form many links.

Densification power law: due to 'community guided attachment' flavor: lot of links formed near the community.

Shrinking diameter also observed.

\oprob: Does it have good clustering coefficient distribution?

\section{Modelling Microscopic network evolution}
Parameters: Node arrival fn N(), $\ga, \gb$.

At each time step: check for Node arrival according to N(); every new node samples lifetime and creates first edge according to preferential attachment; nodes $\set{u}$ which are awake create an edge (e, w) by triangle closing by picking random neighbor v and then a random $w \in \nbd(v)$; they then sample sleep time from distr: $Pr(G=g) = g^{-\ga}e^{-g \gb}$; repeat.

When a real graph is evolved with this model, it achieves power law degree distribution, expected clustering coefficient vs degree graph, distribution of shortest paths. Densification, shrinking diameter observed by other authors.

\section{By evolution of affiliation network G' = (A,C,E')}
\subsection{Preferential attachment + edge copying}
(Lattanzi, Sivakumar).\\
Affiliation nw evolution: Pick $d', d'' \in N$; start with affiliation nw $G'_{0}$ where each $a \in A$ has min degree d', every $c \in C$ has min degree d''. With probability p, evolve A: add an actor a, find a preferentially chosen prototype $a' \in A$, randomly copy d' of its edges. Similarly, with prob (1-p) evolve C.

At each time step, can get social nw G=(A,E) among A by creating an edge (a,b) when they share a community; upon addition of an actor a, can also add edges to s preferentially chosen actors.

\subsubsection{Good properties}
In G': Results in power law degree distribution for degrees of nodes in both A and C. In G: results in power law distribution for A; yields densification of E, shrinking/ stabilizing diameter.

\subsubsection{Algorithmic benefits}
G sparsifiable while mostly preserving distances from a set of relevant nodes.

\section{Kronecker power graphs}
\subsection{Kronecker-multiplied graphs}
(Leskovec et al). Take adjascency matreces A, B. Edge property: $A \kron B$ is a graph with $Edge(X_{i,j}, X_{k,l})$ iff $(i,k)\in edges(A) \land (j,l)\in edges(B)$. So, like placing graph B in stead of every node of A, and then making extra edges. Could be disconnected if no self-loops. Defines Kronecker power graph $G_{k} = G^{(k)}$: $G_{1}$ assumed to have self loops for each node.

Can also use a stochastic adjascency matrix for $G_{1}$.

\subsection{Properties}
From Edge property: Degree distribution of $G_{k}$ is kth Kronecker power of $(d_{1} .. d_{n})$: so multinomial. EW distribution is $(\ew_{1} .. \ew_{n})$. Components of each eigenvector follow a multinomial distribution. \why $E_{k} = E_{1}^{k}, N_{k} = N_{1}^{k}$: so follows densification power law.

From Edge property: If A, B have diameter at most d, $A \kron B$ has diameter at most d. If $diam(G_{1}) = d$, $diam(G_{k}) = d$.

For stochastic Kronecker products: Degree distribution, scree plot shown to compare well with real world graphs. \oprob Demonstrate the theoretical connection with the power law degree distribution.

\subsection{Defects}
Does it have community structure? Does it have a good clustering coefficient distribution? Presumed answer no.

\section{Other generators}
Redirection models: produce constant diameter, logarithmically increasing avg degree. Exponential random graphs

\subsection{Variations}
Slight tweaks to better imitate real graphs: eg: orphans.

\section{Evolution of affiliation nw in social nw websites}
Extends microscopic nw evolution model thus: At each time step, each awakened node v does this: pick number of groups to join, n from $ke^{-kn}$ with mean $k^{-1} = r deg(v)^{g}$: thus striving for exponential distribution of number of groups of nodes of given degree, polynomial growth of avg number of groups wrt degree; join n groups doing this: with prob t create a new group; else join existing group: with prob $p_{v} = \eta deg(v)$ join a group picked through a friend: $\eta$ represents friend's influence; else join random group.

Yields same properties for social nw evolution\\
 as microscopic evolution model; achieves power law group size distr; suitable fraction of singletons in groups (by tweaking t, $\eta$); exponential distr for num of affiliations for nodes of given degree.

\chapter{Link prediction problem}
Graph G = (V, E). Every edge has this label: t(e): The time during which interaction represented by e took place. Given $G[t_{0}, t_{0}']$ (training interval), output a list of edges not presented in it, but present in $G[t_{1}, t_{1}']$ (testing interval). Maybe want to output top n most likely-to-appear edges.

\section{Motivation}
Security. Improving organizational efficiency by going beyond official hierarchy. Inferring missing links from observed network.

\section{Observations about link prediction}
Small world problem: tenuous short links exist between otherwise unrelated nodes: noise wrt to the link prediction problem.

\cite{dln:link-prediction} observed that in their data-sets, new links are 3 or 4 times more often formed at distances $\geq 3$. They compared performance of prediction methods at predicting new links between nodes at distances $\geq 3$.

\section{Using similarity measures between nodes}
\cite{dln:link-prediction} experimented. Performance compared against a random predictor. Katz measure, and variants with added noise reduction performed well. \cite{dln:link-prediction} found that Katz, common neighbors in low rank approximation of G, Adamic/ Adar are similar in terms of common predictions. \tbc

\section{Reducing noise}
Low rank approximation: Take adjacency matrix M; use SVD to get rank k approximation $M_{k}$ of M wrt various norms. Can then use other similarity measure based techniques. Usually, intermediate rank approximation performs best \cite{dln:link-prediction} .

Clustering: Remove tenuous edges.

\section{Enhance similarity measure for nodes with similar neighbors}
Adaptation of 'unseen bigrams' technique from language modelling: word pairs which occur in test corpus but not in training corpus. Take $S_{x}^{d}$: the top d nodes related to x under a similarity measure; get modified score(x,y) = $|\set{z: z \in \nbd(y) \inters S_{x}^{d}}|$.

\section{Model evolution of E}
\subsection{Using a linear model}
Split E into 2 parts \\
thence get adj matreces A and B. Use various F like $F(A) = e^{aA}$; find parameters a of F to get $\norm{F(A) - (A+B)}$. Take $A = U\EW U^{*}$: $\EW$ decomposition; if $F(A) = UF(\EW)U^{*}$, this becomes: $\norm{F(\EW) - (A+B)}$: this is a least squares curve-fitting problem like $\min \sum (e^{a\ew_{i}} - (A+B)_{i,i})^{2}$.

\section{The experimental setup}
\subsection{The problem}
Perhaps you have different snapshots of the graph for different times: test set will be the latest snapshot - previous snapshots. Perhaps you have a single snapshot of the graph: test set will be some random split of the graph into training and test edges. n edges to be predicted.

Often compare against a random predictor. The random predictor: What is the expected number of edges you get right if you pick n edges randomly?

To see understand behavior of a single set: Plot precision curve, recall curve.

\subsubsection{The sparse case}
If data is too sparse, there is not enough signal, and similarity measures such as Katz do worse than random. To compare various methods in this case: one picks many mn edges and then compare 'recall': the correct edges in the prediction set.

\section{Open problems}
\oprob \\
Discover better performing link prediction algorithms. Make link prediction algorithms based on distance metrics more efficient.

\oprob Include more data in the social network (eg: paper titles, areas) and improve link prediction performance.

\oprob Improve link prediction performance by giving greater weight to more recent collaborations.

\oprob View training-period collaborations as samples drawn from a distribution on pairs of people; Try to use work on estimating distribution support in link prediction. \oprob You only have +ve labelled examples; see if machine learning provides better learning heuristics in such cases.

\chapter{Network cascades}
\section{Applications}
Epidemology. Spread of ideas.

\part{Analysis of real world networks}
\chapter{Partitioning nodes}
\section{Number of partitions, k}
See statistics ref for further info.

\section{Minimum cut objectives}
Maybe find $argmin_{(V_{i})} cut((V_{i}))$: but $V_{1} = V$ minimizes it. So, to balance the partitions, associate each vertex with a weight w(v), get W = diag(w(v)); define $w(V_{i}) = \sum_{v \in V_{i}} w(v)$; minimize $Q((V_{i})) = \sum \frac{cut(V_{i})}{w(V_{i})}$.

\subsection{Ratio, normalized cuts}
If $w(v) = 1$, get ratio cut objective fn. So, ratio cut is trying to minimize the weight of cross-cut edges, averaged over the nodes.

If $w(v) = \sum E_{v,w}$, get normalized cut objective fn: $N((V_{i})) = \sum \frac{cut(V_{i})}{w(V_{i})} = 2 - \sum \frac{within(V_{i})}{w(V_{i})}$: so minimizing N is same as maximizing wt of edges lying within each partition. Normalized cut is trying to minimize the weight of cross-cut edges, averaged over the nodes but normalized to allow high-degree vertex-sets to have more cross-cut edges.

These are discrete optimization problems: NP complete \why. Effective heuristics exist.

\section{Modularity of partition C}
(Newman) Amount by which number of edges in C exceed Chung's degree distribution preserving random graph model.

\section{Spectral clustering}
\core A relaxation of normalized cut objective $\equiv$ finding a solution to the generalized eigenvector problem over the graph laplacian L, then partitioning points derived from the top k ev.

\subsection{The objective}
Represent cut by partition vector $p \in \set{\pm 1}^{n}$. Then Rayleigh quotient: $\frac{p^{T}Lp}{p^{T}p}= n^{-1}4 cut(V_{1}, V_{2})$: $p^{T}p=n; p^{T}Lp = \sum_{e_{i,j} \in E} e_{i,j}(p_{i}-p_{j})^{2}$.

Use partition vector q with $q_{i} = \sqrt{\frac{w(V_{2})}{w(V_{1})}}$ for $i \in V_{1}$; else $-\sqrt{\frac{w(V_{1})}{w(V_{2})}}$. Then $q^{T}W1 = \sqrt{\frac{w(V_{2})}{w(V_{1})}} w(V_{1}) - \sqrt{\frac{w(V_{1})}{w(V_{2})}} w(V_{2})= 0$; $q^{T}Wq = w(V)$.

As $q = \frac{w(V_{1})+w(V_{2})}{2\sqrt{w(V_{1})w(V_{2})}} p + \frac{-w(V_{1})+w(V_{2})}{2\sqrt{w(V_{1})w(V_{2})}}e$; so $q^{T}Lq = \frac{[w(V_{1})+w(V_{2})]^{2}}{4 w(V_{1})w(V_{2})}p^{T}Lp$. As $p^{T}Lp = cut(V_{1}, V_{2})$; $\frac{q^{T}Lq}{q^{T}Wq} = Q(V_{1}, V_{2})$.

Minimizing this subject to suitable q's is still NP complete; so use real relaxation. So, solve $\min_{q} \frac{q^{T}Lq}{q^{T}Wq}$ subject to $q^{T}We=0$, deduce partition vector from solution. Opt problem solved when q is ev corresponding to $\ew_{2}$ for generalized ev problem: $Lz = \ew Wz$. \why

To find k custers, do this recursively, or pick $m \geq \log k$ ev and partition points formed by putting various components of these ev together, maybe using k-means.

Requires $O(n^{2})$ time: to find ev; $O(kn^{2})$ to find k clusters among graph nodes.

\subsection{Coclustering nodes in bipartite graph G=(A, B, E)}
\core \\
(Dhillon). Finding the second ev of L $\equiv$: take $|A|*|B|$ adjacency matrix M, and find its 2nd left and right sv: as M is smaller than L, ye save on computation.

\section{Graclus}
Does kernel-k-means among nodes. Equivalent to solving a relaxation of the normalized cuts objective.

\chapter{Identifying dense subgraphs : relevant clusters}
\section{(a, b) cluster C}
(Tarjan etal.) E(v, C): edges between v and C. External sparsity vs internal density of edges: $\forall v \in C: |E(v, C)|\geq b|C|$, $\forall u \notin C: |E(u, C)|\leq a|C|$. Cliques are (a, 1) clusters.

Bound on cluster overlap known. For certain (a, b) it is possible for one cluster to be contained in another, but not if size of largest : smallest cluster size $\leq \frac{1-a}{1-b}$. $b>1/2$ ensures cluster connectedness: condition may be too strong in practice.

r-champion v of C: v has $\leq r|C|$ neighbors outside C. Gives a stronger argument for C being a good cluster.

Good clusters have small r and a, large b.

\subsection{Finding (a, b) clusters}
If there is a large gap between a/2 and $b > 1/2 + (r+a)/2 > 1/2$, then there are $\leq n$ (a, b) clusters with r-champions of size s, can find them deterministically  in $O(mn^{1.2}+n^{2+o(1)})$. Experimentally shown to find 90\% of maximal cliques size 5 or higher.

Alg: Take $t(c) = \nbd(c) \union \nbd(\nbd(c))$. For each $c \in V$: start with $C=\nullSet$; for each $v\in t(c)$: add v to C if $|\nbd(v)\inters \nbd(c)| \geq (2b-1)s$ \footnote{Reminiscent of most new edges closing triangles in graph evolution}; if C is an (a, b) cluster, output it.

\oprob extend to weighted graphs.

\chapter{Network of computers}
Eg: Internet.

\section{Important matrices}
Matrices showing delay, traffic etc..

\subsection{Matrix completion problems}
Often these have missing values. So, use matrix completion approaches.

\subsubsection{Peculiarities}
Maybe try to embed network: but often triangle inequality and symetry doesn't hold.

Missing values tend to be highly structured. But many matrix completion methods assume that missing values tend to be uniformly distributed.

\part{Mapping Graphs and pts in D dim Euclidean space}
\chapter{Graphs from pts in Euclidean space}
See statistics ref.

\chapter{Embedding complete graph with distance weights}
Want the minimum dimensional embedding.

\section{Formulation as matrix factorization}
(Shconberg). Take weighted adj matrix W. $W_{i,j} = \norm{x_{i}-x_{j}}^{2} = x_{i}^{T}x_{i} + x_{j}^{T}x_{j} - 2x_{i}^{T}x_{j}$. Take $X = (x_{i})$; Gram matrix $G = X^{T}X$ normalized to have $g_{i,i} = 1$; then $W = 2.11^{T}-2G$. Then solve for G; then solve $G = X^{T}X$ for X with min number of columns.

\section{Energy minimization techniques}
See section on incomplete graphs with similarity measures for intro to notation.

Use energy fn: $U = \sum_{(i,j)} (f(d_{i,j}) - g(d_{i,j}))$. Minimize over $R^{D}$.

\subsection{Clustering postulate and constraint}
Require $d_{min} = w(1/C)^{\gl}$ in attempting to place clusters $c_{1}, c_{2}$ with coupling C at distance w; so we want $\frac{1}{C} = (\frac{d}{w})^{1/\gl}$. Then, $U = f(d) - \frac{1}{C}g(d)$. Set derivative to 0, get constraint: $f'(d) = (\frac{d}{w})^{1/\gl}g'(d)$.

\subsection{Box Cox family of energy fn}
$U = \sum_{i,j} BC_{\gm + \frac{1}{\gl}}(d_{i,j}) + D_{i,j}^{1/\gl}BC_{\gm}(d)$.

Encompasses energy fn used in multidimensional scaling.

\section{Applications}
If nodes represent sample-points, can infer the dimensionality of the sample space. Then can measure distance distance between arbitrary sample points. Eg: Maybe can sample some hyenas, observe their interactions, conclude that Hyenas are 5 dimensional.

\chapter{Embedding Incomplete Graph G}
Can view this as matrix completion problem; perhaps with additional constraints enforcing symmetry and traingle inequality for the distance metric.

\section{Energy minimization techniques for G with similarity wts}
Put attractive forces f() between verteces connected by edges, and repulsive forces g() between all vertex pairs, define energy: $U = \sum_{(i,j) \in E} f(d_{i,j}) - \sum_{i,j}g(d_{i,j})$ then find minimum energy configuration in $R^{D}$.

\subsection{Clustering postulate and constraint}
Constrain range of choices for these forces with clustering postulates.

Take graph with 2 clusters $c_{1}, c_{2}$ with coupling $C = \frac{E(c_{1}, c_{2})}{E(c_{1})E(c_{2})}$. Require $d_{min} = \frac{1}{C}^{\gl}$ be minimum energy configuration; $\gl$ is clustering power.

Setting $U'(c_{1}, c_{2}) \approx f'(d_{min}) - (1/C)g'(d_{min}) = 0$, yields the clustering constraint: $f'(d) = d^{1/\gl}g'(d)$.

\subsection{Box Cox family of energy functions}
Want to allow cases where $f(d) = d, g(d) = \log d$; so use Box-Cox transformations for f, g: $d>0: BC_{p}(d) = \frac{d^{p}-1}{p}$ if $p\neq 0$, else $\log d$; with $BC_{p}'(d) = d^{p-1}$. So, use $g(d) = BC_{\gm}(d)$, get $ f(d) = BC_{\gm + \gl^{-1}}$.

\section{Eigenmap}
Take laplacian L of the graph G, and take its ev corresponding to bottom few ew, which are functions over V which are smooth over E.

\section{G with distance wts D}
This can be reduced to a complete graph: calculate distance for all pairs by finding shortest paths.

\bibliographystyle{plain}
\bibliography{graphTheory}

\end{document}
