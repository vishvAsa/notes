\documentclass{report}
% \usepackage[left=3cm,top=1.5cm,right=3cm,bottom=3cm]{geometry}
\usepackage{caption, subfigure}
\input{/u/vvasuki/vishvas/work/packages}
\input{/u/vvasuki/vishvas/work/macros}

%opening
\title{Gene Phene Link Prediction}
\author{Nagarajan Natarajan, vishvAs vAsuki}

\begin{document}
\maketitle
\tableofcontents

\chapter{Introduction}
\section{Terminology}

Gene: hereditary information stored in your DNA.

Phene: A phene is an individual genetically determined characteristic or trait which can be possessed by an organism. An example of phenes which could interest us would be various human diseases. We might be interested in identifying what genes are involved in a given human disease.

\section{Problem}
Figure out what genes are responsible for which human diseases.

\section{Data}
\subsection{Notation}
See table \ref{tab:notation}.

\begin{table*}
\centering
\begin{tabular}{| c | p{8cm} |} \hline
Symbol & Description \\ \hline
$S$ & Set of species id's; 1:6 corresponding to Hs At Ce Dm Mm Sc. \\ \hline
$G(1,1)$ & Gene interaction network for Hs \\ \hline
$G(a,b)$ & Ortholog information for species $a$ and $b$ \\ \hline
$genes(a)$ & Set of genes belonging to species $a$ \\ \hline
$phenes(a)$ & Set of phenes belonging to species $a$ \\ \hline
$orth(a,b,i)$ & Orthologs of gene $i \in genes(a)$ among $genes(b)$ \\ \hline
$P(a,b)$ & If $a = b$, this is a binary matrix specifying the knowledge about the participation of $genes(a)$ in the expression of $phenes(a)$. If $a \neq b$ this binary matrix specifies knowledge about the interaction of $orth(a,b,i)$ in $phenes(b)$. \\ \hline
\end{tabular}
\caption{Notation}
\label{tab:notation}
\end{table*}

\subsection{Gene Phene matrices}
See figure \ref{fig:P(1,1)}, \ref{fig:P(1,x)}
.
\begin{figure}
  \begin{center}
    \subfigure[1]{\includegraphics[scale=0.25]{../log/statistics/GenePhene1Sums.jpg}}
  \end{center}
\caption{Top graph: sum along rows (number of genes involved in a phene). Bottom graph: sum along columns (how many phenes is a gene involved in?). 
minSumAlongRows 2, minSumAlongCols 0, maxSumAlongRows 58, maxSumAlongCols 7, density 3.965741e-04, numZeroRows 15101, numZeroCols 0
}
\label{fig:P(1,1)}
\end{figure}

\begin{figure}
    \subfigure[2]{\includegraphics[scale=0.25]{../log/statistics/GenePhene2Sums.jpg}}
    \subfigure[3]{\includegraphics[scale=0.25]{../log/statistics/GenePhene3Sums.jpg}}
    \subfigure[4]{\includegraphics[scale=0.25]{../log/statistics/GenePhene4Sums.jpg}}
    \subfigure[5]{\includegraphics[scale=0.25]{../log/statistics/GenePhene5Sums.jpg}}
    \subfigure[6]{\includegraphics[scale=0.25]{../log/statistics/GenePhene6Sums.jpg}}

\caption{
minSumAlongRows 2, minSumAlongCols 0, maxSumAlongRows 376, maxSumAlongCols 39, density 9.460716e-04, numZeroRows 12515, numZeroCols 0 \\
minSumAlongRows 2, minSumAlongCols 0, maxSumAlongRows 1715, maxSumAlongCols 30, density 2.625097e-03, numZeroRows 13441, numZeroCols 0 \\
minSumAlongRows 2, minSumAlongCols 0, maxSumAlongRows 2748, maxSumAlongCols 358, density 1.247323e-03, numZeroRows 12510, numZeroCols 0 \\
minSumAlongRows 2, minSumAlongCols 0, maxSumAlongRows 1074, maxSumAlongCols 192, density 1.079007e-03, numZeroRows 11794, numZeroCols 0 \\
minSumAlongRows 2, minSumAlongCols 0, maxSumAlongRows 1165, maxSumAlongCols 157, density 3.499176e-03, numZeroRows 12784, numZeroCols 0}
\label{fig:P(1,x)}
\end{figure}



\subsubsection{Notes about gene-id's}
To look up human genes, use entrez db or visit  \url{http://www.ncbi.nlm.nih.gov/gene?term=1}. For other species, the id's wormbase, TAIR, SGD, MGD/ MGI, flybase.

\subsection{Gene interaction network}
The Human Gene Gene interaction matrix: Derived from a source of information independent from the gene-phene matrices. See \ref{fig:G(1,1)}.

\begin{figure}
\includegraphics[scale=0.25]{../log/statistics/GeneGene_HsSums.jpg}

\caption{
minSumAlongRows 0, minSumAlongCols 0, maxSumAlongRows 1.316873e+03, maxSumAlongCols 1.316873e+03, density 3.462266e-03, numZeroRows 346, numZeroCols 346}
\label{fig:G(1,1)}
\end{figure}



\chapter{Experiments}
\section{Experiment setup}
Predictor for a phene: $geneVector \to \set{0, 1}^{numGenes}$.

AUC is a measure of how good a predictor is; considers all cut-offs. In \cite{AUCRef}, we describe a method for obtaining AUC from a score vector.

\todo{Do a 3-fold test for AUC estimation.}


\section{Preliminary Experiments}
\subsection{Path based approaches}
For $A = A^{T}$, 
$$tKatz(A, \gb, k) \dfn A + \sum_{i = 1}^{k} \gb^{i} A^{i}$$

The score matrix $tKatz(A, \gb, k)P(1,1)$ is then computed.

\subsection{Factor based approaches}
SVPi(G, P) is performed as follows. First, the matrix $X(\gl)$ is constructed:
$$C(G) = \mat{\gl G & P\\
P^{T} & G}$$

Then, SVD is used to get a rank k approximation $Score_k$ of $C(G)$, where the best $\gl, k$ are learned using cross-validation.

\subsubsection{Ortholog-based Gene Phene matrices}
Consider the matrix
$$C' = \mat{\gl G(1,1) & P(1,1) & \dots & \gl*P(1,6) \\
           P(1,1)^T & 0 & \dots & 0 \\
           \vdots & \vdots & \ddots & \vdots \\
           \gl*P(1,6)^T & 0 & \hdots & 0}$$.

Get a $k$-rank approximation of $X(\gl)$ for the best $\gl, k$ learned through cross validation. Use the suitable portion of $X(\gl)$ as the score matrix.

$\gl$ can be applied to $P(1,i) \forall i>1$.

\paragraph*{Smoothening/ completing G}
Perhaps constructing \\
$G = tkatz(G(1,1), \gb, 4)$ will cause the factors to more faithfully account for the 'derived correlation' between various genes.

Perhaps G can be smoothened using information from P(1,i), especially P(1,1).

\paragraph*{Motivation}
Clearly, there is more information in $X(\gl)$ than in the matrix used in $SVPi(\sum_i P(1,i)P(1,i)^T,P(1,1))$. In particular, $G(1,1)$ is present in the former, but not in the latter. Also, deriving gene-gene similarity from $\sum_i P(1,i)P(1,i)^T$ may not be the best way of utilizing information from $P(1,i)$.

\subsection{Sparse SVP}
Using the combined matrix proved to be slow.

\subsubsection{Using only the human gene phene matrix.}
Best parameters: numFactors: 100, density: 0.020595 (minDensity * 10). Both these parameters were found at the lower end of the domain; so further parameter search may be required. This validation result indicates that this is better than SVP.

\paragraph*{Validation results}
parameters: 100 0 2.059525e-02  precision: 2.344666e-03, sensitivity: 6.400000e-02, specificity: 9.992973e-01



\subsection{Analysis of results}
\begin{table*}[h]
\centering
\begin{tabular}{| p{7 cm} | c | c |} \hline
Predictor & Avg AUC & Success fraction\\ \hline\hline
$tKatz(P(1,1)P(1,1)^T, 10^{-6}, 1)$ & 0.604 & 101 \\ \hline\hline
$tKatz(\sum_{i>1} P(1,i)P(1,i)^T, 10^{-6}, 1)$ & 0.773 & 228 \\ \hline
$tKatz(\sum_{i>1} P(1,i)P(1,i)^T, 10^{-6}, 4)$ & 0.788 & 244 \\ \hline\hline
$tKatz(G(1,1), 10^{-6}, 2)$& 0.756 & 216 \\ \hline
$tKatz(G(1,1), 10^{-6}, 3)$& 0.776 & 236 \\ \hline
$tKatz(G(1,1), 10^{-6}, 4)$ (best beta)& 0.781, 0.79 & 233, 246 \\ \hline
$tKatz(G(1,1), 10^{-1}, 4)$& 0.664 & 216 \\ \hline\hline
$tKatz(C'(.1), 10^{-2}, 4)$& .8296& 252\\\hline\hline
$SVD(C(G(1,1), P(1,1)))$ & 0.596 & 175 \\ \hline
$SVD(C(\sum_{i>1} P(1,i)P(1,i)^T))$ & 0.767 & 233\\ \hline
$SVD(C')$ with parameters 100 and .4 & 0.779 & 239 \\ \hline
$SVD(C')$ with better parameters& 0.798 & 245 \\ \hline
\end{tabular}
\caption{Comparison of the performance of various predictors on the Gene dataset. The last column 'Success fraction' denotes the number of phenes for which AUC was observed to be better than random, which has 0.5 AUC.}
\label{tab:results}
\end{table*}


\begin{figure}[h!]
\subfigure[tKatz(G(1,1), 4)]{\includegraphics[scale=0.25]{../log/katzHumanGeneInteractions-4Iterations-2010-03-29-18:59:56:189.jpg}}

\subfigure[SVPi(Other Species)]{\includegraphics[scale=0.25]{../log/svdApproxOtherSpecies2010-03-30-15:53:18:113.jpg}}
\label{fig:AUCPreliminary}
\caption{AUC curves for a couple of approaches}
\end{figure}

\subsubsection{Best parameters}
$SVD(C(\sum_{i>1} P(1,i)P(1,i)^T))$ numFactors: 70, l: 0.2.

$SVD(C')$ numFactors: 120, l: 0.2 overall. numFactors: 100, l: 0.4, in range 50-100.

\subsubsection{How different are the predictions?}
It would be good if two (good) predictors were very divergent in their predictions. Then, they could be combined together to make a better predictor. If one predictor were close to the other predictor, one of them would be redundant. 

We consider two ways of measuring how different two predictors are. 1. Average difference in scores. \\
2. Compensatory divergence. It is defined as follows. Let $S_1$ be the set of phenes where predictor 1 performs at least as good as random, while predictor 2 is worse than random. Define $S_2$ symmetrically. Then compensatory divergence is $|S_1| + |S_2|$.\\

Some interesting comparisons of predictors are presented below. In table \ref{tab:results}, we see that a few predictors perform well. Below, we see that they are quite divergent, too!

\begin{lstlisting}
Comparing:
 Predictors.commonNeighborsOtherSpecies2010-03-29-19:25:30:124.mat
 Predictors.katzHumanGeneInteractions-3Iterations-2010-03-30-04:21:58:976.mat
avgScoreDifference: 1.963642e-01 compensatoryDivergence: 62

Comparing:
 Predictors.commonNeighborsOtherSpecies2010-03-29-19:25:30:124.mat
 Predictors.svdApprox2010-03-29-15:46:12:874.mat
avgScoreDifference: 2.834087e-01 compensatoryDivergence: 107

Comparing:
 Predictors.commonNeighborsOtherSpecies2010-03-29-19:25:30:124.mat
 Predictors.svdApproxOtherSpecies2010-03-30-15:53:18:113.mat
avgScoreDifference: 1.693831e-01 compensatoryDivergence: 51

Comparing:
 Predictors.commonNeighbors_2010-03-29-11:07:03:540.mat
 Predictors.katzHumanGeneInteractions-3Iterations-2010-03-30-04:21:58:976.mat
avgScoreDifference: 2.645995e-01 compensatoryDivergence: 155

Comparing:
 Predictors.commonNeighbors_2010-03-29-11:07:03:540.mat
 Predictors.svdApprox2010-03-29-15:46:12:874.mat
avgScoreDifference: 2.064974e-01 compensatoryDivergence: 122

Comparing:
 Predictors.commonNeighbors_2010-03-29-11:07:03:540.mat
 Predictors.svdApproxOtherSpecies2010-03-30-15:53:18:113.mat
avgScoreDifference: 2.637711e-01 compensatoryDivergence: 148

Comparing:
 Predictors.katzHumanGeneInteractions-3Iterations-2010-03-30-04:21:58:976.mat
 Predictors.svdApproxOtherSpecies2010-03-30-15:53:18:113.mat
avgScoreDifference: 2.078900e-01 compensatoryDivergence: 59
\end{lstlisting}

\section{Proposed experiments}
Try ideas from the grant proposal.

\subsection{Factor based approaches}
\subsubsection{Smarter choices for phene-phene part of joint matrix}
In all of the factor based approaches, we can consider constructing the joint matrix by using $P^{T}P$ instead of 0; where P is the (1,2) block of the joint matrix.

\paragraph*{Motivation}
We actually have information about relationship between phenes, and good factors for phenes should try to approximate the relationship between phenes, as seen in $P^{T}P$.

Youtube affiliation network link prediction experiments showed a slight improvement in predictor performance if this is done.

\subsubsection{SVP}
The matrix size is not too big, for e.g. the ones used in\\ $SVPi(\sum_i P(1,i)P(1,i)^T,P(1,1))$. So, we can afford to do SVP for more iterations, and see if it improves performance. Earlier experiments on simulated and YouTube data show that prediction performance improves with the number of iterations.

\subsubsection{Low rank approximation perturbed to account for row similarity matrix}
Take P(1, 1), get low rank approximation Y. Do $\min \norm{X - -Y} + \gl tr(XLX^{T})$: don't know how the latter factor works. Idea proposed by zhengdong for his nips paper.

\subsubsection{Using P(i,i)}
Using the matrix
$$X(\gl) = \mat{\gl G & P(1,1) & \dots & P(6,6) \\
           P(1,1)^T & 0 & \dots & 0 \\
           \vdots & \vdots & \ddots & \vdots \\
           P(6,6)^T & 0 & \hdots & 0}$$.

where $G$ is derived from $G(1,1)$ and $G(a,b), \forall a,b$.

\paragraph*{Motivation}
A factor that claims to explain gene $i \in genes(1)$ is influenced by the goodness of factors corresponding to $orth(1,b,i)$ in explaining entries in $P(b, b)$. Also, $\gl$ is required only at G.

\subsection{Path based approaches}
\subsubsection{Learning the parameters}
Learn the parameters, such as $\gb$, through validation.

\subsubsection{Katz on combined matrix C'}
Best parameters learned: $\gb = 10^-2, l_g = l_h = 10^{-1}$.

\paragraph*{Motivation}
This performed better than latent factors based methods in our experiments on affiliation and social networks.

\subsubsection{Supervised Katz}
\tbc


\subsection{Ideas to explore}
Using outlier detection: non-parametric methods require too many data-points. Using logistic regression. 



\section{Questions}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
