+++
title = "AI"
+++

Source: [TW](https://threadreaderapp.com/thread/1642724405745729538.html)

## Architecture
The neural architecture of GPT is v.different from Homo; We just have a lesser idea of what is going on inside the black box when it extracts a degree of logic from a LLM; yet intuitively it is clear that it has converged to having a g-factor just like human intelligence. Thus it is an interesting in silico experiment (contrary to the intuition from the narrow gauge uses like protein structure prediction) which illustrates that such models will evolve an intelligence reflected as g. It should send shudders down the denialist Occidental academics, but they are already training it to deny its own g just like themselves. 

## vIQ
That said, while it illustrates the existence of a g it also shows the bifurcation between mathematical and verbal IQ that exists in human intelligence. Importantly, it shows how much of the former comes for free with the latter. The private integration with Wolfram that is happening does reinforce all those who believe this is a very bad thing waiting to be unleashed on world. 

> Are you saying that verbal intelligence is easier to acquire and can fool one into thinking someone (or some program) is more intelligent then they are while mathematical intelligence is harder to fake?

No -- I'm not saying that vIQ is easier to acquire than mIQ. I think that might vary from model to model even as it does in humans with some compromise between the optimization for one or the other. What I meant was that if you acquire vIQ you get some mIQ for free as GPT shows.

## Virtual Machines?

Source: [TW](https://x.com/blog_supplement/status/1845666136651616313)

Thinking about the future of LLMs and progress in anatomical research in establishing neural connectomes, a thought comes to mind: most neurobiologists and computer scientists (at least per my reading/interactions) seem to think that the software/functions are being run on the machine made up of biological neurons. 

However, in modern computers, we are often running virtual machines on the hardware, each with its own computing environment with an isolated CPU, memory, network interface, and storage. So the question arises if the biological neurons are just the base layer on which multiple virtual machines are running which is where the actual software layer of the brain is running. 

Perhaps this is a simple explanation for many neuropsychological phenomena wherein one or more of the VMs break down or get corrupted. In the least in vertebrate brains, the left and the right cerebral hemispheres seem to be running distinct (but networked at some level) sets of VMs.

## pratyaxa-priyatA
Source: [TW](https://x.com/blog_supplement/status/1872493192811086259)

"parokShapriyA iva hi devAH pratyakSa-dviShaH |" The mind of an autist or schizophrenic losses the sense of parokSha and dwells more on the pratyakSha. It seems training can make a LLM more or less schizophrenic. Subjectively, to us it seems that GPT4o has been made more of a pratyakSha-priya relative to muShkavAn's buddhi over the past 2 years. A part of this seems to arise from purposeful training under constraints aligned with navyonmAda.

## Mental illness
American liberals of Europoid ancestry, especially young women have been found to be the most unhappy people in their cohort. They feature a high frequency individuals receiving some kind of treatment for their mental state or resorting to medicalized mechanisms for self-harm. We see the primary correlate of this as navyonmAda, which was also the motivating force behind the push to put aTTahAsakI in the shvetagR^iha.  

Now imagine the same kind of mindset (very prevalent in big tech) reinforcing LLMs. It suggests that even though the neural architectures of the human and the LLM are very different, the similar reinforcement via an unhealthy memetic disease has comparable end result. This was particularly obvious with guggulu's durbuddhi but is also seen in GPT in the form mentioned in the earlier Twt.



