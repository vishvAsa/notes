\documentclass[10pt]{amsart}
\usepackage{amssymb, graphics, bm, verbatim, algorithm2e, url}

\input{../../macros}

\input{../../amsartMacros}

%opening
\title{Linear Algebra: Answer to Homework 9}
\author{vishvAs vAsuki}

\begin{document}

\maketitle

\section{33.2}
During some step n of Arnoldi iteration, $h_{n+1,n}$ is encountered.

\subsection{a}
\begin{thm}
$AQ_{n} = Q_{n+1}\hat{H}_{n}$ can be simplified to $AQ_{n} = Q_{n}H_{n}$.
\end{thm}
\begin{proof}
As the last row of $\hat{H}_{n}$ is 0, no vector $Aq_{i}$ has a component along $q_{n+1}$. Therefore the simplification holds.
\end{proof}

\begin{rem}
Implications on the structure of the $A = QHQ^{*}$: The submatrix $H_{n+1:m,1:n}$ is 0.
\end{rem}

\subsection{b}
\begin{thm}
 $K_{n}$ is an invariant subspace of A: $AK_{n} \subseteq K_{n}$.
\end{thm}
\begin{proof}
$K_{n} = \linspan{b, .. A_{n-1}b} = \linspan{q_{1}, .. q_{n}}$. So, any $v \in K_{n}$ can be written as $v=Q_{n}x$. So, $Av = AQ_{n}x = Q_{n}H_{n}x$. So, Av is in the column space of $Q_{n}$. So, Av is still in $K_{n}$.

Thus, $AK_{n} \subseteq K_{n}$.
\end{proof}

\subsection{c}
\begin{thm}
 $K_{n} = \linspan{b, .. A^{n-1}b}$. Then, $K_{n} = K_{n+1} = K_{n+2} ..$.
\end{thm}
\begin{proof}
$K_{n+1} = \linspan{K_{n}, A^{n}b}$. But, by the previous theorem, as $A^{n-1}b \in K_{n}$, $A^{n}b = AA^{n-1}b \in K_{n}$. Thus, $K_{n+1} \subseteq K_{n}$. But trivially, $K_{n} \subseteq K_{n+1}$. Thus, $K_{n} = K_{n+1}$.

Using the same argument inductively, we see that $K_{n} = K_{n+1} = K_{n+2} ..$.
\end{proof}

\subsection{d}
\begin{thm}
 Each ew of $H_{n}$ is an ew of A.
\end{thm}
\begin{proof}
As we had previously remarked, $H_{n+1:m,1:n}$ is 0. So, the characteristic polynomial of A, $det(A-lI)$ can be written as $det(H_{1:n,1:n} - lI_{n})det(H_{n+1:m,n+1:m} - lI_{m-n})$.

So, any value of l which causes $det(H_{1:n,1:n} - lI_{n}) = 0$ also causes $det(A-lI)=0$. Thence the result.
\end{proof}

\subsection{e}
\begin{thm}
 If A is non singular, then solution x to Ax=b lies in $K_{n}$.
\end{thm}
\begin{proof}
$K_{n}$ is spanned by columns of $Q_{n}$. Let $Q_{n \perp}$ be a matrix whose columns form an orthonormal basis for the subspace of Range(A) orthogonal to $K_{n}$.

\begin{eqnarray*}
\texttt{Let:} Q_{n}y + Q_{n \perp}y' &=& x\\
Q_{n}y + Q_{n \perp}y' &=& x\\
Q_{n}y - x  &=& -Q_{n \perp}y'\\
AQ_{n}y - Ax  &=& -AQ_{n \perp}y'\\
\end{eqnarray*}

$Q_{n}y$ is in $K_{n}$; and $K_{n}$ being invariant, $AQ_{n}y$ is also in $K_{n}$. Also, $Ax = b = \norm{b}q_{1}$ is also in $K_{n}$. So, $AQ_{n}y - Ax$ is also in $K_{n}$. 

\tbc

\end{proof}


\section{36.1}
\begin{thm}
A is real and symmetric. $r(x) = \frac{x^{T}Ax}{x^{T}x}$. Stationary values of r(x) are the ew of A. Then, Ritz values at step n of the Lancoz iteration are the stationary values of r(x) if x is restricted to $K_{n}$.
\end{thm}
\begin{proof}
If x is restricted to $K_{n}$, it can be written as $Q_{n}y$. Then:

$r(x) = r(Q_{n}y) = \frac{y^{T}Q_{n}^{T}AQ_{n}y}{y^{T}y} = \frac{y^{T}T_{n}y}{y^{T}y}$. Let us denote this by $r'(y)$. We note that this is the Rayleigh quotient for the matrix $T_{n}$, and that, whenever y is an ev this quantity is the corresponding ew.

Following the analysis in the Rayleigh quotients chapter, we see that $\gradient r'(y) = \frac{2}{y^{T}y}(T_{n}y - r'(y)y)$. This is 0 exactly when y is an ev of $T_{n}$ and $r'(y)$ is the corresponding ew of $T_{n}$.

So, stationery values of r'(y) and r(x) restricted to x of the form $x = Q_{n}y$ are exactly the same: $r'(y) = 0 \equiv r(Q_{n}y) = 0 \equiv r(x) = 0$.
\end{proof}

\section{38.5}
Minimizing $f(x) = 2^{-1}x^{T}Ax - x^{T}b$ using steepest descent: $p_{n} = r_{n}$.

\subsection{a}
\begin{thm}
$\gradient f(x) = -r$.
\end{thm}
\begin{proof}
\begin{eqnarray*}
f(x) &=& 2^{-1}x^{T}Ax - x^{T}b\\
\gradient f(x) &=& \gradient 2^{-1}x^{T}Ax - \gradient x^{T}b\\
 &=& Ax - b\\
 &=& -r\\
\end{eqnarray*}
\end{proof}

\subsection{b}
\begin{thm}
Optimal step $a_{n} = \frac{r_{n-1}^{T}r_{n-1}}{r_{n-1}^{T}Ar_{n-1}}$.
\end{thm}
\begin{proof}
We want to find an optimal $a_{n}$ which minimize $f(x_{n})$. Our search direction is $r_{n-1}$; so we want to find $x_{n} = x_{n-1} + a_{n}r_{n-1}$.

\begin{eqnarray*}
\gradient f(x_{n}) &=& 0\\
Ax_{n} - b  &=& 0\\
A(x_{n-1} + a_{n}r_{n-1}) - b  &=& 0\\
b - Ax_{n-1}  &=& a_{n}Ar_{n-1}\\
r_{n-1} &=& a_{n}Ar_{n-1}\\
a_{n} &=& \frac{r_{n-1}^{T}r_{n-1}}{r_{n-1}^{T}Ar_{n-1}}\\
\end{eqnarray*}
\end{proof}

\subsection{c}
The full steepest descent iteration:

\begin{algorithm}[H]
$x_{0}=0, r_{0}=b$.\\
\ForEach{n = 1,2 ..}{
$a_{n} = \frac{r_{n-1}^{T}r_{n-1}}{r_{n-1}^{T}Ar_{n-1}}$\\
$x_{n} = x_{n-1} + a_{n}r_{n-1}$\\
$r_{n} = b-Ax_{n}$\\
}
\end{algorithm}


\section{38.6}

\tbc

\section{}
Following link provides a data structure to store sparse matrices:

\url{www.cs.utexas.edu/~inderjit/courses/cs383c/sparse_matrices.txt}

Write a matlab code using the above specified data structure to compute the matrix-vector product $y=Ax$ in O($nz$), where $nz$ is the number of non-zeros in the sparse matrix $A$. Also write a matlab code to compute $y=A^Tx$ in O($nz$). Note that you are not allowed to store $A^T$ into a new matrix. 


\tbc


% 
% \bibliographystyle{plain}
% \bibliography{../linAlg}

\end{document}

