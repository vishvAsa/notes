\def\CliqueSet{\mathcal{C}}
\def\clique{C}

\section{Problem Setup and Notation}

\myparagraph{MRFs and their Parameterization} We consider the task of estimating the graph structure associated with a general discrete Markov random field. Let $X = (X_1,\hdots,X_p)$ be a random vector, each variable $X_i$ taking values in a discrete set $\mathcal{X}= \{1,2, \ldots, \statenum\}$ of cardinality $m$. Let $G = (V,E)$ denote a graph with $p$ nodes,  corresponding to the $p$ variables $\{X_1,\hdots,X_p\}$. Let $\CliqueSet$ be a set of cliques (fully-connected subgraphs) of the graph $G$, and let $\{ \phi_{\clique} : \mathcal{X}^{|\clique|} \mapsto \real, \; \clique \in \CliqueSet \}$ be a set of ``clique potential'' functions. With this notation, the distribution of $X$ takes the form
\begin{eqnarray}
\label{EqnGenMRFPot}
\mprob(x) & \propto & \exp \left \{\sum_{\clique \in \CliqueSet} \phi_\clique(x_\clique) \right \}.
\end{eqnarray}
\noindent Since $\mathcal{X}$ is discrete, each potential function $\phi_{\clique}$
can be parameterized as linear combinations of $\{0,1\}$-valued indicator functions -- one for each configuration of $x_\clique$.  
For each $s \in \vertex$ and $j \in \{1, \ldots, \statenum-1\}$, we can define node-wise indicators, 
\begin{eqnarray*}
\I[x_{s} = j] & = & \begin{cases} 1 & \mbox{if $x_s =j$} \\ 
											0 & \mbox{otherwise.}
                    \end{cases}
\end{eqnarray*}
Note that we omit an indicator for $x_s = \statenum$ from the list, since
it is redundant given the indicators for $j = 1, \ldots, \statenum-1$.
In a similar fashion, we can define the $|\clique|$-way clique-wise indicator functions 
$\I[x_\clique = v]$, for $v \in \{1,2,\ldots, \statenum - 1\}^{|\clique|}$.

\noindent With this notation, any set of potential functions can then be written as
\begin{eqnarray*}
\phi_{\clique}(x_\clique) & = & \sum_{v \in \{1, \ldots, \statenum-1 \}^{|\clique|}}
	\theta^*_{\clique;v} \; \I[x_\clique = v] \quad \mbox{for $\clique \in \CliqueSet$}
\end{eqnarray*}
Thus, \eqref{EqnGenMRFPot} can be rewritten as,
\begin{align}
\label{EqnGenMRF}
\mprob_{\theta^*}(x) & \propto & \exp \biggr \{\sum_{\clique \in \CliqueSet; v \in \{1, \ldots, \statenum-1 \}^{|\clique|}} \theta^*_{\clique;v} \, \I[x_\clique = v] \biggr \}.
\end{align}
Thus, the Markov random field can be parameterized in terms of the collection of tensors $\theta^* := \{\theta^*_{\clique;v} \; \clique \in \CliqueSet; v \in \{1, \ldots, \statenum-1 \}^{|\clique|}\}$. In the sequel, it will be useful to collate these into vectors $\theta^*_{\clique} \in \real^{(\statenum-1)^{|\clique|}}$ associated with the cliques $\clique \in \CliqueSet$. 

\myparagraph{Pairwise Markov Random Fields} Here the set of cliques consists of the set of nodes $V$ and the set of edges $E$. Thus,
using nodewise and pairwise indicator functions as before, any pairwise MRF over $(X_1, \ldots, X_\pdim)$
can be expressed as
\begin{align}
\label{EqnGenMRFPairwise}
\nonumber \mathbb{P}(x)  &\propto  \exp \biggr \{\sum_{s \in \vertex; j \in \{1, \ldots, \statenum-1 \}} \theta_{s;j}\I[x_s = j] 
	\\
& + \sum_{(s,t) \in \edge; j,k \in \{1, \ldots, \statenum-1 \}} \theta_{st;jk} \I[x_s = j,\,x_t = k] \biggr \},
\end{align}
for a set of parameters $\theta^* := \{\theta^*_{s;j}, \theta^*_{st;jk} :\, s,t \in V;\, (s,t) \in E;\, j,k \in \{1, \ldots, \statenum-1 \}\}$.
It will be useful to collate these into vectors $\theta^*_s \in \real^{\statenum-1}$ for each $s \in \vertex$, and the vectors $\theta^*_{st} \in \real^{(\statenum-1)^2}$ associated with each edge.

\myparagraph{Graphical Model Selection} Suppose that we are given a collection $\Data \defn \{x^{(1)}, \ldots, x^{(\numobs)} \}$ of $\numobs$ samples, where each
$\pdim$-dimensional vector  $x^{(i)} \in \{1,\ldots,\statenum\}^\pdim$ is drawn i.i.d. from a distribution $\mprob_{\theta^*}$ of the
form~\eqref{EqnGenMRF}, for parameters $\theta^*$ and graph $\graph = (\vertex, \edge^*)$ over the $\pdim$ variables. 
The goal of \emph{graphical model selection} is to infer the edge set $\edge^*$ of the graphical model defining the probability distribution that generates the samples. Note that the true edge set $\edge^*$ can also be expressed as a function of the parameters as
\begin{eqnarray}
\label{EqnEdge}
\edge^* = \{(s,t) \in V \times V :\, \exists \, \clique \in \CliqueSet; \{s,t\} \in \clique;\; \theta^*_{\clique} \neq 0 \}.
\end{eqnarray}
In this paper, we focus largely on the special case of pairwise Markov random fields.

\subsection{Pairwise Model Selection}

%Recall that a pairwise MRF is parameterized by vectors $\theta^*_s \in \real^{\statenum-1}$ for each $s \in \vertex$, and the vectors $\theta^*_{st} \in \real^{(\statenum-1)^2}$ associated with each edge $(s,t) \in E^*$.  It is convenient to view the parameter vector $\theta^*$ as a collection of $\binom{\pdim}{2}$ vectors in  $\real^{\statenum-1}$ indexed by pairs of distinct vertices,  but non-zero if and only if the vertex pair $(s,t)$ belongs to the unknown edge set $\edge^*$ of the underlying graph $\graph$. 

We now describe the graph selection procedure we study for the $m$-ary pairwise model. It is the natural generalization of the procedures for binary graphical models~\citep{RWLIsing} and Gaussian graphical models~\citep{Meinshausen:06}.  Specifically, we first focus on recovering the neighborhood of a fixed vertex $\svert \in V$, and then combine the neighborhood sets across vertices to form the graph estimate.

\noindent Let us define the vector $\Theta^*_{\backslash \svert} \in \real^{(\statenum-1)^2(\pdim-1)}$, which is the concatenation of $(\pdim -1)$ {\em groups} -- i.e. one (short) vector $\theta^*_{\svert t} \in \real^{(\statenum-1)^2}$ for each $t\in\vertex\backslash\{\svert\}$. Note that $\svert$ having a small neighborhood is equivalent to many of these vectors $\theta^*_{\svert t}$ being zero; in particular, the problem of neighborhood estimation for vertex $\svert$ corresponds to the recovery of the set
\begin{eqnarray*}
\N(\svert) & = & \biggr \{ u \in \vertex \backslash \{\svert\} \, \mid
\, \|\theta^*_{\svert u}\|_0 \neq 0 \biggr \}.
\end{eqnarray*}
This is precisely the structure captured by \emph{group-sparsity}. In particular, each $\theta^*_{\svert t}$, with $t\in\vertex\backslash\{\svert\}$, corresponds to a group; if $\svert$ has a small neighborhood,only few of these groups will be non-zero. 

In order to estimate the neighborhood $\N(\svert)$, we thus perform a regression of $X_\svert$ on the rest of the variables $X_{\backslash \svert}$, using the group-sparse regularizer $\norm{\Theta_{\backslash \svert}}{1,2} \, \defn \, \sum_{u \in \vertex \backslash \{\svert\}} \|\theta_{\svert u}\|_2$. The conditional distribution of $X_\svert$ given the other variables $X_{\backslash \svert} = \{X_{t}\;|\; t \in V \backslash \{\svert\}\}$ takes the form
\begin{align}
\label{EqnMultiClassLogistic}
\nonumber & \mathbb{P}_{\Theta^*} \big[X_\svert = j \, \mid X_{\backslash \svert} = x_{\backslash \svert}\big] = \\
& \;\; \frac{\exp\left(\theta_{\svert;j}^* + \sum_{t \in V\backslash \{\svert\}} \sum_{k} \theta^*_{\svert t;jk} \I[x_t = k]\right)} {1+\sum_{\ell}\exp\left(\theta_{\svert;\ell}^* + \sum_{t \in V\backslash \{\svert\}} \sum_{k} \theta^*_{\svert t;\ell k} \I[x_t = k]\right)},
\end{align}
for all $j \in \{1,\hdots,m-1\}$. Thus, $X_\svert$ can be viewed as the response variable in a multiclass
logistic regression, in which the indicator functions associated with
the other variables
\begin{equation*}
\Big\{\I[x_t = k] ,\,t \in V \backslash \{\svert\}, \; k\in \{1, 2, \ldots,
\statenum-1\}\Big\},
\end{equation*}
play the role of the covariates.

Thus, we study the following convex program as an estimate for $\Theta^*_r$
\begin{equation}
\label{EqnPairwiseGroup}
\hat{\Theta}_{\backslash \svert} \in \min_{\Theta_{\backslash \svert} \in \real^{(\statenum-1)^2(\pdim-1)}} \biggr\{ \ell(\Theta_{\backslash \svert}; \Data) +
	\regpar_\numobs \norm{\Theta_{\backslash \svert}}{1,2}
	\biggr\},
\end{equation}
where $\ell(\Theta_{\backslash \svert}; \Data) = \frac{1}{\numobs} \sum_{i=1}^\numobs \ell^{(i)}(\Theta_{\backslash \svert}; \Data) \defn
\frac{1}{\numobs} \sum_{i=1}^\numobs \log \mathbb{P}_{\Theta}
\left[X_\svert = x^{(i)}_{\svert} \, \mid \, X_{\backslash\svert} = x^{(i)}_{\backslash \svert} \right]$
is the rescaled multiclass logistic likelihood defined by the conditional distribution~\eqref{EqnMultiClassLogistic}, and
$\regpar_\numobs > 0$ is a regularization parameter. The convex program~\eqref{EqnPairwiseGroup} is an $\ell_1/\ell_2$-regularized multiclass logistic regression problem, and is thus the multiclass logistic analog of the group Lasso~\citep{YuaLi06}.

The solution to the program~\eqref{EqnPairwiseGroup} yields an estimate $\widehat{\N}(\svert)$ of the neighborhood of node $r$ by 
\begin{align*}
	\widehat{\N}(\svert) = \{t \in \vertex : t \neq r ; \| \hat{\theta}_{rt} \|_2 \neq 0\}. 
\end{align*}

We are interested in the event that all the node neighboorhoods are estimated exactly, $\{\widehat{\N}(\svert) = \N(\svert); \, \forall \svert \in \vertex\}$,
which we also write as $\{\hat{E} = E^*\}$ since it entails that the the full graph is estimated exactly.

\myparagraph{Sparsistency} Our main result is a high-dimensional analysis of the estimator~\eqref{EqnPairwiseGroup}, where allow the problems dimensions such as the number of nodes $p$,
the maximum node degree $d$, the size of the state space $m$ (and in the case of higher-order MRFs, the maximum clique size $c$) to vary with the number of observations $n$. Our goal is to establish sufficient conditions on the scaling of $(\numobs, \pdim, \degmax, \statenum, c)$ such that our proposed estimator is consistent in the sense that
\begin{eqnarray*}
\mprob \left[\widehat{\edge}_\numobs = \edge^* \right] & \rightarrow &
1 \qquad \mbox{as $\numobs \rightarrow +\infty$}.
\end{eqnarray*}
We sometimes call this property sparsistency, as a shorthand for consistency of the sparsity pattern of the parameters.

\subsection{Higher-order Model Selection}

\myparagraph{Natural, high-complexity Extension} Let us first see what this model selection recipe of node-wise regression with group-sparse regularization, would entail when extended to the general higher-order Markov random fields~\eqref{EqnGenMRF} case.  Recall that such a higher-order MRF  is parameterized by vectors $\theta^*_{C} \in \real^{(\statenum - 1)^{|C|}}$ for $C \in \mathcal{C}$. Let $\maxcliquesize$ be the maximum clique size. It would be convenient to view the parameters as a collection of $\sum_{j=1}^{c} \binom{p}{j}$ vectors indexed by a cliques $C$ of size less than or equal to $\maxcliquesize$, but non-zero if and only if the clique $C \in \mathcal{C}$. 

Again, we fix a node $r$, and define the long vector $\Theta^*_{\backslash \svert} \in \real^{\sum_{j=1}^{c-1} \binom{p-1}{j} (m-1)^{j+1}}$ as the concatenation of the parameter vectors $\theta^*_{rC}$ for all $C \subseteq \vertex \backslash \svert;\, |C| < c$. Note that recovery of the neighborhood of a vertex $\svert$ corresponds to the recovery of the set
\begin{eqnarray*}
\N(\svert) & = & \biggr \{ u \in \vertex \backslash \{\svert\} \, \mid
\, \exists \, C \subseteq \vertex \backslash \{\svert,u\} ;\; \|\theta^*_{\svert u C}\|_0 \neq 0 \biggr \}.
\end{eqnarray*}
Thus, we could again make use of group sparsity where in this case, the groups of parameters are the parameter vectors $\theta^*_{rC}$ for different $C \subseteq \vertex \backslash \svert;\, |C| < c$. We can then see that a small neighborhood $\N(\svert)$ for node $r$ entails that $\Theta^*_{\backslash \svert}$ will have many of these groups be zero. The group-structured penalty would then take the form $\|\Theta^*_{\backslash \svert}\|_{1,2} := \sum_{\{C \subseteq  \vertex \backslash \svert\, |C| < c\}} \|\theta^*_{rC}\|_{2}$.


Thus we would solve:
\begin{equation}
\label{EqnGroupHigherOrder}
\min_{\Theta_{\backslash \svert} \in \real^{\sum_{j=1}^{c-1} \binom{p-1}{j} (m-1)^{j+1}}} \biggr\{ \ell(\Theta_{\backslash \svert}; \Data) +
	\regpar_\numobs \norm{\Theta_{\backslash \svert}}{1,2} \biggr\},
\end{equation}

where $\ell(\Theta_{\backslash \svert}; \Data)$ is the likelihood of the data as before. \citet{Dahinden07,Dahinden10} studied the related program of $\ell_1/\ell_2$ regularized maximum likelihood over the complete graph (instead of node-wise regressions) but showed good empirical performance of discrete graphical model structure recovery. The caveat with the higher-order group-sparse approach is the prohibitive computational complexity of this procedure. Note that the number of parameters is $\sum_{j=1}^{c-1} \binom{p-1}{j} (m-1)^{j+1}$ which scales prohibitively even for moderate $c$. Indeed, even the computations in the pairwise case are not inexpensive.

\myparagraph{Sparsistency of a Simpler Estimate}
But as we show in Section~\ref{SecHigherOrder}, even when the underlying model is a higher order MRF, surprisingly just solving the pairwise program~\eqref{EqnPairwiseGroup} is \emph{sufficient} to recover the true edges, under certain conditions. Thus, in our second main result, we again analyze the sparsistency of the estimator in \eqref{EqnPairwiseGroup}, but for the case where the underlying graph is a higher-order MRF.



\subsection{Notation}

We use the following notation for group-structured norms. For any vector $u \in \real^p$ where $\{1,\hdots,p\}$ is partitioned into a set of $T$ disjoint groups $\mathcal{G} = \{G_1,\hdots,G_T\}$, we define $\|u\|_{\mathcal{G},a,b} = \|(\|u_{G_1}\|_{a},\hdots, u_{G_T}\|_{a})\|_b$. In our case, for the pairwise model, the nodewise regression has the parameter vector $\Theta^*_{\backslash \svert} \in \real^{(\statenum-1)^2(\pdim-1)}$. Its groups are collated on the edges:  $\mathcal{G} = \{ \mathcal{G}_{rs}; s \in  \vertex \backslash \svert\}$ where $\mathcal{G}_{rt}$ is the index set of parameters on the  $(r,t)$ edge, $\{\theta_{rt;jk} ;\; j,k \in \{1,\hdots,\statenum - 1\}\}$.

Similarly, suppose $\Theta^*_{\backslash \svert}$ is the nodewise regression parameter for the higher-order model case. Then its groups are collated on the cliques: $\mathcal{G} = \{ \mathcal{G}_{rC}; C \subseteq  \vertex \backslash \svert\, |C| < c\}$, where $\mathcal{G}_{rC}$ is the index set of parameters on the  ${r} \cup C$ clique, $\{\theta_{rC;jv} ;\; j \in \{1,\hdots,\statenum - 1\}; v \in \{1,\hdots,\statenum - 1\}^{|C|}\}$. In the sequel, we will suppress the dependence of the group norms on these group partitions $\mathcal{G}$ when it is clear from context, so that we will simply use 
$\|\Theta^*_{\backslash \svert}\|_{a,b}$ for $\|\Theta^*_{\backslash \svert}\|_{\mathcal{G},a,b}$.

We will be focusing on the choice $a = 1, b = 2$ which yields the group-lasso penalty~\citep{YuaLi06}. For a matrix $M \in \real^{p \times p}$, and denoting the $i$-th row of $M$ by $M^i$, 
we can define the analogs of the group-structured norms on matrices: $\|M\|_{(a,b),(c,d)} := \|(\|M^{1}\|_{c,d},\hdots, \|M^{p}\|_{c,d})\|_{a,b}$. In our analysis, we will always use $b = d = 2$, so that we use the minimized notation: $\|M\|_{a,c}$ to denote $\|M\|_{(a,2),(c,2)}$.




