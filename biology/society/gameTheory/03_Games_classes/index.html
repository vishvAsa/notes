<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Vishvas&#39;s notes  | 03 Games: classes</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.72.0" />
    
    
      <META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
    

    <link href="/storage/emulated/0/notesData/notes/biology/society/gameTheory/03_Games_classes/" rel="alternate" type="application/rss+xml" title="Vishvas&#39;s notes" />
    <link href="/storage/emulated/0/notesData/notes/biology/society/gameTheory/03_Games_classes/" rel="feed" type="application/rss+xml" title="Vishvas&#39;s notes" />

    <meta property="og:title" content="03 Games: classes" />
<meta property="og:description" content="2 player games Aka bimatrix game. Row and column players: \(p_{r}, p_{c}\); their Prob distr over \(S_r\) and \(S_{c}\) as vectors :\(D_{r}, D_{c}\). Utility matrix wrt \(p_{r}\) and \(p_{c}\): R, C.
2 - Player 0 sum games Utility matrix \(A_{i,j} = u_{r}(..)\). (Nash) Eg: Matching pennies.
Value paid by \(p_{c}\) to \(p_{r}\): \(v_{r} = D_{r}^{*} AD_{c}\).
Knowing \(D_{r}\), \(p_{c}\) always selects \(min(D_{r}^{*}A)\) or finds \\(v_{c} = \min_{k_{1}} \max_{D_{r}} E_{k_{r} \distr D_{r}}[u_{1}(k_{1}, k_{2})] = \min_{k_{1}} \max_{D_{r}} u_{1}(k_{1},D_{r})\)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="file:///storage/emulated/0/notesData/notes/biology/society/gameTheory/03_Games_classes/" />

<meta itemprop="name" content="03 Games: classes">
<meta itemprop="description" content="2 player games Aka bimatrix game. Row and column players: \(p_{r}, p_{c}\); their Prob distr over \(S_r\) and \(S_{c}\) as vectors :\(D_{r}, D_{c}\). Utility matrix wrt \(p_{r}\) and \(p_{c}\): R, C.
2 - Player 0 sum games Utility matrix \(A_{i,j} = u_{r}(..)\). (Nash) Eg: Matching pennies.
Value paid by \(p_{c}\) to \(p_{r}\): \(v_{r} = D_{r}^{*} AD_{c}\).
Knowing \(D_{r}\), \(p_{c}\) always selects \(min(D_{r}^{*}A)\) or finds \\(v_{c} = \min_{k_{1}} \max_{D_{r}} E_{k_{r} \distr D_{r}}[u_{1}(k_{1}, k_{2})] = \min_{k_{1}} \max_{D_{r}} u_{1}(k_{1},D_{r})\).">

<meta itemprop="wordCount" content="1850">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="03 Games: classes"/>
<meta name="twitter:description" content="2 player games Aka bimatrix game. Row and column players: \(p_{r}, p_{c}\); their Prob distr over \(S_r\) and \(S_{c}\) as vectors :\(D_{r}, D_{c}\). Utility matrix wrt \(p_{r}\) and \(p_{c}\): R, C.
2 - Player 0 sum games Utility matrix \(A_{i,j} = u_{r}(..)\). (Nash) Eg: Matching pennies.
Value paid by \(p_{c}\) to \(p_{r}\): \(v_{r} = D_{r}^{*} AD_{c}\).
Knowing \(D_{r}\), \(p_{c}\) always selects \(min(D_{r}^{*}A)\) or finds \\(v_{c} = \min_{k_{1}} \max_{D_{r}} E_{k_{r} \distr D_{r}}[u_{1}(k_{1}, k_{2})] = \min_{k_{1}} \max_{D_{r}} u_{1}(k_{1},D_{r})\)."/>

      
    

    <script src="/storage/emulated/0/notesData/notes/webpack_dist/dir_tree-bundle.js"></script>
    <script type="text/javascript">
    
    let baseURL = "file:\/\/\/storage\/emulated\/0\/notesData\/notes";
    let basePath = "/" + baseURL.split("/").slice(3).join("/");
    let siteParams = JSON.parse("{\u0022contactlink\u0022:\u0022https:\/\/github.com\/vvasuki\/notes\/issues\/new\u0022,\u0022disqusshortcode\u0022:\u0022vvasuki-site\u0022,\u0022env\u0022:\u0022production\u0022,\u0022githubeditmepathbase\u0022:\u0022https:\/\/github.com\/vvasuki\/notes\/edit\/master\/content\/\u0022,\u0022mainSections\u0022:[\u0022math\u0022],\u0022mainsections\u0022:[\u0022math\u0022]}");
    let pageDefaultsList = JSON.parse("[{\u0022scope\u0022:{\u0022pathPrefix\u0022:\u0022\u0022},\u0022values\u0022:{\u0022comments\u0022:true,\u0022layout\u0022:\u0022page\u0022,\u0022search\u0022:true,\u0022sidebar\u0022:\u0022home_sidebar\u0022}}]");
    let sidebarsData = JSON.parse("{\u0022home_sidebar\u0022:{\u0022contents\u0022:[{\u0022url\u0022:\u0022recdir:\/\/\u0022},{\u0022contents\u0022:[{\u0022title\u0022:\u0022ज्यौतिषम्\u0022,\u0022url\u0022:\u0022..\/jyotiSham\/\u0022},{\u0022title\u0022:\u0022संस्कृतम्\u0022,\u0022url\u0022:\u0022..\/sanskrit\/\u0022},{\u0022title\u0022:\u0022मीमांसा\u0022,\u0022url\u0022:\u0022..\/mImAMsA\/\u0022},{\u0022title\u0022:\u0022Notes Home\u0022,\u0022url\u0022:\u0022..\/notes\/\u0022},{\u0022title\u0022:\u0022काव्यम्\u0022,\u0022url\u0022:\u0022..\/kAvya\/\u0022},{\u0022title\u0022:\u0022संस्काराः\u0022,\u0022url\u0022:\u0022\/..\/saMskAra\/\u0022},{\u0022title\u0022:\u0022Vishvas\u0027s home page\u0022,\u0022url\u0022:\u0022\/..\/\u0022}],\u0022title\u0022:\u0022सङ्ग्रहान्तरम्\u0022},{\u0022title\u0022:\u0022\\u003ci class=\\\u0022fas fa-hand-holding-heart\\\u0022\\u003e\\u003c\/i\\u003eDonate Via Vishvas\u0022,\u0022url\u0022:\u0022https:\/\/vvasuki.github.io\/interests\/dharma-via-vishvas\/\u0022}],\u0022title\u0022:\u0022Parts\u0022}}");
    let autocompletePageUrl = "\/storage\/emulated\/0\/notesData\/notes\/data\/pages.tsv";
    
    var pageRelUrlTree = {};
</script>

    <script>
    
    let pageVars = {};
    pageVars.pageUrlMinusBasePath = "\/storage\/emulated\/0\/notesData\/notes\/biology\/society\/gameTheory\/03_Games_classes\/".replace(basePath, "/");
    pageVars.pageParams = {};
    pageVars.pageSource = "biology\/society\/gameTheory\/03_Games_classes.md";
    console.log(pageVars.pageSource);
    var pageDefaults;
    for (let possiblePageDefaults of pageDefaultsList) {
      if (pageVars.pageSource.startsWith(possiblePageDefaults.scope.pathPrefix)) {
        pageDefaults = possiblePageDefaults.values
      }
    }
    
    </script>
    <script src="/storage/emulated/0/notesData/notes/webpack_dist/main-bundle.js"></script>
    <script src="/storage/emulated/0/notesData/notes/webpack_dist/transliteration-bundle.js"></script>
    <script src="/storage/emulated/0/notesData/notes/non_webpack_js/disqus.js"></script>
    <script src="/storage/emulated/0/notesData/notes/webpack_dist/ui_lib-bundle.js"></script>
    <link rel="stylesheet" href="/storage/emulated/0/notesData/notes/css/fonts.css">
    <link rel="stylesheet" href="/storage/emulated/0/notesData/notes/css/@fortawesome/fontawesome-free/css/solid.min.css">
    

    <link rel="stylesheet" href="/storage/emulated/0/notesData/notes/css/@fortawesome/fontawesome-free/css/fontawesome.min.css">

    
    <link rel="alternate" hreflang="sa-Deva" href="#ZgotmplZ" />
    <link rel="alternate" hreflang="sa-Deva" href="#ZgotmplZ?transliteration_target=devanagari" />
    <link rel="alternate" hreflang="sa-Knda" href="#ZgotmplZ?transliteration_target=kannada" />
    <link rel="alternate" hreflang="sa-Mlym" href="#ZgotmplZ?transliteration_target=malayalam" />
    <link rel="alternate" hreflang="sa-Telu" href="#ZgotmplZ?transliteration_target=telugu" />
    <link rel="alternate" hreflang="sa-Taml-t-sa-Taml-m0-superscript" href="#ZgotmplZ?transliteration_target=tamil_superscripted" />
    <link rel="alternate" hreflang="sa-Taml" href="#ZgotmplZ?transliteration_target=tamil" />
    <link rel="alternate" hreflang="sa-Gran" href="#ZgotmplZ?transliteration_target=grantha" />
    <link rel="alternate" hreflang="sa-Gujr" href="#ZgotmplZ?transliteration_target=gujarati" />
    <link rel="alternate" hreflang="sa-Orya" href="#ZgotmplZ?transliteration_target=oriya" />
    <link rel="alternate" hreflang="sa-Beng-t-sa-Beng-m0-assamese" href="#ZgotmplZ?transliteration_target=assamese" />
    <link rel="alternate" hreflang="sa-Beng" href="#ZgotmplZ?transliteration_target=bengali" />
    <link rel="alternate" hreflang="sa-Guru" href="#ZgotmplZ?transliteration_target=gurmukhi" />
    <link rel="alternate" hreflang="sa-Cyrl" href="#ZgotmplZ?transliteration_target=cyrillic" />
    <link rel="alternate" hreflang="sa-Sinh" href="#ZgotmplZ?transliteration_target=sinhala" />
    <link rel="alternate" hreflang="sa-Shar" href="#ZgotmplZ?transliteration_target=sharada" />
    <link rel="alternate" hreflang="sa-Brah" href="#ZgotmplZ?transliteration_target=brahmi" />
    <link rel="alternate" hreflang="sa-Modi" href="#ZgotmplZ?transliteration_target=modi" />
    <link rel="alternate" hreflang="sa-Tirh" href="#ZgotmplZ?transliteration_target=tirhuta_maithili" />
    <link rel="alternate" hreflang="sa-Latn-t-sa-Zyyy-m0-iast" href="#ZgotmplZ?transliteration_target=iast" />
  </head>

  <body class="ma0 bg-near-white">
    


    
<header class="p-1 bg-yellow">
    <nav role="navigation">
      <div id="div_top_bar" class="row">
        <div class="col-md-3 justify-content-start">
          <a  href="/storage/emulated/0/notesData/notes/" class="btn col border">
            <i class="fas fa-gopuram ">Vishvas&#39;s notes <br/> 03 Games: classes</i>
          </a>
        </div>
        <div id="div_top_bar_right" class="col-md-auto d-flex justify-content-center">
          <form action="/storage/emulated/0/notesData/notes/search">
            <input id="titleSearchInputBox" placeholder="शीर्षिकान्विष्यताम्" name="s"><i class="btn fas fa-search "></i>
          </form>
          <a href="/storage/emulated/0/notesData/notes/custom_site_search/"  class="btn btn-default">Full search</a>
          <select name="transliterationDropdown" size="1" onchange="module_transliteration.updateTransliteration()">
            <option value="devanagari" selected="">स</option>
            <option value="iast">ā</option>
            <option value="kannada">ಅ</option>
            <option value="malayalam">അ</option>
            <option value="telugu">క</option>
            <option value="tamil_superscripted">க²</option>
            <option value="tamil_extended">க</option>
            <option value="grantha">𑌅</option>
            <option value="gujarati">અ</option>
            <option value="oriya">ଅ</option>
            <option value="assamese">অস</option>
            <option value="bengali">অ</option>
            <option value="gurmukhi">ਅ</option>
            <option value="cyrillic">пу</option>
            <option value="sinhala">අ</option>
            <option value="sharada">𑆑𑇀𑆰</option>
            <option value="brahmi">𑀅</option>
            <option value="modi">𑘦𑘻𑘚𑘲</option>
            <option value="tirhuta_maithili">𑒁</option>
          </select>
        </div>
        <div class="row col  d-flex justify-content-center">
          <div><a  name="previousPage" href="" class="btn btn-secondary">###<i class="fas fa-caret-left"></i></a></div>
          <div ><a name="nextPage" href="" class="btn btn-secondary"><i class="fas fa-caret-right"></i> ### </a></div>
        </div>
        <ul id="top-bar-right-custom" class="list-group list-group-horizontal">
        </ul>
      </div>
    </nav>
</header>

    
    <div class="row" name="contentRow">
      
      <aside class="col-md-3 card border " id="sidebar">
        <div id="sidebarTitle" class="card-title bg-light-gray border d-flex justify-content-between" >
          <a name="sidebarToggleLink" data-toggle="collapse" href="#sidebar_body" role="button" aria-expanded="true" aria-controls="sidebar_body" onclick="module_main.default.sidebarToggleHandler()">
            Menu <i class="fas fa-caret-down"></i></a>
        </div>
        <nav class="card-body p-0 collapse show" id="sidebar_body">
          <ul id="displayed_sidebar" class="list pl2 p-2 bg-yellow">
        </ul>
        </nav>
      </aside>
      <main class="col p-3" role="main">
        
<header class='border d-flex justify-content-between'>
    <h1 id="03 Games: classes">03 Games: classes</h1>
    
    <a id="editLink" class="btn btn-primary"  href="https://github.com/vvasuki/notes/edit/master/content/biology/society/gameTheory/03_Games_classes.md"><i class="fas fa-edit"></i></a>
    
</header>
<article>
  <aside id="toc_card" class="card border ">
    <div id="toc_header" class="card-title border d-flex justify-content-between">
        <a data-toggle="collapse" href="#toc_body" role="button" aria-expanded="true" aria-controls="toc_body">
          What's in this page? <i class="fas fa-caret-down"></i> </a>
    </div>
    <div id="toc_body" class="card-body collapse p-0">
      
      <ul id="toc_ul" class="list p-0">
      </ul>
    </div>
  </aside>
  <div id="post_content">
  <h2 id="2-player-games">2 player games</h2>
<p>Aka bimatrix game. Row and column players: \(p_{r}, p_{c}\); their Prob distr over \(S_r\) and \(S_{c}\) as vectors :\(D_{r}, D_{c}\). Utility matrix wrt \(p_{r}\) and \(p_{c}\): R, C.</p>
<h3 id="2---player-0-sum-games">2 - Player 0 sum games</h3>
<p>Utility matrix \(A_{i,j} = u_{r}(..)\). (Nash) Eg: Matching pennies.</p>
<p>Value paid by \(p_{c}\) to \(p_{r}\): \(v_{r} = D_{r}^{*} AD_{c}\).</p>
<p>Knowing \(D_{r}\), \(p_{c}\) always selects \(min(D_{r}^{*}A)\) or finds \\(v_{c} = \min_{k_{1}} \max_{D_{r}} E_{k_{r} \distr D_{r}}[u_{1}(k_{1}, k_{2})] = \min_{k_{1}} \max_{D_{r}} u_{1}(k_{1},D_{r})\). So, best strategy for \(p_{r}\) is to maximize (Maxmin) \(min(D_{r}^{*}A)\).</p>
<h4 id="solution-by-linear-program">Solution by linear program</h4>
<p>\(v_{r} = max\ v; D_{r}&gt;0; \sum_{i} D_{r,i} = 1; (D_{r}^{*}A)_{j} \geq v \forall j\).</p>
<p>Dual of this LP finds \(v_{c} = -v_{r}\) and \(D_{c}\): aka Minmax / minimax theorem (Neumann).</p>
<h4 id="reduction-from-constant-k-sum-2-player-games">Reduction from constant (k) sum 2 player games</h4>
<p>Use a equivalent utility matrix \(A_{i,j} = u_{r}(..)\). So, any constant sum game has well defined value: \((v_{r}, k-v_{r})\).</p>
<h3 id="symmetric-two-player-games">Symmetric two player games</h3>
<p>R, C are same. \(D_{c}\) is the best response to itself. \(D_{c} = D_{r} = D\).</p>
<h4 id="finding-nash-equilibrium">Finding Nash Equilibrium</h4>
<p>(Lemke - Howson). Consider inequalities \(Ax \leq 1\), \(x \geq 0\); visualize 2d case like an LP: intersecting halfspaces in a plane with axes \(\set{x_{i}}\). \(\binom{2n}{n}\) verteces in n-d polytope.</p>
<p>Solution pt must lie in some vertex where payoff is maximum; at solution pt, \(\forall i\): \(A_{i}x = 1\) and \(i \in D\) by prop of Nash equilib, or \(x_{i} = 0\) and \(i \notin D\). To get the final strategy, take x, and scale it so that \(\sum x_{i}=1\). Move from vertex to vertex by relaxing constraints and moving along edges.</p>
<p>Almost always runs in poly time. (Smoothed complexity.)</p>
<h4 id="reduction-from-general-2-player-games">Reduction from general 2 player games</h4>
<p>R and C are \(m\times n\). Make symmetric game \(\mat{0 &amp; R\C^{T} &amp; 0}\); Find solution distribution \(\mat{x\y}\): now, x and y best responses to each other, so solution to original game.</p>
<h2 id="games-with-n-turns">Games with n turns</h2>
<h3 id="casting-as-a-simultaneous-move-game">Casting as a simultaneous move game</h3>
<p>Each \(p_{i}\) picks full strategy from \(S_{i}^{n}\). But, \(|S_{i}^{n}| = |S_{i}|^{n}\); so games with turns are a compact representation. Extensive form: Game tree with payoffs at leaves.</p>
<h3 id="subgame-perfect-nash-equilibria">Subgame Perfect Nash Equilibria</h3>
<p>Nash Equilib with notion of order of moves: Strategy should be Nash even if any prefix of the game is already played.</p>
<h4 id="ultimatum-game">Ultimatum game</h4>
<p>\(p_{1}\), \(p_{2}\) split money m; 1 turn each: \(p_{1}\) offers n; \(p_{2}\) accepts or reject. \(p_{2}\)&rsquo;s interest to accept whatever offered. Cast to a simultaneous move game. Many Nash equilibria: If \(p_{2}\) rejects if \(n &lt; o\), \(p_{1}\) must offer o. 1 subgame perfect nash equilib.</p>
<h2 id="games-with-partial-info-about-utilities">Games with partial info about utilities</h2>
<p>Work with beliefs about others&rsquo; properties and preferences.</p>
<h3 id="bayesian-games">Bayesian Games</h3>
<p>Eg: Card game: \<br>
Only distribution of others&rsquo; cards known.</p>
<h4 id="bayesian-first-price-auction">Bayesian first price auction</h4>
<p>\(\set{p_{i}}\) have values \(\set{v_{i}}\) for item. If all info available; best strategy for \(p_{i}\): choose \(s_{i}\) = \(v_{j}\) next lower to \(v_{i}\). If only distribution of \(v_{k}\) for other players known, \(p_{i}\) bids second E[\(v_{j}\) next lowest to \(v_{i}|v_{i}\) is max]. \why</p>
<h2 id="cooperative-games">Cooperative games</h2>
<p>Groups (G ..) can change strategies jointly.</p>
<h3 id="strong-nash-equilibrium">Strong Nash Equilibrium</h3>
<p>In s, \(G \subset P\) has \textbf{joint deviation} if \(\exists s&rsquo;<em>{G}| u</em>{i}(s) \leq u_{i}(s&rsquo;_{G}, s_{-A})\forall p_{i} \in G\), and for some \(p_{j} \in G, u_{i}(s) &lt; u_{i}(s&rsquo;_{G}, s_{-A})\).</p>
<p>s is strong Nash if no \(G \subset P\) has joint deviation. Similarly, mixed strategy Nash equilibria. Few games have this. Eg: Stable marriage problem.</p>
<h3 id="stable-marriage-problem">Stable Marriage problem</h3>
<p>\tbc</p>
<h3 id="transferable-utility">Transferable utility</h3>
<p>\tbc</p>
<h2 id="market-equilibria">Market equilibria</h2>
<h3 id="pricing-and-allocation-game-with-linear-utility">Pricing and Allocation game with linear utility</h3>
<p>Aka Fisher&rsquo;s linear case. Bipartite graph G = (I, B) of goods and buyers: edges indicate interest of \(b \in B\) in \(i \in I\). Quantity of i scaled to 1; price vector for I: p; money vector for B: m. Utility of i for b: \(u_{b, i}\).</p>
<p>Want to find optimal p (pricing) and partition items among B: allocation x. Equilibrium properties: all money, goods exhausted.</p>
<p>Best bang per buck for b: \(a_{b} = max_{i}\frac{u_{b, i}}{p_{i}}\): a linear function: so &lsquo;linear case&rsquo;.</p>
<p>Primal dual approach: Start with arbit p = 1; find x; find \(\set{b}\) with excess money; adjust price; repeat.</p>
<p>Finding x by reduction to network flow problem: add source s and sink t; connect s to all I and t to all B; set capacities of edges in original graph to be \(\infty\) and on new edges to match a(i) and m(b); thence find c.</p>
<h3 id="find-best-allocation">Find best allocation</h3>
<p>(Arrow Debreu) Agents come in with goods portfolio, utilities for various goods, leave with goods: money only inbetween. Generalizes Fisher&rsquo;s linear case: .</p>
<p>Auction based approx algorithm solves it: Market clears approximately.</p>
<h2 id="repeated-games-with-partial-info-about-utilities">Repeated games with partial info about utilities</h2>
<p>\(p_{1}\) in uncertain environment (\(p_{-1}\)); utilities of \(p_{-1}\) not known. Eg: Choosing a route to go to school.</p>
<h3 id="model">Model</h3>
<p>Same game repeated T times; At time t:
\(p_{1}\) uses online algorithm H to pick distr \(D_{H}^{(t)}\) over \(S_{1}\). \(p_{1}\) picks action \(k_{1}^{(t)}\) from \(D_{H}^{(t)}\). Loss/ cost function for \(p_{1}\): \(c_{1}:\times_{i}S_{i} \to [0,1]\). \(c_{1}^{(t)}(k_{1}^{(t)}) \dfn c_{1}(k_{1}^{(t)}, D_{-1}^{(t)})\), \(c_{1}(D) \dfn E_{x \distr D}[c_{1}(x)]\).</p>
<h4 id="model-with-full-info-about-costs">Model with full info about costs</h4>
<p>H gets cost vector \(c_{1}^{(t)} \in [0,1]^{|S_{1}|}\), pays cost \<br>
\(c_{1}(D_{H}^{(t)}, D_{-1}^{(t)}) = E_{k_{1}^{(t)} \distr D_{H}^{(t)}}[c_{1}(k_{1}^{(t)}, D_{-1}^{(t)})] = E_{k_{1}^{(t)} \distr D_{H}^{(t)}}[c_{1}^{(t)}(k_{1}^{(t)})]\).</p>
<p>Total loss for H: \(L_{H}^{(T)} = \sum c_{1}(D_{H}^{(t)}, D_{-1}^{(t)})\).</p>
<h4 id="model-with-partial-info-about-costs">Model with partial info about costs</h4>
<p>Aka Multi Armed Bandit (MAB) model.\<br>
\(p_{1}\) (or H) pays cost for \(k_{1}^{(t)}\): \(c_{1}(k_{1}^{(t)}, D_{-1}^{(t)}) = c_{1}^{(t)}(k_{1}^{(t)})\).</p>
<p>Total loss for H: \(L_{H}^{(T)} = \sum c_{1}(k_{1}^{(t)}, D_{-1}^{(t)})\).</p>
<h4 id="goal">Goal</h4>
<p>Minimize \(\frac{L_{?}^{(T)}}{T}\). Maybe other \(p_{i}\) do the same. \(D_{-1}^{(t)}\) and \(c_{1}^{(t)}\) can vary arbitrarily over time; so, model is adversarial.</p>
<h3 id="best-response-algorithm-for-every-i-start-with-s-suppose-s_-i-fixed-do-hill-climbing-by-varying-s_i">Best response algorithm For every i: Start with s; suppose \(s_{-i}\) fixed, do hill climbing by varying \(s_{i}\).</h3>
<h3 id="regret-analysis">Regret analysis</h3>
<p>H incurs loss \(L_{H}^{(T)}\); \(p_{1}\) sees simple policy \(\pi\) would have had much lower loss. Comparison class of orithms G. \(\pi\) best algorithm in G: \(L_{\pi}^{(T)} = min_{g \in G} L_{g}^{(T)}\). Regret \(R_{G} = L_{H}^{(T)} - L_{\pi}^{(T)} = max_{g \in G} (L_{H}^{(T)} - L_{g}^{(T)})\).</p>
<h4 id="goal-1">Goal</h4>
<p>Minimize \(R_{G}\).</p>
<h4 id="regret-wrt-all-policies-lower-bound">Regret wrt all policies: Lower bound</h4>
<p>\(G_{all} = \set{g: T \to S_{1}}\): \(\exists\) sequence of loss vectors \(c_{1}^{(t)}\): \(R_{G_{all}} \geq T(1-|S_{1}|^{-1})\):</p>
<p>For \(k&rsquo; = argmin_{k_{1}^{(t)}} Pr_{D_{H}^{(t)}}(k_{1}^{(t)})\), \(c_{1}^{(t)}(k&rsquo;) = 0\), for others, \<br>
\(c_{1}^{(t)}(k_{1}^{(t)}) = 1\); \(\min_{k_{1}^{(t)}} Pr_{D_{H}^{(t)}}(k_{1}^{(t)}) \leq |S_{1}|^{-1}\).</p>
<p>So, must restrict G.</p>
<h3 id="external-regret">External regret</h3>
<p>Aka Combining Expert Advice. \(G = \set{i^{T} : i \in S_{1}}\), policies where all \(k_{1}^{(t)}\) are the same; \(\pi\) is best single action. \(L_{\pi}^{(T)} = \sum c_{1}(\pi, D_{-1}^{(t)})\).</p>
<p>If H has low external regret bound: H matches performance of offline algorithm. \why  H comparable to optimal prediction rule from some large hyp class H. \why</p>
<h4 id="deterministic-greedy-dg-algorithm">Deterministic Greedy (DG) algorithm</h4>
<p>\(S_{1}^{(t-1)} = \set{i: argmin_{i \in S_{1}} L_{i}^{(t-1)}}\),\<br>
\(k_{1}^{(t)} = \min_{i \in S_{1}^{(t-1)}} i\). \(L_{DG}^{(T)} \leq |S_{1}| min_{i}(L_{i}^{(T)}) + (|S_{1}|-1)\): Suppose \(c_{1}^{(t)} \in \set{0,1}^{|S_{1}|}\). For every increase in
\(\min_{i} L_{i}^{(t)}\), max loss \(|S_{1}|\): For \(L_{DG}^{(t)} = L_{DG}^{(t-1)} + 1\) but \(\min_{i} L_{i}^{(t)} = \min_{i}L_{i}^{(t-1)}\): \(S_{1}^{(t)} \subseteq S_{1}^{(t-1)}\); so count num of times \(S_{1}^{(t)}\) can shrink by 1.</p>
<h4 id="deterministic-algorithm-lower-bound-for-any-deterministic-online-algorithm-h-exists-loss-seq-where-l_ht--t-min_i-in-s_1l_it-leq-floorts_1-c_1tk_1t--1-for-other-i-c_1ti--0-so-l_ht--t-some-action-used-by-h-leq-floorts_1-times-so-min_i-in-s_1l_it-leq-floorts_1">Deterministic algorithm Lower bound For any deterministic online algorithm H&rsquo;, \(\exists\) loss seq where \(L_{H&rsquo;}^{(T)} = T, min_{i \in S_{1}}(L_{i}^{(t)}) \leq \floor{T/|S_{1}|}\): \(c_{1}^{(t)}(k_{1}^{(t)}) = 1\), for other i, \(c_{1}^{(t)}(i) = 0\); so \(L_{H&rsquo;}^{(T)} = T\); some action used by H&rsquo; \(\leq \floor{T/|S_{1}|}\) times; so \(min_{i \in S_{1}}(L_{i}^{(t)}) \leq \floor{T/|S_{1}|}\).</h4>
<p>So find rand algorithm.</p>
<h4 id="rand-weighted-majority-algorithm-rwm">Rand Weighted majority algorithm (RWM)</h4>
<p>Suppose \(c_{1}^{(t)} \in \set{0,1}^{|S_{1}|}\). Treat \(S_{1}\) as a bunch of experts: Want to put as much wt as possible on best expert. Let \(|S_{1}| = N\). Init weights \(w_{i}^{(1)} = 1\), total wt \(W^{(1)} = N\), \(Pr_{D_{H}^{(1)}}(i) = N^{-1}\).</p>
<p>If \(c_{1}^{(t-1)}(i) = 1\), \(w_{i}^{(t)} = w_{i}^{(t)}(1-\eta)\), \(Pr_{D_{1}^{(t)}}(i) = \frac{w_{i}^{(t)}}{W^{(t)}}\). \why Like analysis of mistake bound of panel of k experts in colt ref.</p>
<p>For \(\eta &lt; 2^{-1}\), \(L_{H}^{(T)} \leq (1+ \eta) \min_{i \in S_{1}}L_{i}^{(t)} + \frac{\ln N}{\eta}\). Any time H sees significant expected loss, big drop in W. \(W^{(T+1)} \geq max_{i}w_{i}^{(T+1)} = (1-\eta)^{\min_{i}L_{i}^{(T)}}\). \tbc</p>
<p>For \(\eta = \min \set{\sqrt{\ln N/ T}, 2^{-1}}\): \(L_{H}^{(T)} \leq \min_{i} L_{i}^{(T)} + 2\sqrt{T\ln N}\). If T unknown, use &lsquo;guess and double&rsquo; with const loss in regret. \why</p>
<h4 id="polynomial-weights-algorithm">Polynomial weights algorithm</h4>
<p>Extension of RWM to \(c_{1}^{(t)} \in [0,1]^{|S_{1}|}\). Wt update is \(w_{i}^{(t)} = w_{i}^{(t)}(1-\eta c^{(t-1)}(i))\). \(L_{H}^{(T)} \leq \min_{i} L_{i}^{(T)} + 2\sqrt{T\ln N}\). \why</p>
<h4 id="rand-algorithm-lower-bounds">Rand algorithm Lower bounds</h4>
<p>If \(T &lt;  \log_{2} N\): For any online algorithm H, \(\exists\) stochastic generation of losses: \(E[L_{H}^{(T)}] = T/2\), but \(\min_{i} L_{i}^{(t)} = 0\): at t=1 let N/2 actions get loss 1; at time t: half the actions which had a loss 0 at time t-1 get loss 1; so, probability mass on actions with 0 = \(2^{-1}\).</p>
<p>If N=2, \(\exists\) stochastic generation of losses: \(E[L_{H}^{(T)} - \min_{i} L_{i}^{(T)}] = \Omega(\sqrt{T})\). \why</p>
<h4 id="convergence-to-equilibrium-2-player-constant-sum-repeated-game">Convergence to equilibrium: 2 player constant sum repeated game</h4>
<p>All \(p_{i}\) use algorithm H with external regret R; Value of game: \((v_{i})\). Avg loss: \(\frac{L_{H}^{(T)}}{T} \leq v_{i}\). \why If \(R_{G} = O(\sqrt{T})\), convergence to \(v_{i}\).</p>
<h3 id="low-external-regret-algorithm-in-partial-cost-info-model">Low external regret algorithm in partial cost info model</h3>
<p>Exploration vs exploitation tradeoff in algorithms.</p>
<p>algorithm MAB: Divide time T into K blocks; in each time block \(\tau+1\): explore and get cost vector: execute action i at random time to get vector of RV&rsquo;s: \(\hat{c}^{(\tau)}\), also exploit: use distr \(D^{(\tau)}\) as strategy; pass \(\hat{c}^{(\tau)}\) to full info external regret algorithm F with ext regret \(R^{(K)}\) over K time steps; get distr \(D^{(\tau + 1)}\) from F.</p>
<p>Max Loss during exploration steps: NK. RV for total loss of F over K time blocks: \(\hat{L}<em>{F}^{(T)} = \frac{T}{K}\sum</em>{\tau}p^{\tau}c^{\tau} \leq \frac{T}{K}(min_{i} \hat{L}_{i}^{(K)} + R^{(K)}\). Taking expectation, \(L_{MAB}^{(T)} = E[\hat{L}_{MAB}^{(T)}]= E[\hat{L}_{F}^{(T)} + NK] \leq \frac{T}{K}(E[min_{i} \hat{L}_{i}^{(K)}] + R^{(K)}) + NK \leq \frac{T}{K}(min_{i} E[\hat{L}_{i}^{(K)}] + R^{(K)}) + NK \leq min_{i}L_{i}^{(T)} + \frac{T}{K}R^{(K)} + NK\).</p>
<p>Using the \(O(\sqrt{K\log N})\) algorithm, with \(K=(\frac{T}{K}R_{K})\), we get \(L_{MAB}^{(T)} \leq min_{i}L_{i}^{(T)} + O(T^{2/3}N^{1/3}\log N)\).</p>
<h3 id="swap-regret">Swap regret</h3>
<p>Comparison algorithm (H,g) is H with some swap fn \(g:S_{1} \to S_{1}\).</p>
<h4 id="internal-regret">Internal regret</h4>
<p>A special case: Swap every occurance of action \(b_{1}\) with action \(b_{2}\). Modification fn: \(switch_{i}(k_{i},b_{1}, b_{2}) = k_{i}\) except \(switch_{i}(b_{1},b_{1}, b_{2}) = b_{1}\).</p>
<h4 id="low-internal-regret-algorithm-using-external-regret-minimization-algorithms">Low Internal regret algorithm using external regret minimization algorithms</h4>
<p>Let \(N=|S_{i}|\); \((A_{1}, .., A_{N})\) copies of algorithm with external regret bound R. Master algorithm H gets from \(A_{i}\) distr \(q_{i}^{(t)}\) over \(S_{i}\); makes matrix \(Q^{(t)}\) with \(q_{i}^{(t)}\) as rows; finds stationary distr vector \(p^{(t)} = p^{(t)}Q^{(t)}\): Picking \(k_{i} \in S_{i}\) same as picking \(A_{j}\) first, then picking \(k_{i} \in S_{i}\); gets loss vector \(c^{(t)}\); gives \(A_{i}\) loss vector \(p_{i}^{(t)}c^{(t)}\).</p>
<p>\(\forall j: L_{A_{i}} = \sum_{t} p_{i}^{(t)}\dprod{c^{(t)},q_{i}^{(t)}} \leq \sum_{t} p_{i}^{(t)}c_{j}^{(t)} + R\). Also, Sum of percieved losses = actual loss. So, for any swap fn g, \(L_{H}^{T}\leq \sum_{i}\sum_{t} p_{i}^{(t)}c_{g(i)}^{(t)} + NR = L_{F,g}^{(T)} + NR\).</p>
<p>Thence, using polynomial weights algorithm, swap regret bound\ \(O(\sqrt{|S_{1}| T \log |S_{1}|})\).</p>
<h4 id="convergence-to-correlated-equilibrium">Convergence to Correlated equilibrium</h4>
<p>Every \(p_{i}\) uses strategy with swap regret \(\leq R\): then empirical distr Q over \(\times_{i} S_{i}\) is an \(\frac{R}{T}\) correlated equilibrium. \(R = L_{H}^{(T)} - L_{H,g}^{(T)} = \sum_{t} E_{s^{(t)} \distr D^{(t)}}[r_{i}(s,g)] = T E_{s \distr Q}[r_{i}(s,g)]\).</p>
<p>Convergence if all players have sublinear swap regret.</p>
<h4 id="frequency-of-dominated-strategies">Frequency of dominated strategies</h4>
<p>\(p_{1}\) uses algorithm with swap regret R over time T; w: avg over T of prob weight on \(\eps\) dominated strategies; so \(\eps wT \leq R\); so \(w \leq \frac{R}{T\eps}\).</p>
<p>If algorithm minimizes external regret using polynomial weights algorithm, freq of doing dominated actions tends to 0.</p>

  </div>
</article>







<aside class="card border" id="section-tree-item-">
    <div class="card-title bg-light-gray border d-flex justify-content-between">
        <a href="#ZgotmplZ">03 Games: classes </a>
        <a data-toggle="collapse" href="#section-tree-item-body-" role="button" aria-expanded="false" aria-controls="section-tree-item-body-" >…<i class="fas fa-caret-down" class="collapsed"></i> </a>
    </div>
    <nav id="section-tree-item-body-" class="card-body p-0 collapse">
        
        <li>draft: false</li>
        
        <li>iscjklanguage: false</li>
        
        <li>title: 03 Games: classes</li>
        
    </nav>
</aside>


      </main>
    </div>
    <footer class="bg-yellow  p-1" role="contentinfo">
  <div id="disqus_thread"></div>
  <div id="div_footer_bar" class="container-fluid d-flex justify-content-between">
    <a class="btn btn-secondary" href="https://github.com/vvasuki/notes/issues/new" >
      प्रतिस्पन्दः
    </a>
    <a class="btn btn-secondary" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" /></a>
    <div class="btn small border">
      Built on 2020 Nov 22 10:54:55 UTC. (<a href="http://google.com/search?q=10%3a54%3a55%20UTC to IST">IST</a>)
    </div>
  <ul id="footer-bar-right-custom" class="list-group list-group-horizontal">
  </ul>
  </div>
</footer>

  </body>
  <script type='text/javascript'>
    
    
    
    module_main.default.onDocumentReadyTasks();
  </script>
</html>
